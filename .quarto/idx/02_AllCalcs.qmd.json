{"title":"All Similarity Calculations","markdown":{"yaml":{"title":"All Similarity Calculations","title-block-banner":"#014240","title-block-banner-color":"#FFFFFF","subtitle":"Synergy of severe climate change and extinctions result in taxonomic homogenization in deep yime","date":"last-modified","date-format":"YYYY-MM-DD","mainfont":"Figtree","sansfont":"Figtree","footnotes-hover":true,"reference-location":"margin","lang":"en","number-sections":false,"crossref":{"chapters":true},"author":[{"name":"anonymous","url":"https://github.com/Anonymous-paleobio"}],"highlight-style":"pygments","fig-cap-location":"top","format":{"html":{"toc":true,"toc-expand":5,"toc-location":"left","code-fold":true,"html-math-method":"katex","embed-resources":true}},"editor_options":{"chunk_output_type":"console"}},"headingText":"**Introduction**","containsRefs":false,"markdown":"\n\n```{=html}\n<style> \n\n.author-info {\n  margin-left: 0;\n}\n\n.affiliation-info {\n  font-size: 0.8em;\n}\n\n.content {\n  margin-left: 50px;\n}\n</style>\n```\n\n\nCode compiled and curated by anonymous.\n\n\nThis code is used to calculate the “**Before**” and “**After**” similarity values for each interval-set, and can be\ncustomized for an interval and/or data of your choosing. The calculations for all intervals in our analysis are nearly identical by using this script, with the exception of differences in the interval\nnames and ages. Here, we provide an example using the calculations for the Late Ordovician mass extinction.\n\n## Libraries\n\n```{r, message=FALSE}\nrpkg <- c(\"dplyr ggplot2 readr boot divvy terra divDyn conflicted piggyback CoordinateCleaner fossilbrush rgplates icosa tidyr tibble readr purrr downloadthis ggpubr\")\n\nimport_pkg <- function(x)\n  x |> trimws() |> strsplit(\"\\\\s+\")  |> unlist() |> \n  lapply(function(x) library(x, character.only = T)) |> \n  invisible()\n\nrpkg |> import_pkg()\n\n# Resolve conflicted functions.\nconflicted::conflict_prefer(name = \"filter\", winner = \"dplyr\",losers = \"stats\")\n\n# Test\n```\n\n## Custom functions\n\nMost of the functions we created for this script are stored here.\n\n```{r}\n#' @return calculate great circle distance in kilometers (km).\n#' @param R Earth mean radius (km)\n#' @param long1.r convert from degrees to radians for latitudes and longitudes.\n#' @export\n\ngcd.slc <- function(long1, lat1, long2, lat2) {\n  R <- 6371\n  long1.r <- long1*pi/180\n  long2.r <- long2*pi/180\n  lat1.r <- lat1*pi/180\n  lat2.r <- lat2*pi/180\n  d <- acos(sin(lat1.r)*sin(lat2.r) + cos(lat1.r)*cos(lat2.r) * cos(long2.r-long1.r)) * R\n  return(d) \n  }\n\n# Return calculate jaccard similarity coefficient\n\njaccard_similarity <- function(x) {\n  js_table <- list()\n  for (k in seq_along(x)) {\n  \n  # Unique cells.\n  unique_cells <- unique(x[[k]]$cell)\n  jaccard_similarity_table <- data.frame(cell_x = character(), cell_y = character(), jaccard_similarity = numeric(), stringsAsFactors = F)\n  \n  for (i in 1:length(unique_cells)) {\n    cell_x <- unique_cells[i]\n    # Cell_x\n    unique_names_cell_x <- unique(x[[k]]$accepted_name[x[[k]]$cell == cell_x])\n    \n    for (j in 1:length(unique_cells)) {\n      cell_y <- unique_cells[j]\n      \n      # Duplicate comparisons.\n      if (cell_x == cell_y || cell_x > cell_y) {\n        next\n      }\n      \n      # Cell_y\n      unique_names_cell_y <- unique(x[[k]]$accepted_name[x[[k]]$cell == cell_y])\n      # Intersections.\n      intersection <- length(generics::intersect(unique_names_cell_x, unique_names_cell_y))\n      Un <- length(generics::union(unique_names_cell_x, unique_names_cell_y))\n      jaccard_similarity <- intersection/Un\n      # Combine results.\n      jaccard_similarity_table <- rbind(jaccard_similarity_table, data.frame(cell_x = cell_x, cell_y = cell_y, jaccard_similarity = jaccard_similarity))\n    }\n  }\n  \n  # Results.\n  js_table[[k]] <- jaccard_similarity_table \n  }\n  return(js_table)\n}\n\n# Calculate jaccard similarity coefficient\nczekanowski_similarity <- function(x) {\n  2*abs(sum(x$minimum))/((sum(x$count_cell_x) + sum(x$count_cell_y)))\n}\n\n# Cross-join function.\ngridComb <- function(x, cell, accepted_name) {\n  cA <- expand.grid(cell = unique(x$cell), unique(x$accepted_name)) |> setNames(nm = c(\"cell\",\"accepted_name\"))\n  return(cA)\n}\n\n# Count taxon occurrence per unique cell combination.\nczekanowski_data_prep <- function(x, cell, accepted_name) {  \n  \n  count_taxa_x <- x |> \n    group_by(cell, accepted_name) |>\n    summarize(count = n(), .groups = 'drop') |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" =\"count\")\n  \n  count_taxa_y <- x |> \n    group_by(cell, accepted_name) |>\n    summarize(count = n(), .groups = 'drop') |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" =\"count\")\n\n  # Cell pairs.\n  cell <- unique(x[[cell]])\n  taxa <- unique(x[[accepted_name]])\n  \n  cell_combinations <- expand.grid(cell_x = cell, cell_y = cell,accepted_name = taxa) |>  filter(cell_x != cell_y)\n  \n  result <- cell_combinations |> \n    left_join(count_taxa_x, by = c(\"cell_x\",\"accepted_name\"), relationship = \"many-to-many\") |> \n    # Second join (y) \n    left_join(count_taxa_y, by = c(\"cell_y\", \"accepted_name\")) |> \n    select(\"cell_x\", \"cell_y\", \"accepted_name\", \"count_cell_x\", \"count_cell_y\") |>\n    # Replace NA with 0\n    replace_na(replace = list(count_cell_x = 0, count_cell_y = 0)) |> \n    # Remove rows that at 0 in both count fields.\n    filter(!(count_cell_x == 0 & count_cell_y == 0)) |> \n    # Remove duplicated cell combinations\n    filter(cell_x == cell_y | cell_x > cell_y) |> \n    # Split by cell combination\n    group_split(cell_x,cell_y)\n    \n    return(result)\n}\n\n```\n\n## Paleobiology Database\n\nThe fossil occurrence data analysed in this study was retrieved from the [Paleobiology Database](https://paleobiodb.org/#/) on November of 2024. Data pre-processing made use of functions from the `fossilbrush` and `CoordinateCleaner` R packages.\n\nThe following script shows you how we processed the full Phanerozoic `Pbdb` dataset that was downloaded using the `Pbdb_download_new.R` script, which is available in our repository.\n\nHowever, since that requires you to download the data yourself, we have provided the files of `Pbdb` data used for each interval pair in the `Pbdb_data`\nfolder and this section of the script can therefore be skipped. In the\ncase of the Late Ordovician mass extinction, it is labelled `pbdb_lome.csv` within that folder. So, you have the option to simply load in that file and skip this section if you so choose!\n\n```{r}\n# Load the csv file directly from our repository: \npbdb <- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\npbdb <- pbdb[, -c(1, 22, 21)]\n\n\n \n #If you want to process the raw Phanerozoic-level dataset from 01 yourself, then follow the rest of this chunk of code instead and remove the hashtags from this section using ctrl (or command) + shift + C.\n \n \n# Read occurrence dataset you generated in step 01. Replace the directory with the one of where you stored it:\n# pbdb <-read.csv(file = '~/Documents/BioHom/pbdb.data.Nov2024.csv')\n\n \n# # Adjust radiometric ages\n# interval.ma    <- pbdb |> \n#   group_by(early_interval) |> \n#  summarise(min_ma = min(min_ma))\n# names(interval.ma) <-c(\"early_interval\", \"interval.ma\")\n# pbdb       <- merge(pbdb, interval.ma, by=c(\"early_interval\"))\n# \n# # Find first and last occurrences and merge back into data frame, using min_ma column\n# fadlad <- pbdb |> \n#   group_by(accepted_name)  |> \n#   summarise(\n#     fad = max(interval.ma),\n#     lad = min(interval.ma)\n#   )\n# \n# # Merge fad and lad information into data frame\n# pbdb <- merge(pbdb, fadlad, by=c(\"accepted_name\"))\n# \n# # Add extinction/survivor binary variable\n# pbdb$ex <- 0\n# pbdb$ex[pbdb$interval.ma==pbdb$lad] <- 1\n# \n# # Select variables.\n# pbdb <- pbdb |> \n#   select(any_of(c(\"interval.ma\",\"early_interval\",\"interval.ma\",\"fad\",\"lad\",\n#                   \"accepted_name\",\"genus\",\"ex\",\"phylum\",\"class\",\n#                   \"order\",\"family\",\"paleolat\",\"paleolng\",\"formation\",\"member\",\n#                   \"occurrence_no\",\"collection_no\",\"collection_name\",\n#                   \"reference_no\")))\n# \n# # Keep two classes and select the age-pair you want.\n#  pbdb <- pbdb |> \n#      filter(class %in% c(\"Gastropoda\", \"Bivalvia\", \"Trilobita\", \"Rhynchonellata\", \"Strophomenata\", \"Anthozoa\") &\n#     interval.ma %in% c(\"445.2\", \"443.8\", \"440.8\"))\n#  \n# # Identify Invalid Coordinates.\n#   cl <- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n#   cl_rec <- pbdb[!cl,] #extract and check them\n#   \n#  pbdb <- pbdb |> \n#    cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n#  \n# # Use fossilbrush to clean taxonomic errors\n# b_ranks <- c(\"phylum\", \"class\", \"order\", \"family\", \"accepted_name\") #accepted_name is genus name\n# \n# # Define a list of suffixes to be used at each taxonomic level when scanning for synonyms\n# b_suff = list(NULL, NULL, NULL, NULL, c(\"ina\", \"ella\", \"etta\"))\n# \n# pbdb2 <- check_taxonomy(pbdb, suff_set = b_suff, ranks = b_ranks, verbose = FALSE,clean_name = TRUE, resolve_duplicates = TRUE, jump = 5)\n# # resolves homonyms, and jump refers to which taxonomic rank is the highest we resolve to. jump = 5 will stop before phylum since phylum level usually has little error.\n# \n# # Extract PBDB data from obdb2 so we have the corrected taxa:\n# pbdb <- pbdb2$data[1:nrow(pbdb),]\n# \n# pbdb_fulldata <- pbdb # keep a record of all pertinent information, just in case\n```\n\n## Visualization of Cells and Occurrences\n\nThe globe is divided into a grid of equal-area icosahedral hexagonal cells using the `hexagrid()` function in `icosa`. In `hexagrid(deg = x)`, is roughly equivalent to longitudinal degrees, so that a degree of 1 is roughly equal to 111 km. This selects a tessellation vector, which translates to the amount of area you select for each cell. In our specified grid, each cell is roughly 629,000 km\\^2 and results in a grid of 812 cells.\n\n```{r, message=FALSE}\n# Use this chunk of code if you are processing the raw dataset yourself. Otherwise, skip it!\n\npbdb.2before <- pbdb |> filter(interval.ma==445.2)\npbdb.2after <- pbdb |> filter(interval.ma==443.8)\npbdb.2after2 <- pbdb |> filter(interval.ma == 440.8)\n\n# Find raw locations for each stage:\ncoords.before <- subset(pbdb.2before, select = c(paleolng, paleolat)) \ncoords.before <- coords.before |>  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\ncoords.after<- subset(pbdb.2after, select = c(paleolng, paleolat)) \ncoords.after <- coords.after |>  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\ncoords.after2<- subset(pbdb.2after2, select = c(paleolng, paleolat)) \ncoords.after2<- coords.after2 |>  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\n\n# Set up the grid\nhexa <- hexagrid(deg= 4.5, sf=TRUE) #each deg = ~111 km\nhexa\n```\n\n```{r, message=FALSE}\n# Find cell locations for each occurrence\ncells.before <-locate(hexa, coords.before) \n# str(cells.before) #to see which cells have occ's\ncells.after <-locate(hexa, coords.after)\n#str(cells.after)\ncells.after2 <-locate(hexa, coords.after2)\n\n\n# Next add cells df to coords df in order to match cells with their coordinates:\ncoords.before$cell <- cells.before \nnames(coords.before) <- c(\"long\", \"lat\", \"cell\")\ncoords.after$cell <- cells.after \nnames(coords.after) <- c(\"long\", \"lat\", \"cell\")\ncoords.after2$cell <- cells.after2\nnames(coords.after2) <- c(\"long\", \"lat\", \"cell\")\n\ntcells.before <- table(cells.before) #to get no. of occupied cells\n#str(tcells.cha) #get frequency of cell occ's\ntcells.after <- table(cells.after)\n#str(tcells.ind)\ntcells.after2<- table(cells.after2)\n\ndata.2before <- cbind(pbdb.2before, coords.before) #assigns cell number for each occurrence\ndata.2after <- cbind(pbdb.2after, coords.after)\ndata.2after2 <- cbind(pbdb.2after2, coords.after2)\n\n# pbdb <- rbind(data.2before, data.2after, data.2after2) # use this line only if you are processing the raw dataset yourself.\n```\n\n**Grid Plots**\n\nNext, visualize all occurrences for each stage, using the package `rgplates` and `icosa` on R. Please note that this version requires that you have the [GPlates](https://www.earthbyte.org/) software (v.2.5.0 as of writing this script) installed in your computer, as it is the most optimal version of `rgplates`.\n\n```{r, message=FALSE}\n# Call to Gplates offline (requires installed Gplates software)\n\ntd <-tempdir() #temporary directory\n#td\nrgPath <- system.file(package=\"rgplates\")\n#list.files(rgPath) #confirm that this is the correct path\nunzip(file.path(rgPath, \"extdata/paleomap_v3.zip\"), exdir=td)\n#list.files(file.path(td)) #confirm extraction has happened by looking at temporary directory\npathToPolygons <- file.path(td, \"PALEOMAP_PlatePolygons.gpml\") #static plate polygons\npathToRotations <- file.path(td, \"PALEOMAP_PlateModel.rot\")\n\npm <- platemodel(\n  features = c(\"static_polygons\" = pathToPolygons),\n  rotation = pathToRotations\n)\n\n# Plot it out:\nedge <-mapedge() #edge of the map\nplates.lome<- reconstruct(\"static_polygons\", age= 440, model =pm)\nplot(edge, col = \"lightblue2\")\nplot(plates.lome$geometry, col = \"gray60\", border = NA, add = TRUE)\nplot(hexa,  border=\"white\",add = TRUE)\ngridlabs(hexa, cex=0.5) #get labels for each cell, labeled as spiral from North pole of grid\n```\n\n**Before occurrences**\n\nOccurrences in the Before stage, with colors indicating the number of occurrences in occupied cells.\n\n```{r}\n# Before\nplatesMoll <- sf::st_transform(plates.lome, \"ESRI:54009\")\n#^transform plates to Mollweide projection to plot\nplot(hexa, tcells.before, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n```\n\n**After occurrences - Pulse 1**\n\nOccurrences in the After (Pulse 1) stage, with colors indicating the number of occurrences in occupied cells.\n\n```{r}\n# After\nplot(hexa, tcells.after, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n```\n**After occurrences - Pulse 2**\n\nOccurrences in the After (Pulse 2) stage, with colors indicating the number of occurrences in occupied cells.\n```{R, warning =FALSE}\n# After\nplot(hexa, tcells.after2, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n#save as landscape,10*6 \n\n```\n\n## Data pre-processing\n\nWe investigate the data by dividing it by stage and taxonomic class. We determine the number of cells and occurrences for each stage.\n\n```{r, message=FALSE}\n# Data balance.\npbdb |> \n  arrange(early_interval) |> \n  mutate(early_interval = factor(early_interval, levels=c(\"Katian\", \"Hirnantian\", \"Rhuddanian\"))) |> # reorder the intervals\n  group_by(class,early_interval) |> \n  count() |> \n  ggplot(mapping = aes(x = class, y = n, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = NULL, y = \"Sample Size\") +\n  scale_fill_manual(values =  c(\"#FFBF00\",\"#0072B2\",\"#D5006D\",\"#009E73\",\"black\",\"#984EA3\", \"#003F5C\"))+\n  scale_color_manual(values = c(\"#FFBF00\",\"#0072B2\",\"#D5006D\",\"#009E73\", \"black\",\"#984EA3\", \"#003F5C\")) +\n  facet_wrap(.~ early_interval, scales = \"free\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 15, face = \"bold\"),\n        axis.title = element_text(size = 12,face = \"bold\"),\n        axis.text.x = element_text(size = 12, angle=45, hjust=1),\n        axis.text.y= element_text(size=12),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"none\",\n        aspect.ratio = 0.8)\n```\n\n```{r, warning=FALSE}\n# Set min occurrences\nmin_occ <- 15\n\n# Katian cells.\n before_pbdb <-\n  pbdb |> \n  filter(early_interval == \"Katian\")\n\n# before_pbdb <- \n#  before_pbdb |> \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = before_pbdb |> select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# before_pbdb <-\n#  before_pbdb |> \n#  group_by(cell) |>\n#  count() |> \n#  setNames(nm = c(\"cell\",\"occs\")) |> \n#  inner_join(before_pbdb, by = c(\"cell\")) |>\n#  filter(occs >= min_occ)\n\n# Cell centroids.\n# before_centroid <- \n# as.data.frame(centers(hexa))[names(table(before_pbdb$cell)),] |> \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid to master dataframe: Longitude and Latitude.\n# before_pbdb <- \n#  before_pbdb |> \n#  left_join(before_centroid, by = \"cell\")\n\n# after< cells\n after_pbdb <-\n  pbdb |> \n  filter(early_interval == \"Hirnantian\")\n\n# after_pbdb <- \n# after_pbdb |> \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = after_pbdb |> select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# after_pbdb <-\n#  after_pbdb |> \n#  group_by(cell) |>\n#  count() |> \n#  setNames(nm = c(\"cell\",\"occs\")) |> \n#  inner_join(after_pbdb,by = c(\"cell\")) |>\n#  filter(occs >= min_occ)\n\n# Cell centroids\n# after_centroid <- \n#  as.data.frame(centers(hexa))[names(table(after_pbdb$cell)),] |> \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid coordinates to master dataframe.\n# after_pbdb <- \n#  after_pbdb |> \n#  left_join(after_centroid, by = \"cell\")\n\n## After2\n after_pbdb2 <-\n  pbdb |> \n  filter(early_interval == \"Rhuddanian\")\n\n# after_pbdb2 <- \n#  after_pbdb2 |> \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = after_pbdb2|> select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# after_pbdb2 <-\n#  after_pbdb2 |> \n#  group_by(cell) |>\n#  count() |> \n#  setNames(nm = c(\"cell\",\"occs\")) |> \n#  inner_join(after_pbdb2,by = c(\"cell\")) |>\n#  filter(occs >= min_occ)\n\n# Cell centroids\n# after_centroid2 <- \n#  as.data.frame(centers(hexa))[names(table(after_pbdb2$cell)),] |> \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid coordinates to master dataframe.\n# after_pbdb2 <- \n#  after_pbdb2 |> \n#  left_join(after_centroid2, by = \"cell\")\n\n# Combine the two datasets: before & after.\n# The pbdb dataset is has now been fully pre-processed.\n# pbdb <- bind_rows(before_pbdb, after_pbdb, after_pbdb2)\n\n# Create unique identifier for each cell.\n# pbdb <- \n#  data.frame(unique(pbdb$cell)) |> \n#  setNames(nm = \"cell\") |> \n#  mutate(cell_id = c(1:length(cell))) |> \n#  inner_join(pbdb, by = \"cell\")\n\n# Get number of cells for each age\npbdb |> group_by(interval.ma) |> summarise(unique_cells = n_distinct(cell))\n```\n\n```{r, warning=FALSE}\n# Plot number of occurrences per stage and cell.\ncell_text <- \n  data.frame(\n  label = c(\"N = 23 cells\", \"N = 62 cells\", \"N = 15 cells\"),\n  early_interval = c(\"Hirnantian\", \"Katian\", \"Rhuddanian\")\n)\n\n# Plot it\npbdb |> \n   group_by(early_interval,cell) |> \n    count() |>\n mutate(early_interval = factor(early_interval, levels=c(\"Katian\", \"Hirnantian\", \"Rhuddanian\"))) |> # reorder the intervals\n  ggplot(mapping = aes(x = cell, y = n)) + \n  geom_col(col = \"white\", bg = \"#53565A\") +\n  coord_flip() +\n  geom_hline(yintercept = 15, color = \"#B83A4B\") +\n  labs(x = NULL, y = \"Occurrences\") +\n  geom_text(data = cell_text, mapping = aes(x = c(6,12,18), y = 100, label = label),\n            hjust   = -1, vjust = -0.1, size = 3) +\n  facet_wrap(.~ early_interval,scales = \"free\",nrow = 1) +\n  theme_bw() +\n  theme(aspect.ratio = 1.25,\n        axis.text  = element_text(size = 8),\n        axis.title = element_text(face = \"bold\"),\n        strip.text = element_text(face = \"bold\"))\n\n\n\n```\nFor each stage we create individual dataframes based on the cell units and store these into separate lists.\n```{r, warning=FALSE}\n# Data splitting based on cell id and stage.\nbefore_split <-\n  pbdb |> \n  filter(early_interval == \"Katian\") |>\n  group_split(cell_id) |> \n  lapply(as.data.frame)\n\nafter_split <-\n  pbdb |> \n  filter(early_interval == \"Hirnantian\") |>\n  group_split(cell_id) |> \n  lapply(as.data.frame)\n\nafter_split2 <-\n  pbdb |> \n  filter(early_interval == \"Rhuddanian\") |>\n  group_split(cell_id) |> \n  lapply(as.data.frame)\n```\n\n\n## Subsampling by cells and occurrence\n\nHere we perform subsampling without replacement on our stage-level datasets using 99 iterations. For the \"before\" age we randomly sample 15 occurrences per cell and repeated the process as stated above. Conversely, for the \"after\" age, we applied a two-step subsampling procedure by first subsampling down to match the number cells and then by occurrences. The results are subsampled datasets (cell-specific) saved as nested objects within a larger list. These are subsequently, merged into single master dataframes (i.e., the cells) to create one single list containing 99 dataframes.\n\n```{r}\n# after.\nset.seed(31)\n\nboot_after2 <- purrr::map(1:99, ~ {\n  after_split2 |> \n  # Samples rows uniformly.\n purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\nset.seed(3)\n\nboot_after <- purrr::map(1:99, ~ {\n  after_split |> \n  # Samples rows uniformly.\n sample(15, replace = FALSE) |> \n purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\n# before.\nset.seed(4)\n\nboot_before <- purrr::map(1:99, ~ {\n  before_split |> \n  # Step 1. Cells.\n  sample(15, replace = FALSE) |> \n  # Step 2. Rows (i.e., occurrences).\n  purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n```\n\nAs indicated in the previous section, we here combine cell-specific dataframes (N=13) into single joint dataframes (13\\*20 = 260 rows). This is repeated for all 99 sub-sampled dataframes. Worthy of note, the cells in the after list, will inevitably vary between the subsampled datasets, whereas, in the case of the \"before\" age they are all identical. This is because our analysis seeks to assess the impact by cell heterogeneity across geologic stages.\n\n```{r}\n# Before.\ncombined_boot_before <- \n  list()\n\nfor(i in seq_along(boot_before)) {\n  pBe <- purrr::map_dfr(boot_before[[i]], bind_rows)\n  combined_boot_before[[i]] <- pBe\n}\n\n# after.\ncombined_boot_after <- \n  list()\n\nfor(i in seq_along(boot_after)) {\n  pAf <- purrr::map_dfr(boot_after[[i]], bind_rows)\n  combined_boot_after[[i]] <- pAf\n}\n\n# after-2.\ncombined_boot_after2 <- \n  list()\n\nfor(i in seq_along(boot_after2)) {\n  pAf2 <- purrr::map_dfr(boot_after2[[i]], bind_rows)\n  combined_boot_after2[[i]] <- pAf2\n}\n```\n\n## Generic occurrence per cell\n\nFor each subsampled dataset in both the After and Before lists we here count the number of occurrence of each genera by cell. This is done for all dataframes and are then combined into one master dataframe.\n\n```{r message=FALSE}\n# before.\nbefore_count_ls <- \n  purrr::map(combined_boot_before, ~ .x |> group_by(cell,accepted_name) |> summarise(occs = n(), .groups = 'drop')) |> \n  lapply(as.data.frame) |> \n  bind_rows()\n\n# after.\nafter_count_ls <- \n  purrr::map(combined_boot_after, ~ .x |> group_by(cell,accepted_name) |> summarise(occs = n(), .groups = 'drop')) |> \n  lapply(as.data.frame) |> \n  bind_rows()\n\n#after.2\nafter_count_ls2 <- \n  purrr::map(combined_boot_after2, ~ .x |> group_by(cell,accepted_name) |> summarise(occs = n(), .groups = 'drop')) |> \n  lapply(as.data.frame) |> \n  bind_rows()\n```\n\n## Unique cell pairs\n\n```{r}\n# before & after.\ncells_distinct_before <- \n  tibble(unique(before_count_ls$cell)) |> setNames(nm = \"x\") |> \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |> \n  arrange(n_part) |> \n  pull(x)\n\ncells_distinct_after <- \n  tibble(unique(after_count_ls$cell)) |> setNames(nm = \"x\") |> \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |> \n  arrange(n_part) |> \n  pull(x)\n\ncells_distinct_after2 <- \n  tibble(unique(after_count_ls2$cell)) |> setNames(nm = \"x\") |> \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |> \n  arrange(n_part) |> \n  pull(x)\n\n# Distinct cell pairs.\ncells_distinct_pair_before <-\n  expand.grid(cells_distinct_before,cells_distinct_before,stringsAsFactors = F) |> \n  setNames(nm = c(\"x\",\"y\")) |> \n  filter(x<y) |> \n  as_tibble() \n\ncells_distinct_pair_after <-\n  expand.grid(cells_distinct_after,cells_distinct_after,stringsAsFactors = F) |>\n  setNames(nm = c(\"x\",\"y\")) |> \n  filter(x<y) |> \n  as_tibble() \n\ncells_distinct_pair_after2 <-\n  expand.grid(cells_distinct_after2,cells_distinct_after2,stringsAsFactors = F) |>\n  setNames(nm = c(\"x\",\"y\")) |> \n  filter(x<y) |> \n  as_tibble() \n```\n\n## Jaccard indices\n\n**The Jaccard similiary equation** following Miller et al., 2009\n\n$$ J(Cell X, Cell Y) = \\frac{|Cell X \\cap Cell Y|}{|Cell X \\cup Cell Y|} $$\n\n```{r, message=FALSE}\n# before.\nbefore_jaccard <- \n  jaccard_similarity(combined_boot_before)\n\n# after.\nafter_jaccard <- \n  jaccard_similarity(combined_boot_after)\n\n# after2.\nafter_jaccard2 <- \n  jaccard_similarity(combined_boot_after2)\n```\n\n```{r, message=FALSE}\n# Average similarity for each cell-pair and stage.\n\n# before.\nave_before_jaccard <- \n  bind_rows(before_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after_jaccard <- \n  bind_rows(after_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after2\nave_after_jaccard2 <- \n  bind_rows(after_jaccard2) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n## Great circle distance\n\n```{r}\n# colnames(pbdb)[23] <- \"lat\"\n# colnames(pbdb)[22] <- \"long\" \n\n#before\nbefore_res_matrix <- cells_distinct_pair_before |> \n  # X-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,long,lat) |> \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |> \n  # Y-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,x_long,x_lat,long,lat) |> \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |> \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |> \n  as.data.frame()\n\n# after.\nafter_res_matrix <- cells_distinct_pair_after |> \n  # X-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,long,lat) |> \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |> \n  # Y-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,x_long,x_lat,long,lat) |> \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |> \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |> \n  as.data.frame()\n\n# after2.\nafter_res_matrix2 <- cells_distinct_pair_after2 |> \n  # X-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,long,lat) |> \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |> \n  # Y-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,x_long,x_lat,long,lat) |> \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |> \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |> \n  as.data.frame()\n```\n\n## Czekanowski indices\n\n**Czekanowski equation** following Miller et al., 2009\n\n$$ Czekanowski = 2 * \\frac{\\sum \\min(x_{1k}, x_{2k})}{\\sum x_{1k} + \\sum x_{2k}} $$ The occurrence of a given taxa between distinct cells are evaluated against each other.\n\n```{r}\n# before.\n# before.\nbefore_combs <- gridComb(x = before_pbdb,cell = cell, accepted_name = accepted_name) # 13*221 = 2873 rows.\n\n# after.\nafter_combs <- gridComb(x = after_pbdb,cell = cell, accepted_name = accepted_name) # 20*93 = 1860 rows. 190 unique cell pairs (check!)\n\nafter_combs2 <- gridComb(x = after_pbdb2,cell = cell, accepted_name = accepted_name) # 20*93 = 1860 rows. 190 unique cell pairs (check!)\n\n# Next count the occurrence of genera per unique cell. This will also include genera with no occurrence in any given cell (i.e. 0).\n# These are subsequently removed in the next step.\n\ncountGen <- function(combinations, age_lists) {\n  purrr::map(seq_along(age_lists), function(i) {\n    name_counts <- \n      combinations |> \n      left_join(age_lists[[i]] |>  group_by(cell, accepted_name) |> count(), by = c(\"cell\", \"accepted_name\")) |> \n      # Replace NA with 0.\n      replace_na(list(n = 0))\n    return(name_counts)\n  })\n}\n\n# before.\nbefore_genCell <- countGen(combinations = before_combs, age_lists = combined_boot_before)\n# after.\nafter_genCell <- countGen(combinations = after_combs, age_lists = combined_boot_after)\nafter_genCell2 <- countGen(combinations = after_combs2, age_lists = combined_boot_after2)\n\n# Create two identical count dataframes for each pair to join against.\n\n# before.\nbefore_count_lsX <- purrr::map(before_genCell, ~ .x |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nbefore_count_lsY <- purrr::map(before_genCell, ~ .x |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n# after.\nafter_count_lsX <- purrr::map(after_genCell, ~ .x |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nafter_count_lsY <- purrr::map(after_genCell, ~ .x |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n# after.2\nafter_count_lsX2 <- purrr::map(after_genCell2, ~ .x |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nafter_count_lsY2<- purrr::map(after_genCell2, ~ .x |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n\n\nset.seed(5)\n\n# Merge counts for each cell pair.\n mCount <- function(cell_pairs, X, Y) {\n  \n  purrr::map(1:99, function(i) {\n    # Rename the fields so that it matches.\n    oG <- \n      cell_pairs |> rename(\"cell_x\" = \"x\", \"cell_y\" = \"y\") |> \n      # First join (x)\n      left_join(X[[i]], by = \"cell_x\", relationship = \"many-to-many\") |> \n      # Second join (y) \n      left_join(Y[[i]], by = c(\"cell_y\", \"accepted_name\")) |> \n      select(\"cell_x\", \"cell_y\", \"accepted_name\", \"count_cell_x\", \"count_cell_y\")\n    \n    return(oG)\n  })\n}\n\n# before.\nbefore_joined <- mCount(cell_pairs = cells_distinct_pair_before,X = before_count_lsX, Y = before_count_lsY)\n\n# after.\nafter_joined <- mCount(cell_pairs = cells_distinct_pair_after,X = after_count_lsX, Y = after_count_lsY)\n\n# after.2\nafter_joined2 <- mCount(cell_pairs = cells_distinct_pair_after2,X = after_count_lsX2, Y = after_count_lsY2)\n\n# We then split based on distinct cell pairs. This will creates a nested list with X splits each dataframe i.e. 99. We also remove any genera (i.e. accepted name) were 0 occurrences is recorded between cell pairs.\n# This step also add a new field (the minimum field) which is based on the lowest number occurrences of a particular taxa between two cells.\n\nczekanowski_splits <- function(joined_lists) {\n  \n  purrr::map(1:99, function(i) {\n  oP <- joined_lists[[i]] |>\n    # Remove\n    filter(!(count_cell_x == 0 & count_cell_y == 0)) |> \n    # Compute the minimum value between cell x and cell y (use count variable)\n    mutate(minimum = pmin(count_cell_x, count_cell_y)) |> \n    group_by(cell_x, cell_y) |>  \n    group_split()\n  \n  return(oP)\n  })\n}\n\ncz_before_prep <- czekanowski_splits(before_joined)\ncz_after_prep <- czekanowski_splits(after_joined)\ncz_after_prep2 <- czekanowski_splits(after_joined2)\n\n# Compute the czekanowski index.\nbefore_czekanowski <- vector(mode = \"list\")\nfor(i in seq_along(cz_before_prep)) {\n  cz <- lapply(cz_before_prep[[i]], czekanowski_similarity)\n  before_czekanowski[[i]] <- cz\n}\n\nafter_czekanowski <- vector(mode = \"list\")\nfor(i in seq_along(cz_after_prep)) {\n  cz <- lapply(cz_after_prep[[i]], czekanowski_similarity)\n  after_czekanowski[[i]] <- cz\n}\n\nafter_czekanowski2 <- vector(mode = \"list\")\nfor(i in seq_along(cz_after_prep2)) {\n  cz <- lapply(cz_after_prep2[[i]], czekanowski_similarity)\n  after_czekanowski2[[i]] <- cz\n}\n\n# Cell pairs.\npairs_before <- do.call(\"rbind\",lapply(cz_before_prep[[1]], function(x) x[1:2][1,]))\n\n# Find all pairs in the After\npairs_after <- vector(mode = \"list\")\npairs_after2 <- vector(mode = \"list\")\n\nfor(i in 1:99) {\n  append_cells <- do.call(\"rbind\",lapply(cz_after_prep[[i]], function(x) x[1:2][1,]))\n  pairs_after[[i]] <- append_cells\n}\nfor(i in 1:99) {\n  append_cells <- do.call(\"rbind\",lapply(cz_after_prep2[[i]], function(x) x[1:2][1,]))\n  pairs_after2[[i]] <- append_cells\n}\n\n# Reformat \nbefore_cz_results <- \n  purrr::map(before_czekanowski, ~as.data.frame(unlist(.x)) |> \n               rename(\"cz\" = 1) |>\n               cbind(pairs_before) |> \n               relocate(.after = \"cell_y\",\"cz\") |> \n               rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") )\n\nafter_cz_results <- \n  purrr::map(after_czekanowski, ~as.data.frame(unlist(.x)) |> \n               rename(\"cz\" = 1))\n\nafter_cz_results2 <- \n  purrr::map(after_czekanowski2, ~as.data.frame(unlist(.x)) |> \n               rename(\"cz\" = 1))\n\n# Now bind the cell pairs to the Af\\ter datasets.\nafter_cz_results <- mapply(function(x, y) cbind(y, x), after_cz_results, pairs_after, SIMPLIFY = FALSE)\nafter_cz_results2 <- mapply(function(x, y) cbind(y, x), after_cz_results2, pairs_after2, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell pair\nbefore_czekanowski_dataframe <- bind_rows(before_cz_results) \nafter_czekanowski_dataframe <- bind_rows(after_cz_results)\nafter_czekanowski_dataframe2 <- bind_rows(after_cz_results2)\n```\n\n## Results\n\n```{r}\n# before\nbefore_res_matrix <- \n  before_res_matrix |> \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |> \n  left_join(ave_before_jaccard,by = c(\"x.cell\",\"y.cell\"))\n\n# after\nafter_res_matrix <- \n  after_res_matrix |> \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |> \n  left_join(ave_after_jaccard,by = c(\"x.cell\",\"y.cell\"))\n\n# after2\nafter_res_matrix2 <- \n  after_res_matrix2 |> \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |> \n  left_join(ave_after_jaccard2,by = c(\"x.cell\",\"y.cell\"))\n\n# Bin by distance between cells (GCD in km's)\nbefore_res_matrix$cutdist <- \n  cut(before_res_matrix$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\nafter_res_matrix$cutdist <- \n  cut(after_res_matrix$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\nafter_res_matrix2$cutdist <- \n  cut(after_res_matrix2$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\n# Average and sd for Before.\nsumRes_01 <-\n  before_res_matrix |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm= TRUE),\n    second = quantile(avg_jaccard,probs=0.975, na.rm = TRUE)\n  ) |> \n  mutate(label = 'Before',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Average and sd for the After.\nsumRes_02 <- \n  after_res_matrix |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n     # Quantiles\n    first = quantile(avg_jaccard,probs=0.25),\n    second = quantile(avg_jaccard,probs=0.975)\n  ) |> \n  mutate(label = 'After',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Average and sd for the After 2.\nsumRes_025 <- \n  after_res_matrix2 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n     # Quantiles\n    first = quantile(avg_jaccard,probs=0.25),\n    second = quantile(avg_jaccard,probs=0.95)\n  ) |> \n  mutate(label = 'After-2',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Combine the two.\nsumRes_03 <- bind_rows(sumRes_01,sumRes_02, sumRes_025)\n\n\n# Plot.\nsumRes_03 |> \n   arrange(label) |> \n  mutate(label = factor(label, levels=c(\"Before\", \"After\", \"After-2\"))) |> # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"black\", \"darkorange4\",\"goldenrod3\")) + \n  scale_color_manual(values = c(\"black\",\"darkorange4\",\"goldenrod3\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Jaccard\",\n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\n<!-- Czekanowski -->\n```{r, warning=FALSE}\n# before\nbefore_res_matrix <-\n  before_res_matrix |>\n  left_join(\n    before_czekanowski_dataframe |> #rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |>\n      group_by(x.cell,y.cell) |>\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |>\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n# after\nafter_res_matrix <-\n  after_res_matrix |>\n  left_join(\n    after_czekanowski_dataframe |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |> \n      group_by(x.cell,y.cell) |>\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |>\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n# after2\nafter_res_matrix2 <-\n  after_res_matrix2 |>\n  left_join(\n    after_czekanowski_dataframe2 |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |> \n      group_by(x.cell,y.cell) |>\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |>\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n# Average and standard deviation for Before.\nsumRes_045 <-\n  after_res_matrix2 |>\n  group_by(cutdist) |>\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n        # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |>\n  mutate(label = 'After-2',label = as.factor(label)) |>\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |>\n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Average and standard deviation for Before.\nsumRes_04 <-\n  before_res_matrix |>\n  group_by(cutdist) |>\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n        # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |>\n  mutate(label = 'Before',label = as.factor(label)) |>\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |>\n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\nsumRes_05 <-\n  after_res_matrix |>\n  group_by(cutdist) |>\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |>\n  mutate(label = 'After',label = as.factor(label)) |>\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |>\n  as.data.frame() |> suppressWarnings()\n\nsumRes_06 <- bind_rows(sumRes_05, sumRes_04, sumRes_045)\n\n# Plot.\nsumRes_06 |> \n   arrange(label) |> \n  mutate(label = factor(label, levels=c(\"Before\", \"After\", \"After-2\"))) |> # reorder the intervals\n  ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values = c(\"black\",\"darkorange4\", \"goldenrod3\")) + \n  scale_color_manual(values = c(\"black\", \"darkorange4\",\"goldenrod3\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\", \n       title = \"Czekanowski\", \n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"), \n        axis.title = element_text(face = \"bold\"), \n        legend.title = element_text(face = \"bold\"), \n        aspect.ratio = 1)\n```\n\n\n## Sensitivity analysis\n\nSimilarity measurements by survival status.\n\n```{r}\n# Retain occurrences with or greater than 15.\npbdb_sensitivity <- \n  pbdb |> \n  filter(occs >= 15) # 3578 observations.\n\n# Split survival datasets by unique cell id.\n\n#First pulse:\n\n# after survivors\nsAft <- pbdb_sensitivity |> \n  filter(early_interval == \"Hirnantian\" & ex==0 & fad >=443.8) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# before survivors\nsV <- pbdb_sensitivity |>\n  filter(early_interval == \"Katian\" & ex==1) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# after survivors.\nsBef <- pbdb_sensitivity |> \n  filter(early_interval  == \"Katian\" & ex==0) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n## THIS IS FOR ANALYSIS OF SECOND PULSE: \n\n# Survivors After:\nsAftHir <- pbdb_sensitivity |> \n  filter(early_interval == \"Rhuddanian\" & ex==0 & fad >=440.8) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# Victims of Hirnantian pulse.\nsVHir <- pbdb_sensitivity |>\n  filter(early_interval == \"Hirnantian\" & ex==1) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# Survivors of Hirnantian pulse.\nsBefHir <- pbdb_sensitivity |> \n  filter(early_interval  == \"Hirnantian\" & ex==0) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n\n# Subsampling.\nsubsampling_fun <-\n  function(x, n_boot = 99, sample_size = 12, seed = 5) {\n  set.seed(seed)\n  # Samples.\n  boot_samples <- purrr::map(1:n_boot, ~ sample(x, sample_size, replace = FALSE))\n  # Combine cells into single dataframes.\n  comb_samples <- purrr::map(boot_samples, ~ map_dfr(.x, bind_rows))\n  \n  return(comb_samples)\n}\n\n\n# Subsampled data.\nsBef_boot <- subsampling_fun(sBef)\nsV_boot <- subsampling_fun(sV)\nsAft_boot <- subsampling_fun(sAft)\n\n\n# Second pulse.\nsBef_boot_Hir <- subsampling_fun(sBefHir)\nsV_boot_Hir <- subsampling_fun(sVHir)\nsAft_boot_Hir <- subsampling_fun(sAftHir)\n```\n###Jaccard index calculation\n```{r, warning=FALSE}\n# After survivors.\nafter_survivors_jaccard <- jaccard_similarity(sAft_boot)\n# Before victims.\nbefore_victims_jaccard <- jaccard_similarity(sV_boot)\n# Before survivors.\nbefore_survivors_jaccard <- jaccard_similarity(sBef_boot)\n\n\n#Second pulse:\n\n# After survivors.\nafter_survivors_jaccard_Hir <- jaccard_similarity(sAft_boot_Hir)\n# Before victims.\nbefore_victims_jaccard_Hir <- jaccard_similarity(sV_boot_Hir)\n# Before survivors.\nbefore_survivors_jaccard_Hir <- jaccard_similarity(sBef_boot_Hir)\n```\n**Averages**\n```{r, warning=FALSE}\n##First pulse:\n\n# Mean jaccard for the After survivors.\naAftsJ <- \n  bind_rows(after_survivors_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# Mean jaccard for the Before victims.\navJ <- \n  bind_rows(before_victims_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# before survivors.\naBefsJ <- \n  bind_rows(before_survivors_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n## Second pulse:\n\n# Mean jaccard for the After survivors.\naAftsJ_Hir <- \n  bind_rows(after_survivors_jaccard_Hir) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# Mean jaccard for the Before victims.\navJ_Hir <- \n  bind_rows(before_victims_jaccard_Hir) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# before survivors.\naBefsJ_Hir <- \n  bind_rows(before_survivors_jaccard_Hir) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n**Visualize results**\n\n```{r, message=FALSE, warning=FALSE}\n## FIRST PULSE: \n\n# After results: survivors.\nmRes_01 <- after_res_matrix[,c(1:7,10)] |> left_join(aAftsJ,by = c(\"x.cell\",\"y.cell\"))\n# Before results: survivors & victims.\nmRes_02 <- before_res_matrix[,c(1:7,10)] |> left_join(avJ,by = c(\"x.cell\",\"y.cell\"))\nmRes_03 <- before_res_matrix[,c(1:7,10)] |> left_join(aBefsJ,by = c(\"x.cell\",\"y.cell\"))\n\n# SECOND PULSE \nmRes_04 <- after_res_matrix2[,c(1:7,10)] |>  left_join(aAftsJ_Hir,by = c(\"x.cell\",\"y.cell\"))\n# Before results: survivors & victims.\nmRes_05 <- after_res_matrix[,c(1:7,10)] |> left_join(avJ_Hir,by = c(\"x.cell\",\"y.cell\"))\nmRes_06  <- after_res_matrix[,c(1:7,10)] |> left_join(aBefsJ_Hir,by = c(\"x.cell\",\"y.cell\"))\n\n# Summary statistics for each survival category.\n\n\n## FIRST PULSE \n# after survivors.\nsumRes_07 <-\n  mRes_01 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25,na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'After survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame()\n```\n```{r, warning=FALSE}\n# before victims.\nsumRes_08 <-\n  mRes_02 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before victims',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# before survivors.\nsumRes_09 <-\n  mRes_03 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n## SECOND PULSE \n\n# after survivors.\nsumRes_10 <-\n  mRes_04 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25,na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'After survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame()\n\n# before victims.\nsumRes_11  <-\n  mRes_05 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before victims',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# before survivors.\nsumRes_12 <-\n  mRes_06 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# Combine all results.\nsumRes_13 <- bind_rows(sumRes_07,sumRes_08,sumRes_09) # First pulse\nsumRes_14 <- bind_rows(sumRes_10,sumRes_11,sumRes_12) # Second pulse\n\n# Plot.\nsumRes_13 |> \n  arrange(label) |> \n   mutate(label = factor(label, levels=c(\"Before victims\", \"Before survivors\", \"After survivors\"))) |> # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"gray60\",\"black\",\"goldenrod\")) + \n  scale_color_manual(values = c(\"gray60\",\"black\",\"goldenrod\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"First Pulse\",\n       subtitle = \"Jaccard by survival status\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\n\n```{r, warning=FALSE}\n# Plot for second pulse\nsumRes_14 |> \n  arrange(label) |> \n   mutate(label = factor(label, levels=c(\"Before victims\", \"Before survivors\", \"After survivors\"))) |> # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n scale_fill_manual(values =  c(\"gray60\",\"black\",\"goldenrod\")) + \n  scale_color_manual(values = c(\"gray60\",\"black\",\"goldenrod\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Second Pulse \",\n       subtitle = \"Jaccard by survival status\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\nSetup for the second pulse analysis:\n\n#### Czekanowski index calculation\n\n**After survivors**\n\n```{r}\n# Data table \nprep_Afts <- map(sAft_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_Afts <- map(prep_Afts, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_Afts <- vector(mode = \"list\")\nfor(i in seq_along(prep_Afts)) {\n  cZ <- lapply(prep_Afts[[i]],czekanowski_similarity)\n  cz_Afts[[i]] <- cZ\n}\n\n# Find all pairs.\nsp_01 <- vector(mode = \"list\")\nfor(i in 1:99) {\n  pA <- do.call(\"rbind\",lapply(prep_Afts[[i]], function(x) x[1:2][1,]))\n  sp_01[[i]] <- pA\n}\n\n# Convert to dataframe and unlist.\nflat_Afts <- purrr::map(cz_Afts, ~as.data.frame(unlist(.x)) |> rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_afts <- mapply(function(x, y) cbind(y, x), flat_Afts, sp_01, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_Afts <- bind_rows(append_flat_afts) |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n**Before victims**\n\n```{r, warning=FALSE}\n# Data table \nprep_v <- map(sV_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_v <- map(prep_v, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_v <- vector(mode = \"list\")\nfor(i in seq_along(prep_v)) {\n  cZ <- lapply(prep_v[[i]],czekanowski_similarity)\n  cz_v[[i]] <- cZ\n}\n\n# Find all pairs.\nsp_02 <- vector(mode = \"list\")\nfor(i in 1:99) {\n  pQ <- do.call(\"rbind\",lapply(prep_v[[i]], function(x) x[1:2][1,]))\n  sp_02[[i]] <- pQ\n}\n\n# Convert to dataframe and unlist.\nflat_v <- purrr::map(cz_v, ~as.data.frame(unlist(.x)) |> rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_v <- mapply(function(x, y) cbind(y, x), flat_v, sp_02, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_v <- bind_rows(append_flat_v) |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n**Before survivors**\n\n```{r, warning=FALSE}\n# Data table \nprep_Befs <- map(sBef_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_Befs <- map(prep_Befs, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_Befs <- vector(mode = \"list\")\nfor(i in seq_along(prep_Befs)) {\n  cM <- lapply(prep_Befs[[i]],czekanowski_similarity)\n  cz_Befs[[i]] <- cM\n}\n\n# Find all pairs.\nsp_03 <- vector(mode = \"list\")\nfor(i in 1:99) {\n  pO <- do.call(\"rbind\",lapply(prep_Befs[[i]], function(x) x[1:2][1,]))\n  sp_03[[i]] <- pO\n}\n\n# Convert to dataframe and unlist.\nflat_Befs <- purrr::map(cz_Befs, ~as.data.frame(unlist(.x)) |> rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_Befs <- mapply(function(x, y) cbind(y, x), flat_Befs, sp_03, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_Befs <- bind_rows(append_flat_Befs) |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n```{r, warning=FALSE}\n# Averages.\nave_s1 <- res_Afts |> \n  group_by(x.cell,y.cell) |>\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\nave_s2 <- res_v |> \n  group_by(x.cell,y.cell) |>\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\nave_s3 <- res_Befs |> \n  group_by(x.cell,y.cell) |>\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n```\n**Visualize results**\n\n```{r, warning=FALSE}\n# Reverse cell pairs first.\nave_s1_reversed <- ave_s1|> rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\nave_s2_reversed <- ave_s2|> rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\nave_s3_reversed <- ave_s3|> rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\n\n# after results: survivors.\nqRes_01 <- after_res_matrix[,c(1:7,10)] |>\n  inner_join(ave_s1_reversed, by = c(\"x.cell\",\"y.cell\"))\n# before results: survivors & victims.\nqRes_02 <- ave_s2_reversed |> \n  left_join(before_res_matrix[,c(1:7,10)],by = c(\"x.cell\",\"y.cell\"))\n\nqRes_03 <- ave_s3_reversed |> \n  left_join(before_res_matrix[,c(1:7,10)],by = c(\"x.cell\",\"y.cell\"))\n\n# Summary statistics for each survival category.\n\n# After survivors.\nsumRes_15 <-\n  qRes_01 |> \n  group_by(cutdist) |> \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |> \n  mutate(label = 'after survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame()\n```\n```{r, warning=FALSE}\n# Before victims.\nsumRes_16 <-\n  qRes_02 |> \n  group_by(cutdist) |> \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |> \n  mutate(label = 'before victims',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# before survivors.\nsumRes_17 <-\n  qRes_03 |> \n  group_by(cutdist) |> \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |> \n  mutate(label = 'before survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.97, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# Combine all results.\nsumRes_18 <- bind_rows(sumRes_15,sumRes_16,sumRes_17)\n\n# Plot.\nsumRes_18 |> \n ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"goldenrod\",\"gray60\", \"black\")) + \n  scale_color_manual(values = c(\"goldenrod\",\"gray60\", \"black\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Czekanowski by survival status\",\n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\n## Extra\n\nCount occurrences per genus in the Hirnantian to determine whether they match the disaster taxa in literature, such as the genus Hirnantia.\n\n```{r extra}\n# Find the top 5 genera based on number of occurrences\npbdb.hir  <- pbdb |> filter(interval.ma==443.8)\n\ngenus_counts <- pbdb.hir |>\n  group_by(accepted_name) |> \n             summarise(count = n(), .groups = 'drop') |> \n             top_n(5, wt = count) |>\n             arrange(desc(count))\n\n# Convert accepted_name to a factor with levels in descending order of count\n\ngenus_counts$accepted_name <- factor(genus_counts$accepted_name, levels = genus_counts$accepted_name[order(genus_counts$count, decreasing = TRUE)])\n\n# Visualize it with switched axes\n\nggplot(genus_counts, aes(x = count, y = accepted_name)) + theme_classic() + geom_bar(stat = \"identity\", fill = \"dodgerblue3\") + labs(x = \"Number of Occurrences\", y = \"Genus Name\")+ coord_flip() # Optional: this can be omitted if you want the horizontal bars\n```\n\n###  Wastebin taxa removed\n\nLocate and remove all wastebin taxa, following the methods of *Plotnick and Wagner (2005)*. First, using the full database, locate all wastebin taxa throughout the \"before\" and \"after\" intervals by finding the 5 most frequent genus occurrences.\n\n```{r wastebin}\noptions(download.file.method = \"libcurl\")\npbdb <- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\n\npbdb <- pbdb[, -c(1, 22, 21)]\n\n\n \n# Get the frequency of each genus, then keep top 20 of them. these are our wastebasket genera\n \npbdb_occs <- pbdb |> \n  group_by(accepted_name) |> \n  summarise(frequency = n()) |> \n  arrange(desc(frequency)) |> \n   slice_head(n = 20) |>  # keep only the top 20 rows\n  arrange(accepted_name) # order alphabetically\n        \n\n# Next, continue with the normal process of getting Jaccard similarity.\n\n# Here is where we remove the wastebin taxa:            \npbdb <- pbdb |>    \n  filter(interval.ma %in% c(\"445.2\", \"443.8\", \"440.8\") &\n       (!accepted_name %in% pbdb_occs$accepted_name)) # particularly, this line\n \n# Identify Invalid Coordinates.\n  cl <- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n```\n\n```{r}\n  cl_rec <- pbdb[!cl,] #extract and check them\n  \n pbdb <- pbdb |> \n   cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n```\n \n \nNext, follow all the steps from **Visualization of Cells and Occurrences** down to **Czekanowski indices** again. You will notice slight changes to the number of cells since filtering out the wastebin taxa reduces the number of occurrences we have. \n\n##Spatial standardization\n\nHere, we will apply the `Divvy` package to spatially standardize our data using the circular \"cookie\" method and re-run our Jaccard calculations with the standarized data.\n\n```{r Divvy, message=FALSE, warning=FALSE}\n library(raster)\npbdb <- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\n\npbdb <- pbdb[, -c(1, 22, 21)]\n\n# Make sure coordinates are numerical\npbdb$paleolat <- as.numeric(pbdb$paleolat)\npbdb$paleolng <- as.numeric(pbdb$paleolng)\n\n# Subset by age\npbdb.before <- pbdb |> filter(early_interval == \"Katian\")\npbdb.after <- pbdb |> filter(early_interval == \"Hirnantian\")\npbdb.after2 <- pbdb |> filter(early_interval == \"Rhuddanian\")\n\n# Get the frequency of each genus, then keep top 20 of them. these are our wastebasket genera\n \n# Initialize equal earth projections and coordinate:\nrWorld <-rast()\nprj <- 'EPSG:8857'\n\n# Match the divvy resolution with our hexa resution from the icosa package.\ndeg <- 4.5 # hexa's resolution\n\n# Approximate meters per degree at the equator\nmeters_per_deg <- 111320  \n\n# Adjust for the latitude of area\nlat_center <- mean(raster::yFromCell(rWorld, 1:ncell(rWorld)))  # rough center latitude\nmeters_res <- deg * meters_per_deg * cos(lat_center * pi/180)\nmeters_res\n```\n\n```{r}\n# Get the new resolution\nnew_res <- meters_res\n\n\n# New_res is the target resolution in meters\nrPrj <- project(rWorld, prj, method = \"bilinear\", res = new_res)\n\n\nterra::values(rPrj) <- 1:ncell(rPrj)\n\n# Coordinate column names for the current and target coordinate reference system\nxyCartes <- c('paleolng','paleolat')\nxyCell   <- c('cellX','cellY')\n```\n\n```{r}\nllOccs <- vect(pbdb.before, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.before$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.before[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.before$cell)\n\nsdSumry(pbdb.before, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()\n```\n\n```{r}\n# Disregard SCOR \n\noccUniq <- uniqify(pbdb.before, xyCell)\nptsUniq <- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\nworldP <- ggplot(data = plates.lome$geometry) +\n  theme_bw() +\n  geom_sf() +\n  geom_sf(data = ptsUniq, shape = 16, color = 'red3')\n\ncircLocs <- cookies(dat = pbdb.before,\n                    xy = xyCell,\n                    iter = 100,\n                    nSite = 5,\n                    r = 2000,\n                    weight = TRUE,\n                    crs = prj,\n                    output = \"full\")\n\n\n# Convert one circular sample to sf\nsmplPts <- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr <- smplPts[1, ]\nr_km <- 2000\nbuf <- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n\n# Assume circLocs is your list of circular subsamples (1000 iterations)\n# We'll just plot the first 5 for clarity\nn_plot <- 3\nset.seed(567)\nsubsamples <- circLocs[1:n_plot]\n\n# Convert world map to sf and match projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n# Create a data frame of points and buffers for all subsamples\nall_points <- list()\nall_buffers <- list()\n\nfor (i in seq_along(subsamples)) {\n  smplPts <- st_as_sf(subsamples[[i]], coords = xyCell, crs = prj)\n  \n  # Pick the first point as center\n  cntr <- smplPts[1, ]\n  buf <- st_buffer(cntr, dist = 2000 * 1000)  # 2000 km in meters\n  \n  smplPts$iteration <- paste0(\"#\", i)\n  buf$iteration <- paste0(\"#\", i)\n  \n  all_points[[i]] <- smplPts\n  all_buffers[[i]] <- buf\n}\n\n# Combine all iterations\nall_points_sf <- do.call(rbind, all_points)\nall_buffers_sf <- do.call(rbind, all_buffers)\n\n\n\n# Plot\nggplot() +\n  geom_sf(data = world_sf, fill = \"gray80\", color = \"white\") +\n    geom_sf(data = ptsUniq, shape = 16, color = 'black')+\n  geom_sf(data = all_buffers_sf, fill = NA, linewidth = 1, color = \"navyblue\") +\n  geom_sf(\n    data = all_points_sf,\n    aes(color = iteration),\n    shape = 16,\n    size = 3,\n  ) +\n  theme_bw() +\n  labs(\n    title = \"Circular subsamples (first 3 iterations)\",\n    subtitle=\"Katian\",\n    x = NULL, y = NULL,\n    color = \"Cells in Iteration\"\n  )\n```\nCircLocs now has 100 lists of subsampled occurrences which can be viewed with str(circLocs[[1]]):\n```{r}\nstr(circLocs[[1]])\n```\n\n```{r}\ncircLocs_before <- circLocs \n```\n\n**After the first pulse, spatially standardized:**\n\n```{r, warning=FALSE}\n# Extract cell number and centroid coordinates associated with each occurrence\n\nllOccs <- vect(pbdb.after, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.after$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.after[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.after$cell)\n\nsdSumry(pbdb.after, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()# Extract cell number and centroid coordinates associated with each occurrence\n\nllOccs <- vect(pbdb.after, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.after$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.after[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.after$cell)\n\nsdSumry(pbdb.after, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()\n```\n\n```{r}\n# Disregard SCOR \n\noccUniq <- uniqify(pbdb.after, xyCell)\nptsUniq <- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\n# Circular subsampling technique\n\ncircLocsAft <- cookies(dat = pbdb.after,\n                    xy = xyCell,\n                    iter = 100,\n                    nSite = 5,\n                    r = 2000,\n                    weight = TRUE,\n                    crs = prj,\n                    output = \"full\")\n\n\n# Convert one circular sample to sf\nsmplPts <- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr <- smplPts[1, ]\nr_km <- 2000\nbuf <- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n# worldP +\n#  geom_sf(data = smplPts, shape = 17, color = 'navyblue') +\n#  geom_sf(data = buf, fill = NA, linewidth = 1, color = 'navyblue')\n```\nNow, for the same thing with the “after-2” age for the second pulse.\n\n**After the second pulse, spatially standardized:**\n\n```{r}\n\nllOccs <- vect(pbdb.after2, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.after2$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.after2[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.after2$cell)\n\nsdSumry(pbdb.after2, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()\n\n```\n\n```{r}\n# Disregard SCOR \n\noccUniq <- uniqify(pbdb.after2, xyCell)\nptsUniq <- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\ncircLocsAft2 <- cookies(dat = pbdb.after2,\n                    xy = xyCell,\n                    iter = 1000, # number of iterations\n                    nSite = 5, # number of cells\n                    r = 2000, # radial distance in km\n                    weight = TRUE, \n                    crs = prj, # Equal Earth projection\n                    output = 'full')\n\n\n# Convert one circular sample to sf\nsmplPts <- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr <- smplPts[1, ]\nr_km <- 2000\nbuf <- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n# worldP +\n#  geom_sf(data = smplPts, shape = 17, color = 'navyblue') +\n#  geom_sf(data = buf, fill = NA, linewidth = 1, color = 'navyblue')\n``` \n\nNext, we will check to see how many InF values we have in `sdSumry$SCOR`. This won't matter for most interval-pairs, but for the P-T and LOME intervals in which homogenization does occur, it's a sanity check to understand why the SCOR does not reflect homogenization -- taxon that are present in all cells analyzed here are given an InF score instead of a numerical value and are not included in the SCOR calculation. As per the [Divvy vignette](https://gawainantell.github.io/divvy/articles/habitat-rangesize-case-study.html) walkthrough: \"When a taxon is present in all sampled locations, its log probability of incidence is infinite. Infinity is nonsensical in an empirical comparison (...).\"\n\nSince we won't use SCOR beyond this next chunk of code, the SCOR value will not affect our Jaccard calculations even for homogenized intervals, so we can still spatially standardize the data and see how that effects the Jaccard values.\n```{r}\nsdbef <- sdSumry(circLocs, taxVar = 'genus', xy = xyCell, crs = prj)\nsdaft <- sdSumry(circLocsAft, taxVar = 'genus', xy = xyCell, crs = prj) #tons of InF scores here in PT and LOME indicating widespread occurrences and homogenization has occurred.\nsdaft2 <- sdSumry(circLocsAft2, taxVar = 'genus', xy = xyCell, crs = prj) \n\nis.infinite(sdaft$SCOR) # occurrences that are everywhere are labeled InF. We need to change this to 100 to reflect that they are everywhere, but I don't know how to do that. So, ignore SCOR for now.\n```\n\n```{r}\n# before\nsdbef_inf <- lapply(sdbef, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdbef_mean <- mean(sdbef_inf$SCOR)\n\n\n# after:\nsdaft_inf <- lapply(sdaft, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdaft_mean <- mean(sdaft_inf$SCOR)\n\n\n# after-2:\nsdaft2_inf <- lapply(sdaft2, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdaf2t_mean <- mean(sdaft2_inf$SCOR)\n```\n\nNow, we will calculate the spatially standardized Jaccard similarity for each geologic age:\n\n```{r} \n# before.\nbefore_jaccard <- \n  jaccard_similarity(circLocs)\n\n# after.\nafter_jaccard <- \n  jaccard_similarity(circLocsAft)\n\n# after.\nafter2_jaccard <- \n  jaccard_similarity(circLocsAft2)\n\n\n#before\nave_before_jaccard <- \n  bind_rows(before_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after_jaccard <- \n  bind_rows(after_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after2_jaccard <- \n  bind_rows(after2_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n\n##Spatially Standardized Jaccard for the LOME\n```{r}\n# this function will get mean and 95% confidence intervals \nmean_ci <- function(x, conf = 0.95) {\n  x <- as.numeric(x)\n  x <- x[!is.na(x)]\n  n <- length(x)\n  if (n == 0) return(c(mean = NA, lower = NA, upper = NA))\n  m <- mean(x)\n  se <- sd(x) / sqrt(n)\n  t_crit <- qt(1 - (1 - conf)/2, df = n - 1)\n  ci <- t_crit * se\n  c(mean = m, lower = m - ci, upper = m + ci)\n}\n\n\ndivvy_before  <- ave_before_jaccard |>  mutate(age = \"Before\")\ndivvy_after   <- ave_after_jaccard   |> mutate(age = \"Pulse-1\")\ndivvy_after2  <- ave_after2_jaccard |>  mutate(age = \"Pulse-2\")\n\n\n#  add age columns\ndivvy_before  <- divvy_before  |> mutate(age = \"Before\")\ndivvy_after   <- divvy_after  |>  mutate(age = \"After\")\ndivvy_after2  <- divvy_after2  |>  mutate(age = \"After-2\")\n\n# combine all data\ndivvy_combined <- bind_rows(divvy_before, divvy_after, divvy_after2)\n\n# create a unique pair identifier\ndivvy_combined <- divvy_combined |>  mutate(pair = paste(x.cell, y.cell, sep = \"_\"))\n\n\n# get average and 95% ci\nsummary_divvy <- divvy_combined |> \n  group_by(age) |> \n  summarise(\n    mean = mean(avg_jaccard, na.rm = TRUE),\n    lower = mean_ci(avg_jaccard)[\"lower\"],\n    upper = mean_ci(avg_jaccard)[\"upper\"],\n    .groups = \"drop\"\n  )\n\n# rename so it's in order\nsummary_divvy$age <- c(\"Pulse-1 (Hirnantian)\", \"Pulse-2 (Rhuddanian)\",\"Before (Katian)\" )\n\n\n# now plot!\nggplot(summary_divvy, aes(x = age, y = mean, color = age)) +\n  scale_color_manual(values=c(\"black\", \"darkorange4\",\"goldenrod3\"))+\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n  theme_minimal(base_size = 14) +\n  theme_classic()+\n  labs(\n    title = \"Spatially Standardized Global Jaccard (with 95th and 5th percentile)\",\n    x = \"Age\",\n    y = \"Mean Jaccard Similarity\"\n  ) +\n  theme(legend.position = \"none\")\n```\nAs a final note, what we just analyzed is the average global Jaccard value of the spatially standardized data. This plot does not separate similarity as a function of distance the way the main calculations do. So, that one number is a global value after the late Ordovician mass extinction after spatially subsampling 100 times using the cookie method, which agrees with our Jaccard calculations beforehand. What we calculated in `Results` beforehand also shows that most of the lowered similarity is from cells that are farther apart from one another and that, for this interval, similarity can vary regionally. By using both methods, we can account for regional differences and for differences in the dispersion of cells.\n\n","srcMarkdownNoYaml":"\n\n```{=html}\n<style> \n\n.author-info {\n  margin-left: 0;\n}\n\n.affiliation-info {\n  font-size: 0.8em;\n}\n\n.content {\n  margin-left: 50px;\n}\n</style>\n```\n\n## **Introduction**\n\nCode compiled and curated by anonymous.\n\n\nThis code is used to calculate the “**Before**” and “**After**” similarity values for each interval-set, and can be\ncustomized for an interval and/or data of your choosing. The calculations for all intervals in our analysis are nearly identical by using this script, with the exception of differences in the interval\nnames and ages. Here, we provide an example using the calculations for the Late Ordovician mass extinction.\n\n## Libraries\n\n```{r, message=FALSE}\nrpkg <- c(\"dplyr ggplot2 readr boot divvy terra divDyn conflicted piggyback CoordinateCleaner fossilbrush rgplates icosa tidyr tibble readr purrr downloadthis ggpubr\")\n\nimport_pkg <- function(x)\n  x |> trimws() |> strsplit(\"\\\\s+\")  |> unlist() |> \n  lapply(function(x) library(x, character.only = T)) |> \n  invisible()\n\nrpkg |> import_pkg()\n\n# Resolve conflicted functions.\nconflicted::conflict_prefer(name = \"filter\", winner = \"dplyr\",losers = \"stats\")\n\n# Test\n```\n\n## Custom functions\n\nMost of the functions we created for this script are stored here.\n\n```{r}\n#' @return calculate great circle distance in kilometers (km).\n#' @param R Earth mean radius (km)\n#' @param long1.r convert from degrees to radians for latitudes and longitudes.\n#' @export\n\ngcd.slc <- function(long1, lat1, long2, lat2) {\n  R <- 6371\n  long1.r <- long1*pi/180\n  long2.r <- long2*pi/180\n  lat1.r <- lat1*pi/180\n  lat2.r <- lat2*pi/180\n  d <- acos(sin(lat1.r)*sin(lat2.r) + cos(lat1.r)*cos(lat2.r) * cos(long2.r-long1.r)) * R\n  return(d) \n  }\n\n# Return calculate jaccard similarity coefficient\n\njaccard_similarity <- function(x) {\n  js_table <- list()\n  for (k in seq_along(x)) {\n  \n  # Unique cells.\n  unique_cells <- unique(x[[k]]$cell)\n  jaccard_similarity_table <- data.frame(cell_x = character(), cell_y = character(), jaccard_similarity = numeric(), stringsAsFactors = F)\n  \n  for (i in 1:length(unique_cells)) {\n    cell_x <- unique_cells[i]\n    # Cell_x\n    unique_names_cell_x <- unique(x[[k]]$accepted_name[x[[k]]$cell == cell_x])\n    \n    for (j in 1:length(unique_cells)) {\n      cell_y <- unique_cells[j]\n      \n      # Duplicate comparisons.\n      if (cell_x == cell_y || cell_x > cell_y) {\n        next\n      }\n      \n      # Cell_y\n      unique_names_cell_y <- unique(x[[k]]$accepted_name[x[[k]]$cell == cell_y])\n      # Intersections.\n      intersection <- length(generics::intersect(unique_names_cell_x, unique_names_cell_y))\n      Un <- length(generics::union(unique_names_cell_x, unique_names_cell_y))\n      jaccard_similarity <- intersection/Un\n      # Combine results.\n      jaccard_similarity_table <- rbind(jaccard_similarity_table, data.frame(cell_x = cell_x, cell_y = cell_y, jaccard_similarity = jaccard_similarity))\n    }\n  }\n  \n  # Results.\n  js_table[[k]] <- jaccard_similarity_table \n  }\n  return(js_table)\n}\n\n# Calculate jaccard similarity coefficient\nczekanowski_similarity <- function(x) {\n  2*abs(sum(x$minimum))/((sum(x$count_cell_x) + sum(x$count_cell_y)))\n}\n\n# Cross-join function.\ngridComb <- function(x, cell, accepted_name) {\n  cA <- expand.grid(cell = unique(x$cell), unique(x$accepted_name)) |> setNames(nm = c(\"cell\",\"accepted_name\"))\n  return(cA)\n}\n\n# Count taxon occurrence per unique cell combination.\nczekanowski_data_prep <- function(x, cell, accepted_name) {  \n  \n  count_taxa_x <- x |> \n    group_by(cell, accepted_name) |>\n    summarize(count = n(), .groups = 'drop') |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" =\"count\")\n  \n  count_taxa_y <- x |> \n    group_by(cell, accepted_name) |>\n    summarize(count = n(), .groups = 'drop') |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" =\"count\")\n\n  # Cell pairs.\n  cell <- unique(x[[cell]])\n  taxa <- unique(x[[accepted_name]])\n  \n  cell_combinations <- expand.grid(cell_x = cell, cell_y = cell,accepted_name = taxa) |>  filter(cell_x != cell_y)\n  \n  result <- cell_combinations |> \n    left_join(count_taxa_x, by = c(\"cell_x\",\"accepted_name\"), relationship = \"many-to-many\") |> \n    # Second join (y) \n    left_join(count_taxa_y, by = c(\"cell_y\", \"accepted_name\")) |> \n    select(\"cell_x\", \"cell_y\", \"accepted_name\", \"count_cell_x\", \"count_cell_y\") |>\n    # Replace NA with 0\n    replace_na(replace = list(count_cell_x = 0, count_cell_y = 0)) |> \n    # Remove rows that at 0 in both count fields.\n    filter(!(count_cell_x == 0 & count_cell_y == 0)) |> \n    # Remove duplicated cell combinations\n    filter(cell_x == cell_y | cell_x > cell_y) |> \n    # Split by cell combination\n    group_split(cell_x,cell_y)\n    \n    return(result)\n}\n\n```\n\n## Paleobiology Database\n\nThe fossil occurrence data analysed in this study was retrieved from the [Paleobiology Database](https://paleobiodb.org/#/) on November of 2024. Data pre-processing made use of functions from the `fossilbrush` and `CoordinateCleaner` R packages.\n\nThe following script shows you how we processed the full Phanerozoic `Pbdb` dataset that was downloaded using the `Pbdb_download_new.R` script, which is available in our repository.\n\nHowever, since that requires you to download the data yourself, we have provided the files of `Pbdb` data used for each interval pair in the `Pbdb_data`\nfolder and this section of the script can therefore be skipped. In the\ncase of the Late Ordovician mass extinction, it is labelled `pbdb_lome.csv` within that folder. So, you have the option to simply load in that file and skip this section if you so choose!\n\n```{r}\n# Load the csv file directly from our repository: \npbdb <- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\npbdb <- pbdb[, -c(1, 22, 21)]\n\n\n \n #If you want to process the raw Phanerozoic-level dataset from 01 yourself, then follow the rest of this chunk of code instead and remove the hashtags from this section using ctrl (or command) + shift + C.\n \n \n# Read occurrence dataset you generated in step 01. Replace the directory with the one of where you stored it:\n# pbdb <-read.csv(file = '~/Documents/BioHom/pbdb.data.Nov2024.csv')\n\n \n# # Adjust radiometric ages\n# interval.ma    <- pbdb |> \n#   group_by(early_interval) |> \n#  summarise(min_ma = min(min_ma))\n# names(interval.ma) <-c(\"early_interval\", \"interval.ma\")\n# pbdb       <- merge(pbdb, interval.ma, by=c(\"early_interval\"))\n# \n# # Find first and last occurrences and merge back into data frame, using min_ma column\n# fadlad <- pbdb |> \n#   group_by(accepted_name)  |> \n#   summarise(\n#     fad = max(interval.ma),\n#     lad = min(interval.ma)\n#   )\n# \n# # Merge fad and lad information into data frame\n# pbdb <- merge(pbdb, fadlad, by=c(\"accepted_name\"))\n# \n# # Add extinction/survivor binary variable\n# pbdb$ex <- 0\n# pbdb$ex[pbdb$interval.ma==pbdb$lad] <- 1\n# \n# # Select variables.\n# pbdb <- pbdb |> \n#   select(any_of(c(\"interval.ma\",\"early_interval\",\"interval.ma\",\"fad\",\"lad\",\n#                   \"accepted_name\",\"genus\",\"ex\",\"phylum\",\"class\",\n#                   \"order\",\"family\",\"paleolat\",\"paleolng\",\"formation\",\"member\",\n#                   \"occurrence_no\",\"collection_no\",\"collection_name\",\n#                   \"reference_no\")))\n# \n# # Keep two classes and select the age-pair you want.\n#  pbdb <- pbdb |> \n#      filter(class %in% c(\"Gastropoda\", \"Bivalvia\", \"Trilobita\", \"Rhynchonellata\", \"Strophomenata\", \"Anthozoa\") &\n#     interval.ma %in% c(\"445.2\", \"443.8\", \"440.8\"))\n#  \n# # Identify Invalid Coordinates.\n#   cl <- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n#   cl_rec <- pbdb[!cl,] #extract and check them\n#   \n#  pbdb <- pbdb |> \n#    cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n#  \n# # Use fossilbrush to clean taxonomic errors\n# b_ranks <- c(\"phylum\", \"class\", \"order\", \"family\", \"accepted_name\") #accepted_name is genus name\n# \n# # Define a list of suffixes to be used at each taxonomic level when scanning for synonyms\n# b_suff = list(NULL, NULL, NULL, NULL, c(\"ina\", \"ella\", \"etta\"))\n# \n# pbdb2 <- check_taxonomy(pbdb, suff_set = b_suff, ranks = b_ranks, verbose = FALSE,clean_name = TRUE, resolve_duplicates = TRUE, jump = 5)\n# # resolves homonyms, and jump refers to which taxonomic rank is the highest we resolve to. jump = 5 will stop before phylum since phylum level usually has little error.\n# \n# # Extract PBDB data from obdb2 so we have the corrected taxa:\n# pbdb <- pbdb2$data[1:nrow(pbdb),]\n# \n# pbdb_fulldata <- pbdb # keep a record of all pertinent information, just in case\n```\n\n## Visualization of Cells and Occurrences\n\nThe globe is divided into a grid of equal-area icosahedral hexagonal cells using the `hexagrid()` function in `icosa`. In `hexagrid(deg = x)`, is roughly equivalent to longitudinal degrees, so that a degree of 1 is roughly equal to 111 km. This selects a tessellation vector, which translates to the amount of area you select for each cell. In our specified grid, each cell is roughly 629,000 km\\^2 and results in a grid of 812 cells.\n\n```{r, message=FALSE}\n# Use this chunk of code if you are processing the raw dataset yourself. Otherwise, skip it!\n\npbdb.2before <- pbdb |> filter(interval.ma==445.2)\npbdb.2after <- pbdb |> filter(interval.ma==443.8)\npbdb.2after2 <- pbdb |> filter(interval.ma == 440.8)\n\n# Find raw locations for each stage:\ncoords.before <- subset(pbdb.2before, select = c(paleolng, paleolat)) \ncoords.before <- coords.before |>  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\ncoords.after<- subset(pbdb.2after, select = c(paleolng, paleolat)) \ncoords.after <- coords.after |>  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\ncoords.after2<- subset(pbdb.2after2, select = c(paleolng, paleolat)) \ncoords.after2<- coords.after2 |>  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\n\n# Set up the grid\nhexa <- hexagrid(deg= 4.5, sf=TRUE) #each deg = ~111 km\nhexa\n```\n\n```{r, message=FALSE}\n# Find cell locations for each occurrence\ncells.before <-locate(hexa, coords.before) \n# str(cells.before) #to see which cells have occ's\ncells.after <-locate(hexa, coords.after)\n#str(cells.after)\ncells.after2 <-locate(hexa, coords.after2)\n\n\n# Next add cells df to coords df in order to match cells with their coordinates:\ncoords.before$cell <- cells.before \nnames(coords.before) <- c(\"long\", \"lat\", \"cell\")\ncoords.after$cell <- cells.after \nnames(coords.after) <- c(\"long\", \"lat\", \"cell\")\ncoords.after2$cell <- cells.after2\nnames(coords.after2) <- c(\"long\", \"lat\", \"cell\")\n\ntcells.before <- table(cells.before) #to get no. of occupied cells\n#str(tcells.cha) #get frequency of cell occ's\ntcells.after <- table(cells.after)\n#str(tcells.ind)\ntcells.after2<- table(cells.after2)\n\ndata.2before <- cbind(pbdb.2before, coords.before) #assigns cell number for each occurrence\ndata.2after <- cbind(pbdb.2after, coords.after)\ndata.2after2 <- cbind(pbdb.2after2, coords.after2)\n\n# pbdb <- rbind(data.2before, data.2after, data.2after2) # use this line only if you are processing the raw dataset yourself.\n```\n\n**Grid Plots**\n\nNext, visualize all occurrences for each stage, using the package `rgplates` and `icosa` on R. Please note that this version requires that you have the [GPlates](https://www.earthbyte.org/) software (v.2.5.0 as of writing this script) installed in your computer, as it is the most optimal version of `rgplates`.\n\n```{r, message=FALSE}\n# Call to Gplates offline (requires installed Gplates software)\n\ntd <-tempdir() #temporary directory\n#td\nrgPath <- system.file(package=\"rgplates\")\n#list.files(rgPath) #confirm that this is the correct path\nunzip(file.path(rgPath, \"extdata/paleomap_v3.zip\"), exdir=td)\n#list.files(file.path(td)) #confirm extraction has happened by looking at temporary directory\npathToPolygons <- file.path(td, \"PALEOMAP_PlatePolygons.gpml\") #static plate polygons\npathToRotations <- file.path(td, \"PALEOMAP_PlateModel.rot\")\n\npm <- platemodel(\n  features = c(\"static_polygons\" = pathToPolygons),\n  rotation = pathToRotations\n)\n\n# Plot it out:\nedge <-mapedge() #edge of the map\nplates.lome<- reconstruct(\"static_polygons\", age= 440, model =pm)\nplot(edge, col = \"lightblue2\")\nplot(plates.lome$geometry, col = \"gray60\", border = NA, add = TRUE)\nplot(hexa,  border=\"white\",add = TRUE)\ngridlabs(hexa, cex=0.5) #get labels for each cell, labeled as spiral from North pole of grid\n```\n\n**Before occurrences**\n\nOccurrences in the Before stage, with colors indicating the number of occurrences in occupied cells.\n\n```{r}\n# Before\nplatesMoll <- sf::st_transform(plates.lome, \"ESRI:54009\")\n#^transform plates to Mollweide projection to plot\nplot(hexa, tcells.before, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n```\n\n**After occurrences - Pulse 1**\n\nOccurrences in the After (Pulse 1) stage, with colors indicating the number of occurrences in occupied cells.\n\n```{r}\n# After\nplot(hexa, tcells.after, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n```\n**After occurrences - Pulse 2**\n\nOccurrences in the After (Pulse 2) stage, with colors indicating the number of occurrences in occupied cells.\n```{R, warning =FALSE}\n# After\nplot(hexa, tcells.after2, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n#save as landscape,10*6 \n\n```\n\n## Data pre-processing\n\nWe investigate the data by dividing it by stage and taxonomic class. We determine the number of cells and occurrences for each stage.\n\n```{r, message=FALSE}\n# Data balance.\npbdb |> \n  arrange(early_interval) |> \n  mutate(early_interval = factor(early_interval, levels=c(\"Katian\", \"Hirnantian\", \"Rhuddanian\"))) |> # reorder the intervals\n  group_by(class,early_interval) |> \n  count() |> \n  ggplot(mapping = aes(x = class, y = n, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = NULL, y = \"Sample Size\") +\n  scale_fill_manual(values =  c(\"#FFBF00\",\"#0072B2\",\"#D5006D\",\"#009E73\",\"black\",\"#984EA3\", \"#003F5C\"))+\n  scale_color_manual(values = c(\"#FFBF00\",\"#0072B2\",\"#D5006D\",\"#009E73\", \"black\",\"#984EA3\", \"#003F5C\")) +\n  facet_wrap(.~ early_interval, scales = \"free\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 15, face = \"bold\"),\n        axis.title = element_text(size = 12,face = \"bold\"),\n        axis.text.x = element_text(size = 12, angle=45, hjust=1),\n        axis.text.y= element_text(size=12),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"none\",\n        aspect.ratio = 0.8)\n```\n\n```{r, warning=FALSE}\n# Set min occurrences\nmin_occ <- 15\n\n# Katian cells.\n before_pbdb <-\n  pbdb |> \n  filter(early_interval == \"Katian\")\n\n# before_pbdb <- \n#  before_pbdb |> \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = before_pbdb |> select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# before_pbdb <-\n#  before_pbdb |> \n#  group_by(cell) |>\n#  count() |> \n#  setNames(nm = c(\"cell\",\"occs\")) |> \n#  inner_join(before_pbdb, by = c(\"cell\")) |>\n#  filter(occs >= min_occ)\n\n# Cell centroids.\n# before_centroid <- \n# as.data.frame(centers(hexa))[names(table(before_pbdb$cell)),] |> \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid to master dataframe: Longitude and Latitude.\n# before_pbdb <- \n#  before_pbdb |> \n#  left_join(before_centroid, by = \"cell\")\n\n# after< cells\n after_pbdb <-\n  pbdb |> \n  filter(early_interval == \"Hirnantian\")\n\n# after_pbdb <- \n# after_pbdb |> \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = after_pbdb |> select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# after_pbdb <-\n#  after_pbdb |> \n#  group_by(cell) |>\n#  count() |> \n#  setNames(nm = c(\"cell\",\"occs\")) |> \n#  inner_join(after_pbdb,by = c(\"cell\")) |>\n#  filter(occs >= min_occ)\n\n# Cell centroids\n# after_centroid <- \n#  as.data.frame(centers(hexa))[names(table(after_pbdb$cell)),] |> \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid coordinates to master dataframe.\n# after_pbdb <- \n#  after_pbdb |> \n#  left_join(after_centroid, by = \"cell\")\n\n## After2\n after_pbdb2 <-\n  pbdb |> \n  filter(early_interval == \"Rhuddanian\")\n\n# after_pbdb2 <- \n#  after_pbdb2 |> \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = after_pbdb2|> select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# after_pbdb2 <-\n#  after_pbdb2 |> \n#  group_by(cell) |>\n#  count() |> \n#  setNames(nm = c(\"cell\",\"occs\")) |> \n#  inner_join(after_pbdb2,by = c(\"cell\")) |>\n#  filter(occs >= min_occ)\n\n# Cell centroids\n# after_centroid2 <- \n#  as.data.frame(centers(hexa))[names(table(after_pbdb2$cell)),] |> \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid coordinates to master dataframe.\n# after_pbdb2 <- \n#  after_pbdb2 |> \n#  left_join(after_centroid2, by = \"cell\")\n\n# Combine the two datasets: before & after.\n# The pbdb dataset is has now been fully pre-processed.\n# pbdb <- bind_rows(before_pbdb, after_pbdb, after_pbdb2)\n\n# Create unique identifier for each cell.\n# pbdb <- \n#  data.frame(unique(pbdb$cell)) |> \n#  setNames(nm = \"cell\") |> \n#  mutate(cell_id = c(1:length(cell))) |> \n#  inner_join(pbdb, by = \"cell\")\n\n# Get number of cells for each age\npbdb |> group_by(interval.ma) |> summarise(unique_cells = n_distinct(cell))\n```\n\n```{r, warning=FALSE}\n# Plot number of occurrences per stage and cell.\ncell_text <- \n  data.frame(\n  label = c(\"N = 23 cells\", \"N = 62 cells\", \"N = 15 cells\"),\n  early_interval = c(\"Hirnantian\", \"Katian\", \"Rhuddanian\")\n)\n\n# Plot it\npbdb |> \n   group_by(early_interval,cell) |> \n    count() |>\n mutate(early_interval = factor(early_interval, levels=c(\"Katian\", \"Hirnantian\", \"Rhuddanian\"))) |> # reorder the intervals\n  ggplot(mapping = aes(x = cell, y = n)) + \n  geom_col(col = \"white\", bg = \"#53565A\") +\n  coord_flip() +\n  geom_hline(yintercept = 15, color = \"#B83A4B\") +\n  labs(x = NULL, y = \"Occurrences\") +\n  geom_text(data = cell_text, mapping = aes(x = c(6,12,18), y = 100, label = label),\n            hjust   = -1, vjust = -0.1, size = 3) +\n  facet_wrap(.~ early_interval,scales = \"free\",nrow = 1) +\n  theme_bw() +\n  theme(aspect.ratio = 1.25,\n        axis.text  = element_text(size = 8),\n        axis.title = element_text(face = \"bold\"),\n        strip.text = element_text(face = \"bold\"))\n\n\n\n```\nFor each stage we create individual dataframes based on the cell units and store these into separate lists.\n```{r, warning=FALSE}\n# Data splitting based on cell id and stage.\nbefore_split <-\n  pbdb |> \n  filter(early_interval == \"Katian\") |>\n  group_split(cell_id) |> \n  lapply(as.data.frame)\n\nafter_split <-\n  pbdb |> \n  filter(early_interval == \"Hirnantian\") |>\n  group_split(cell_id) |> \n  lapply(as.data.frame)\n\nafter_split2 <-\n  pbdb |> \n  filter(early_interval == \"Rhuddanian\") |>\n  group_split(cell_id) |> \n  lapply(as.data.frame)\n```\n\n\n## Subsampling by cells and occurrence\n\nHere we perform subsampling without replacement on our stage-level datasets using 99 iterations. For the \"before\" age we randomly sample 15 occurrences per cell and repeated the process as stated above. Conversely, for the \"after\" age, we applied a two-step subsampling procedure by first subsampling down to match the number cells and then by occurrences. The results are subsampled datasets (cell-specific) saved as nested objects within a larger list. These are subsequently, merged into single master dataframes (i.e., the cells) to create one single list containing 99 dataframes.\n\n```{r}\n# after.\nset.seed(31)\n\nboot_after2 <- purrr::map(1:99, ~ {\n  after_split2 |> \n  # Samples rows uniformly.\n purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\nset.seed(3)\n\nboot_after <- purrr::map(1:99, ~ {\n  after_split |> \n  # Samples rows uniformly.\n sample(15, replace = FALSE) |> \n purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\n# before.\nset.seed(4)\n\nboot_before <- purrr::map(1:99, ~ {\n  before_split |> \n  # Step 1. Cells.\n  sample(15, replace = FALSE) |> \n  # Step 2. Rows (i.e., occurrences).\n  purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n```\n\nAs indicated in the previous section, we here combine cell-specific dataframes (N=13) into single joint dataframes (13\\*20 = 260 rows). This is repeated for all 99 sub-sampled dataframes. Worthy of note, the cells in the after list, will inevitably vary between the subsampled datasets, whereas, in the case of the \"before\" age they are all identical. This is because our analysis seeks to assess the impact by cell heterogeneity across geologic stages.\n\n```{r}\n# Before.\ncombined_boot_before <- \n  list()\n\nfor(i in seq_along(boot_before)) {\n  pBe <- purrr::map_dfr(boot_before[[i]], bind_rows)\n  combined_boot_before[[i]] <- pBe\n}\n\n# after.\ncombined_boot_after <- \n  list()\n\nfor(i in seq_along(boot_after)) {\n  pAf <- purrr::map_dfr(boot_after[[i]], bind_rows)\n  combined_boot_after[[i]] <- pAf\n}\n\n# after-2.\ncombined_boot_after2 <- \n  list()\n\nfor(i in seq_along(boot_after2)) {\n  pAf2 <- purrr::map_dfr(boot_after2[[i]], bind_rows)\n  combined_boot_after2[[i]] <- pAf2\n}\n```\n\n## Generic occurrence per cell\n\nFor each subsampled dataset in both the After and Before lists we here count the number of occurrence of each genera by cell. This is done for all dataframes and are then combined into one master dataframe.\n\n```{r message=FALSE}\n# before.\nbefore_count_ls <- \n  purrr::map(combined_boot_before, ~ .x |> group_by(cell,accepted_name) |> summarise(occs = n(), .groups = 'drop')) |> \n  lapply(as.data.frame) |> \n  bind_rows()\n\n# after.\nafter_count_ls <- \n  purrr::map(combined_boot_after, ~ .x |> group_by(cell,accepted_name) |> summarise(occs = n(), .groups = 'drop')) |> \n  lapply(as.data.frame) |> \n  bind_rows()\n\n#after.2\nafter_count_ls2 <- \n  purrr::map(combined_boot_after2, ~ .x |> group_by(cell,accepted_name) |> summarise(occs = n(), .groups = 'drop')) |> \n  lapply(as.data.frame) |> \n  bind_rows()\n```\n\n## Unique cell pairs\n\n```{r}\n# before & after.\ncells_distinct_before <- \n  tibble(unique(before_count_ls$cell)) |> setNames(nm = \"x\") |> \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |> \n  arrange(n_part) |> \n  pull(x)\n\ncells_distinct_after <- \n  tibble(unique(after_count_ls$cell)) |> setNames(nm = \"x\") |> \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |> \n  arrange(n_part) |> \n  pull(x)\n\ncells_distinct_after2 <- \n  tibble(unique(after_count_ls2$cell)) |> setNames(nm = \"x\") |> \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |> \n  arrange(n_part) |> \n  pull(x)\n\n# Distinct cell pairs.\ncells_distinct_pair_before <-\n  expand.grid(cells_distinct_before,cells_distinct_before,stringsAsFactors = F) |> \n  setNames(nm = c(\"x\",\"y\")) |> \n  filter(x<y) |> \n  as_tibble() \n\ncells_distinct_pair_after <-\n  expand.grid(cells_distinct_after,cells_distinct_after,stringsAsFactors = F) |>\n  setNames(nm = c(\"x\",\"y\")) |> \n  filter(x<y) |> \n  as_tibble() \n\ncells_distinct_pair_after2 <-\n  expand.grid(cells_distinct_after2,cells_distinct_after2,stringsAsFactors = F) |>\n  setNames(nm = c(\"x\",\"y\")) |> \n  filter(x<y) |> \n  as_tibble() \n```\n\n## Jaccard indices\n\n**The Jaccard similiary equation** following Miller et al., 2009\n\n$$ J(Cell X, Cell Y) = \\frac{|Cell X \\cap Cell Y|}{|Cell X \\cup Cell Y|} $$\n\n```{r, message=FALSE}\n# before.\nbefore_jaccard <- \n  jaccard_similarity(combined_boot_before)\n\n# after.\nafter_jaccard <- \n  jaccard_similarity(combined_boot_after)\n\n# after2.\nafter_jaccard2 <- \n  jaccard_similarity(combined_boot_after2)\n```\n\n```{r, message=FALSE}\n# Average similarity for each cell-pair and stage.\n\n# before.\nave_before_jaccard <- \n  bind_rows(before_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after_jaccard <- \n  bind_rows(after_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after2\nave_after_jaccard2 <- \n  bind_rows(after_jaccard2) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n## Great circle distance\n\n```{r}\n# colnames(pbdb)[23] <- \"lat\"\n# colnames(pbdb)[22] <- \"long\" \n\n#before\nbefore_res_matrix <- cells_distinct_pair_before |> \n  # X-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,long,lat) |> \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |> \n  # Y-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,x_long,x_lat,long,lat) |> \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |> \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |> \n  as.data.frame()\n\n# after.\nafter_res_matrix <- cells_distinct_pair_after |> \n  # X-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,long,lat) |> \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |> \n  # Y-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,x_long,x_lat,long,lat) |> \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |> \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |> \n  as.data.frame()\n\n# after2.\nafter_res_matrix2 <- cells_distinct_pair_after2 |> \n  # X-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,long,lat) |> \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |> \n  # Y-coordinates\n  left_join(pbdb |> select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |> \n  distinct(x,y,x_long,x_lat,long,lat) |> \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |> \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |> \n  as.data.frame()\n```\n\n## Czekanowski indices\n\n**Czekanowski equation** following Miller et al., 2009\n\n$$ Czekanowski = 2 * \\frac{\\sum \\min(x_{1k}, x_{2k})}{\\sum x_{1k} + \\sum x_{2k}} $$ The occurrence of a given taxa between distinct cells are evaluated against each other.\n\n```{r}\n# before.\n# before.\nbefore_combs <- gridComb(x = before_pbdb,cell = cell, accepted_name = accepted_name) # 13*221 = 2873 rows.\n\n# after.\nafter_combs <- gridComb(x = after_pbdb,cell = cell, accepted_name = accepted_name) # 20*93 = 1860 rows. 190 unique cell pairs (check!)\n\nafter_combs2 <- gridComb(x = after_pbdb2,cell = cell, accepted_name = accepted_name) # 20*93 = 1860 rows. 190 unique cell pairs (check!)\n\n# Next count the occurrence of genera per unique cell. This will also include genera with no occurrence in any given cell (i.e. 0).\n# These are subsequently removed in the next step.\n\ncountGen <- function(combinations, age_lists) {\n  purrr::map(seq_along(age_lists), function(i) {\n    name_counts <- \n      combinations |> \n      left_join(age_lists[[i]] |>  group_by(cell, accepted_name) |> count(), by = c(\"cell\", \"accepted_name\")) |> \n      # Replace NA with 0.\n      replace_na(list(n = 0))\n    return(name_counts)\n  })\n}\n\n# before.\nbefore_genCell <- countGen(combinations = before_combs, age_lists = combined_boot_before)\n# after.\nafter_genCell <- countGen(combinations = after_combs, age_lists = combined_boot_after)\nafter_genCell2 <- countGen(combinations = after_combs2, age_lists = combined_boot_after2)\n\n# Create two identical count dataframes for each pair to join against.\n\n# before.\nbefore_count_lsX <- purrr::map(before_genCell, ~ .x |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nbefore_count_lsY <- purrr::map(before_genCell, ~ .x |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n# after.\nafter_count_lsX <- purrr::map(after_genCell, ~ .x |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nafter_count_lsY <- purrr::map(after_genCell, ~ .x |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n# after.2\nafter_count_lsX2 <- purrr::map(after_genCell2, ~ .x |> rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nafter_count_lsY2<- purrr::map(after_genCell2, ~ .x |> rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n\n\nset.seed(5)\n\n# Merge counts for each cell pair.\n mCount <- function(cell_pairs, X, Y) {\n  \n  purrr::map(1:99, function(i) {\n    # Rename the fields so that it matches.\n    oG <- \n      cell_pairs |> rename(\"cell_x\" = \"x\", \"cell_y\" = \"y\") |> \n      # First join (x)\n      left_join(X[[i]], by = \"cell_x\", relationship = \"many-to-many\") |> \n      # Second join (y) \n      left_join(Y[[i]], by = c(\"cell_y\", \"accepted_name\")) |> \n      select(\"cell_x\", \"cell_y\", \"accepted_name\", \"count_cell_x\", \"count_cell_y\")\n    \n    return(oG)\n  })\n}\n\n# before.\nbefore_joined <- mCount(cell_pairs = cells_distinct_pair_before,X = before_count_lsX, Y = before_count_lsY)\n\n# after.\nafter_joined <- mCount(cell_pairs = cells_distinct_pair_after,X = after_count_lsX, Y = after_count_lsY)\n\n# after.2\nafter_joined2 <- mCount(cell_pairs = cells_distinct_pair_after2,X = after_count_lsX2, Y = after_count_lsY2)\n\n# We then split based on distinct cell pairs. This will creates a nested list with X splits each dataframe i.e. 99. We also remove any genera (i.e. accepted name) were 0 occurrences is recorded between cell pairs.\n# This step also add a new field (the minimum field) which is based on the lowest number occurrences of a particular taxa between two cells.\n\nczekanowski_splits <- function(joined_lists) {\n  \n  purrr::map(1:99, function(i) {\n  oP <- joined_lists[[i]] |>\n    # Remove\n    filter(!(count_cell_x == 0 & count_cell_y == 0)) |> \n    # Compute the minimum value between cell x and cell y (use count variable)\n    mutate(minimum = pmin(count_cell_x, count_cell_y)) |> \n    group_by(cell_x, cell_y) |>  \n    group_split()\n  \n  return(oP)\n  })\n}\n\ncz_before_prep <- czekanowski_splits(before_joined)\ncz_after_prep <- czekanowski_splits(after_joined)\ncz_after_prep2 <- czekanowski_splits(after_joined2)\n\n# Compute the czekanowski index.\nbefore_czekanowski <- vector(mode = \"list\")\nfor(i in seq_along(cz_before_prep)) {\n  cz <- lapply(cz_before_prep[[i]], czekanowski_similarity)\n  before_czekanowski[[i]] <- cz\n}\n\nafter_czekanowski <- vector(mode = \"list\")\nfor(i in seq_along(cz_after_prep)) {\n  cz <- lapply(cz_after_prep[[i]], czekanowski_similarity)\n  after_czekanowski[[i]] <- cz\n}\n\nafter_czekanowski2 <- vector(mode = \"list\")\nfor(i in seq_along(cz_after_prep2)) {\n  cz <- lapply(cz_after_prep2[[i]], czekanowski_similarity)\n  after_czekanowski2[[i]] <- cz\n}\n\n# Cell pairs.\npairs_before <- do.call(\"rbind\",lapply(cz_before_prep[[1]], function(x) x[1:2][1,]))\n\n# Find all pairs in the After\npairs_after <- vector(mode = \"list\")\npairs_after2 <- vector(mode = \"list\")\n\nfor(i in 1:99) {\n  append_cells <- do.call(\"rbind\",lapply(cz_after_prep[[i]], function(x) x[1:2][1,]))\n  pairs_after[[i]] <- append_cells\n}\nfor(i in 1:99) {\n  append_cells <- do.call(\"rbind\",lapply(cz_after_prep2[[i]], function(x) x[1:2][1,]))\n  pairs_after2[[i]] <- append_cells\n}\n\n# Reformat \nbefore_cz_results <- \n  purrr::map(before_czekanowski, ~as.data.frame(unlist(.x)) |> \n               rename(\"cz\" = 1) |>\n               cbind(pairs_before) |> \n               relocate(.after = \"cell_y\",\"cz\") |> \n               rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") )\n\nafter_cz_results <- \n  purrr::map(after_czekanowski, ~as.data.frame(unlist(.x)) |> \n               rename(\"cz\" = 1))\n\nafter_cz_results2 <- \n  purrr::map(after_czekanowski2, ~as.data.frame(unlist(.x)) |> \n               rename(\"cz\" = 1))\n\n# Now bind the cell pairs to the Af\\ter datasets.\nafter_cz_results <- mapply(function(x, y) cbind(y, x), after_cz_results, pairs_after, SIMPLIFY = FALSE)\nafter_cz_results2 <- mapply(function(x, y) cbind(y, x), after_cz_results2, pairs_after2, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell pair\nbefore_czekanowski_dataframe <- bind_rows(before_cz_results) \nafter_czekanowski_dataframe <- bind_rows(after_cz_results)\nafter_czekanowski_dataframe2 <- bind_rows(after_cz_results2)\n```\n\n## Results\n\n```{r}\n# before\nbefore_res_matrix <- \n  before_res_matrix |> \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |> \n  left_join(ave_before_jaccard,by = c(\"x.cell\",\"y.cell\"))\n\n# after\nafter_res_matrix <- \n  after_res_matrix |> \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |> \n  left_join(ave_after_jaccard,by = c(\"x.cell\",\"y.cell\"))\n\n# after2\nafter_res_matrix2 <- \n  after_res_matrix2 |> \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |> \n  left_join(ave_after_jaccard2,by = c(\"x.cell\",\"y.cell\"))\n\n# Bin by distance between cells (GCD in km's)\nbefore_res_matrix$cutdist <- \n  cut(before_res_matrix$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\nafter_res_matrix$cutdist <- \n  cut(after_res_matrix$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\nafter_res_matrix2$cutdist <- \n  cut(after_res_matrix2$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\n# Average and sd for Before.\nsumRes_01 <-\n  before_res_matrix |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm= TRUE),\n    second = quantile(avg_jaccard,probs=0.975, na.rm = TRUE)\n  ) |> \n  mutate(label = 'Before',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Average and sd for the After.\nsumRes_02 <- \n  after_res_matrix |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n     # Quantiles\n    first = quantile(avg_jaccard,probs=0.25),\n    second = quantile(avg_jaccard,probs=0.975)\n  ) |> \n  mutate(label = 'After',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Average and sd for the After 2.\nsumRes_025 <- \n  after_res_matrix2 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n     # Quantiles\n    first = quantile(avg_jaccard,probs=0.25),\n    second = quantile(avg_jaccard,probs=0.95)\n  ) |> \n  mutate(label = 'After-2',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Combine the two.\nsumRes_03 <- bind_rows(sumRes_01,sumRes_02, sumRes_025)\n\n\n# Plot.\nsumRes_03 |> \n   arrange(label) |> \n  mutate(label = factor(label, levels=c(\"Before\", \"After\", \"After-2\"))) |> # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"black\", \"darkorange4\",\"goldenrod3\")) + \n  scale_color_manual(values = c(\"black\",\"darkorange4\",\"goldenrod3\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Jaccard\",\n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\n<!-- Czekanowski -->\n```{r, warning=FALSE}\n# before\nbefore_res_matrix <-\n  before_res_matrix |>\n  left_join(\n    before_czekanowski_dataframe |> #rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |>\n      group_by(x.cell,y.cell) |>\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |>\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n# after\nafter_res_matrix <-\n  after_res_matrix |>\n  left_join(\n    after_czekanowski_dataframe |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |> \n      group_by(x.cell,y.cell) |>\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |>\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n# after2\nafter_res_matrix2 <-\n  after_res_matrix2 |>\n  left_join(\n    after_czekanowski_dataframe2 |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |> \n      group_by(x.cell,y.cell) |>\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |>\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n# Average and standard deviation for Before.\nsumRes_045 <-\n  after_res_matrix2 |>\n  group_by(cutdist) |>\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n        # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |>\n  mutate(label = 'After-2',label = as.factor(label)) |>\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |>\n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\n# Average and standard deviation for Before.\nsumRes_04 <-\n  before_res_matrix |>\n  group_by(cutdist) |>\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n        # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |>\n  mutate(label = 'Before',label = as.factor(label)) |>\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |>\n  as.data.frame() |> suppressWarnings() # This was added to ignore the last observation.\n\nsumRes_05 <-\n  after_res_matrix |>\n  group_by(cutdist) |>\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |>\n  mutate(label = 'After',label = as.factor(label)) |>\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |>\n  as.data.frame() |> suppressWarnings()\n\nsumRes_06 <- bind_rows(sumRes_05, sumRes_04, sumRes_045)\n\n# Plot.\nsumRes_06 |> \n   arrange(label) |> \n  mutate(label = factor(label, levels=c(\"Before\", \"After\", \"After-2\"))) |> # reorder the intervals\n  ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values = c(\"black\",\"darkorange4\", \"goldenrod3\")) + \n  scale_color_manual(values = c(\"black\", \"darkorange4\",\"goldenrod3\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\", \n       title = \"Czekanowski\", \n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"), \n        axis.title = element_text(face = \"bold\"), \n        legend.title = element_text(face = \"bold\"), \n        aspect.ratio = 1)\n```\n\n\n## Sensitivity analysis\n\nSimilarity measurements by survival status.\n\n```{r}\n# Retain occurrences with or greater than 15.\npbdb_sensitivity <- \n  pbdb |> \n  filter(occs >= 15) # 3578 observations.\n\n# Split survival datasets by unique cell id.\n\n#First pulse:\n\n# after survivors\nsAft <- pbdb_sensitivity |> \n  filter(early_interval == \"Hirnantian\" & ex==0 & fad >=443.8) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# before survivors\nsV <- pbdb_sensitivity |>\n  filter(early_interval == \"Katian\" & ex==1) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# after survivors.\nsBef <- pbdb_sensitivity |> \n  filter(early_interval  == \"Katian\" & ex==0) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n## THIS IS FOR ANALYSIS OF SECOND PULSE: \n\n# Survivors After:\nsAftHir <- pbdb_sensitivity |> \n  filter(early_interval == \"Rhuddanian\" & ex==0 & fad >=440.8) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# Victims of Hirnantian pulse.\nsVHir <- pbdb_sensitivity |>\n  filter(early_interval == \"Hirnantian\" & ex==1) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n# Survivors of Hirnantian pulse.\nsBefHir <- pbdb_sensitivity |> \n  filter(early_interval  == \"Hirnantian\" & ex==0) |>\n  group_split(cell_id) |> lapply(as.data.frame)\n\n\n# Subsampling.\nsubsampling_fun <-\n  function(x, n_boot = 99, sample_size = 12, seed = 5) {\n  set.seed(seed)\n  # Samples.\n  boot_samples <- purrr::map(1:n_boot, ~ sample(x, sample_size, replace = FALSE))\n  # Combine cells into single dataframes.\n  comb_samples <- purrr::map(boot_samples, ~ map_dfr(.x, bind_rows))\n  \n  return(comb_samples)\n}\n\n\n# Subsampled data.\nsBef_boot <- subsampling_fun(sBef)\nsV_boot <- subsampling_fun(sV)\nsAft_boot <- subsampling_fun(sAft)\n\n\n# Second pulse.\nsBef_boot_Hir <- subsampling_fun(sBefHir)\nsV_boot_Hir <- subsampling_fun(sVHir)\nsAft_boot_Hir <- subsampling_fun(sAftHir)\n```\n###Jaccard index calculation\n```{r, warning=FALSE}\n# After survivors.\nafter_survivors_jaccard <- jaccard_similarity(sAft_boot)\n# Before victims.\nbefore_victims_jaccard <- jaccard_similarity(sV_boot)\n# Before survivors.\nbefore_survivors_jaccard <- jaccard_similarity(sBef_boot)\n\n\n#Second pulse:\n\n# After survivors.\nafter_survivors_jaccard_Hir <- jaccard_similarity(sAft_boot_Hir)\n# Before victims.\nbefore_victims_jaccard_Hir <- jaccard_similarity(sV_boot_Hir)\n# Before survivors.\nbefore_survivors_jaccard_Hir <- jaccard_similarity(sBef_boot_Hir)\n```\n**Averages**\n```{r, warning=FALSE}\n##First pulse:\n\n# Mean jaccard for the After survivors.\naAftsJ <- \n  bind_rows(after_survivors_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# Mean jaccard for the Before victims.\navJ <- \n  bind_rows(before_victims_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# before survivors.\naBefsJ <- \n  bind_rows(before_survivors_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n## Second pulse:\n\n# Mean jaccard for the After survivors.\naAftsJ_Hir <- \n  bind_rows(after_survivors_jaccard_Hir) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# Mean jaccard for the Before victims.\navJ_Hir <- \n  bind_rows(before_victims_jaccard_Hir) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# before survivors.\naBefsJ_Hir <- \n  bind_rows(before_survivors_jaccard_Hir) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n**Visualize results**\n\n```{r, message=FALSE, warning=FALSE}\n## FIRST PULSE: \n\n# After results: survivors.\nmRes_01 <- after_res_matrix[,c(1:7,10)] |> left_join(aAftsJ,by = c(\"x.cell\",\"y.cell\"))\n# Before results: survivors & victims.\nmRes_02 <- before_res_matrix[,c(1:7,10)] |> left_join(avJ,by = c(\"x.cell\",\"y.cell\"))\nmRes_03 <- before_res_matrix[,c(1:7,10)] |> left_join(aBefsJ,by = c(\"x.cell\",\"y.cell\"))\n\n# SECOND PULSE \nmRes_04 <- after_res_matrix2[,c(1:7,10)] |>  left_join(aAftsJ_Hir,by = c(\"x.cell\",\"y.cell\"))\n# Before results: survivors & victims.\nmRes_05 <- after_res_matrix[,c(1:7,10)] |> left_join(avJ_Hir,by = c(\"x.cell\",\"y.cell\"))\nmRes_06  <- after_res_matrix[,c(1:7,10)] |> left_join(aBefsJ_Hir,by = c(\"x.cell\",\"y.cell\"))\n\n# Summary statistics for each survival category.\n\n\n## FIRST PULSE \n# after survivors.\nsumRes_07 <-\n  mRes_01 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25,na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'After survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame()\n```\n```{r, warning=FALSE}\n# before victims.\nsumRes_08 <-\n  mRes_02 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before victims',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# before survivors.\nsumRes_09 <-\n  mRes_03 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n## SECOND PULSE \n\n# after survivors.\nsumRes_10 <-\n  mRes_04 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25,na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'After survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame()\n\n# before victims.\nsumRes_11  <-\n  mRes_05 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before victims',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# before survivors.\nsumRes_12 <-\n  mRes_06 |> \n  group_by(cutdist) |> \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |> \n  mutate(label = 'Before survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# Combine all results.\nsumRes_13 <- bind_rows(sumRes_07,sumRes_08,sumRes_09) # First pulse\nsumRes_14 <- bind_rows(sumRes_10,sumRes_11,sumRes_12) # Second pulse\n\n# Plot.\nsumRes_13 |> \n  arrange(label) |> \n   mutate(label = factor(label, levels=c(\"Before victims\", \"Before survivors\", \"After survivors\"))) |> # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"gray60\",\"black\",\"goldenrod\")) + \n  scale_color_manual(values = c(\"gray60\",\"black\",\"goldenrod\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"First Pulse\",\n       subtitle = \"Jaccard by survival status\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\n\n```{r, warning=FALSE}\n# Plot for second pulse\nsumRes_14 |> \n  arrange(label) |> \n   mutate(label = factor(label, levels=c(\"Before victims\", \"Before survivors\", \"After survivors\"))) |> # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n scale_fill_manual(values =  c(\"gray60\",\"black\",\"goldenrod\")) + \n  scale_color_manual(values = c(\"gray60\",\"black\",\"goldenrod\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Second Pulse \",\n       subtitle = \"Jaccard by survival status\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\nSetup for the second pulse analysis:\n\n#### Czekanowski index calculation\n\n**After survivors**\n\n```{r}\n# Data table \nprep_Afts <- map(sAft_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_Afts <- map(prep_Afts, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_Afts <- vector(mode = \"list\")\nfor(i in seq_along(prep_Afts)) {\n  cZ <- lapply(prep_Afts[[i]],czekanowski_similarity)\n  cz_Afts[[i]] <- cZ\n}\n\n# Find all pairs.\nsp_01 <- vector(mode = \"list\")\nfor(i in 1:99) {\n  pA <- do.call(\"rbind\",lapply(prep_Afts[[i]], function(x) x[1:2][1,]))\n  sp_01[[i]] <- pA\n}\n\n# Convert to dataframe and unlist.\nflat_Afts <- purrr::map(cz_Afts, ~as.data.frame(unlist(.x)) |> rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_afts <- mapply(function(x, y) cbind(y, x), flat_Afts, sp_01, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_Afts <- bind_rows(append_flat_afts) |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n**Before victims**\n\n```{r, warning=FALSE}\n# Data table \nprep_v <- map(sV_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_v <- map(prep_v, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_v <- vector(mode = \"list\")\nfor(i in seq_along(prep_v)) {\n  cZ <- lapply(prep_v[[i]],czekanowski_similarity)\n  cz_v[[i]] <- cZ\n}\n\n# Find all pairs.\nsp_02 <- vector(mode = \"list\")\nfor(i in 1:99) {\n  pQ <- do.call(\"rbind\",lapply(prep_v[[i]], function(x) x[1:2][1,]))\n  sp_02[[i]] <- pQ\n}\n\n# Convert to dataframe and unlist.\nflat_v <- purrr::map(cz_v, ~as.data.frame(unlist(.x)) |> rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_v <- mapply(function(x, y) cbind(y, x), flat_v, sp_02, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_v <- bind_rows(append_flat_v) |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n**Before survivors**\n\n```{r, warning=FALSE}\n# Data table \nprep_Befs <- map(sBef_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_Befs <- map(prep_Befs, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_Befs <- vector(mode = \"list\")\nfor(i in seq_along(prep_Befs)) {\n  cM <- lapply(prep_Befs[[i]],czekanowski_similarity)\n  cz_Befs[[i]] <- cM\n}\n\n# Find all pairs.\nsp_03 <- vector(mode = \"list\")\nfor(i in 1:99) {\n  pO <- do.call(\"rbind\",lapply(prep_Befs[[i]], function(x) x[1:2][1,]))\n  sp_03[[i]] <- pO\n}\n\n# Convert to dataframe and unlist.\nflat_Befs <- purrr::map(cz_Befs, ~as.data.frame(unlist(.x)) |> rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_Befs <- mapply(function(x, y) cbind(y, x), flat_Befs, sp_03, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_Befs <- bind_rows(append_flat_Befs) |> rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n```{r, warning=FALSE}\n# Averages.\nave_s1 <- res_Afts |> \n  group_by(x.cell,y.cell) |>\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\nave_s2 <- res_v |> \n  group_by(x.cell,y.cell) |>\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\nave_s3 <- res_Befs |> \n  group_by(x.cell,y.cell) |>\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n```\n**Visualize results**\n\n```{r, warning=FALSE}\n# Reverse cell pairs first.\nave_s1_reversed <- ave_s1|> rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\nave_s2_reversed <- ave_s2|> rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\nave_s3_reversed <- ave_s3|> rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\n\n# after results: survivors.\nqRes_01 <- after_res_matrix[,c(1:7,10)] |>\n  inner_join(ave_s1_reversed, by = c(\"x.cell\",\"y.cell\"))\n# before results: survivors & victims.\nqRes_02 <- ave_s2_reversed |> \n  left_join(before_res_matrix[,c(1:7,10)],by = c(\"x.cell\",\"y.cell\"))\n\nqRes_03 <- ave_s3_reversed |> \n  left_join(before_res_matrix[,c(1:7,10)],by = c(\"x.cell\",\"y.cell\"))\n\n# Summary statistics for each survival category.\n\n# After survivors.\nsumRes_15 <-\n  qRes_01 |> \n  group_by(cutdist) |> \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |> \n  mutate(label = 'after survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |>\n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame()\n```\n```{r, warning=FALSE}\n# Before victims.\nsumRes_16 <-\n  qRes_02 |> \n  group_by(cutdist) |> \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |> \n  mutate(label = 'before victims',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# before survivors.\nsumRes_17 <-\n  qRes_03 |> \n  group_by(cutdist) |> \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |> \n  mutate(label = 'before survivors',label = as.factor(label)) |> \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |> \n  mutate(ci = se * qt(.97, n - 1), ci = as.numeric(ci)) |> \n  as.data.frame() |> suppressWarnings()\n\n# Combine all results.\nsumRes_18 <- bind_rows(sumRes_15,sumRes_16,sumRes_17)\n\n# Plot.\nsumRes_18 |> \n ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"goldenrod\",\"gray60\", \"black\")) + \n  scale_color_manual(values = c(\"goldenrod\",\"gray60\", \"black\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Czekanowski by survival status\",\n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n```\n## Extra\n\nCount occurrences per genus in the Hirnantian to determine whether they match the disaster taxa in literature, such as the genus Hirnantia.\n\n```{r extra}\n# Find the top 5 genera based on number of occurrences\npbdb.hir  <- pbdb |> filter(interval.ma==443.8)\n\ngenus_counts <- pbdb.hir |>\n  group_by(accepted_name) |> \n             summarise(count = n(), .groups = 'drop') |> \n             top_n(5, wt = count) |>\n             arrange(desc(count))\n\n# Convert accepted_name to a factor with levels in descending order of count\n\ngenus_counts$accepted_name <- factor(genus_counts$accepted_name, levels = genus_counts$accepted_name[order(genus_counts$count, decreasing = TRUE)])\n\n# Visualize it with switched axes\n\nggplot(genus_counts, aes(x = count, y = accepted_name)) + theme_classic() + geom_bar(stat = \"identity\", fill = \"dodgerblue3\") + labs(x = \"Number of Occurrences\", y = \"Genus Name\")+ coord_flip() # Optional: this can be omitted if you want the horizontal bars\n```\n\n###  Wastebin taxa removed\n\nLocate and remove all wastebin taxa, following the methods of *Plotnick and Wagner (2005)*. First, using the full database, locate all wastebin taxa throughout the \"before\" and \"after\" intervals by finding the 5 most frequent genus occurrences.\n\n```{r wastebin}\noptions(download.file.method = \"libcurl\")\npbdb <- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\n\npbdb <- pbdb[, -c(1, 22, 21)]\n\n\n \n# Get the frequency of each genus, then keep top 20 of them. these are our wastebasket genera\n \npbdb_occs <- pbdb |> \n  group_by(accepted_name) |> \n  summarise(frequency = n()) |> \n  arrange(desc(frequency)) |> \n   slice_head(n = 20) |>  # keep only the top 20 rows\n  arrange(accepted_name) # order alphabetically\n        \n\n# Next, continue with the normal process of getting Jaccard similarity.\n\n# Here is where we remove the wastebin taxa:            \npbdb <- pbdb |>    \n  filter(interval.ma %in% c(\"445.2\", \"443.8\", \"440.8\") &\n       (!accepted_name %in% pbdb_occs$accepted_name)) # particularly, this line\n \n# Identify Invalid Coordinates.\n  cl <- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n```\n\n```{r}\n  cl_rec <- pbdb[!cl,] #extract and check them\n  \n pbdb <- pbdb |> \n   cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n```\n \n \nNext, follow all the steps from **Visualization of Cells and Occurrences** down to **Czekanowski indices** again. You will notice slight changes to the number of cells since filtering out the wastebin taxa reduces the number of occurrences we have. \n\n##Spatial standardization\n\nHere, we will apply the `Divvy` package to spatially standardize our data using the circular \"cookie\" method and re-run our Jaccard calculations with the standarized data.\n\n```{r Divvy, message=FALSE, warning=FALSE}\n library(raster)\npbdb <- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\n\npbdb <- pbdb[, -c(1, 22, 21)]\n\n# Make sure coordinates are numerical\npbdb$paleolat <- as.numeric(pbdb$paleolat)\npbdb$paleolng <- as.numeric(pbdb$paleolng)\n\n# Subset by age\npbdb.before <- pbdb |> filter(early_interval == \"Katian\")\npbdb.after <- pbdb |> filter(early_interval == \"Hirnantian\")\npbdb.after2 <- pbdb |> filter(early_interval == \"Rhuddanian\")\n\n# Get the frequency of each genus, then keep top 20 of them. these are our wastebasket genera\n \n# Initialize equal earth projections and coordinate:\nrWorld <-rast()\nprj <- 'EPSG:8857'\n\n# Match the divvy resolution with our hexa resution from the icosa package.\ndeg <- 4.5 # hexa's resolution\n\n# Approximate meters per degree at the equator\nmeters_per_deg <- 111320  \n\n# Adjust for the latitude of area\nlat_center <- mean(raster::yFromCell(rWorld, 1:ncell(rWorld)))  # rough center latitude\nmeters_res <- deg * meters_per_deg * cos(lat_center * pi/180)\nmeters_res\n```\n\n```{r}\n# Get the new resolution\nnew_res <- meters_res\n\n\n# New_res is the target resolution in meters\nrPrj <- project(rWorld, prj, method = \"bilinear\", res = new_res)\n\n\nterra::values(rPrj) <- 1:ncell(rPrj)\n\n# Coordinate column names for the current and target coordinate reference system\nxyCartes <- c('paleolng','paleolat')\nxyCell   <- c('cellX','cellY')\n```\n\n```{r}\nllOccs <- vect(pbdb.before, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.before$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.before[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.before$cell)\n\nsdSumry(pbdb.before, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()\n```\n\n```{r}\n# Disregard SCOR \n\noccUniq <- uniqify(pbdb.before, xyCell)\nptsUniq <- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\nworldP <- ggplot(data = plates.lome$geometry) +\n  theme_bw() +\n  geom_sf() +\n  geom_sf(data = ptsUniq, shape = 16, color = 'red3')\n\ncircLocs <- cookies(dat = pbdb.before,\n                    xy = xyCell,\n                    iter = 100,\n                    nSite = 5,\n                    r = 2000,\n                    weight = TRUE,\n                    crs = prj,\n                    output = \"full\")\n\n\n# Convert one circular sample to sf\nsmplPts <- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr <- smplPts[1, ]\nr_km <- 2000\nbuf <- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n\n# Assume circLocs is your list of circular subsamples (1000 iterations)\n# We'll just plot the first 5 for clarity\nn_plot <- 3\nset.seed(567)\nsubsamples <- circLocs[1:n_plot]\n\n# Convert world map to sf and match projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n# Create a data frame of points and buffers for all subsamples\nall_points <- list()\nall_buffers <- list()\n\nfor (i in seq_along(subsamples)) {\n  smplPts <- st_as_sf(subsamples[[i]], coords = xyCell, crs = prj)\n  \n  # Pick the first point as center\n  cntr <- smplPts[1, ]\n  buf <- st_buffer(cntr, dist = 2000 * 1000)  # 2000 km in meters\n  \n  smplPts$iteration <- paste0(\"#\", i)\n  buf$iteration <- paste0(\"#\", i)\n  \n  all_points[[i]] <- smplPts\n  all_buffers[[i]] <- buf\n}\n\n# Combine all iterations\nall_points_sf <- do.call(rbind, all_points)\nall_buffers_sf <- do.call(rbind, all_buffers)\n\n\n\n# Plot\nggplot() +\n  geom_sf(data = world_sf, fill = \"gray80\", color = \"white\") +\n    geom_sf(data = ptsUniq, shape = 16, color = 'black')+\n  geom_sf(data = all_buffers_sf, fill = NA, linewidth = 1, color = \"navyblue\") +\n  geom_sf(\n    data = all_points_sf,\n    aes(color = iteration),\n    shape = 16,\n    size = 3,\n  ) +\n  theme_bw() +\n  labs(\n    title = \"Circular subsamples (first 3 iterations)\",\n    subtitle=\"Katian\",\n    x = NULL, y = NULL,\n    color = \"Cells in Iteration\"\n  )\n```\nCircLocs now has 100 lists of subsampled occurrences which can be viewed with str(circLocs[[1]]):\n```{r}\nstr(circLocs[[1]])\n```\n\n```{r}\ncircLocs_before <- circLocs \n```\n\n**After the first pulse, spatially standardized:**\n\n```{r, warning=FALSE}\n# Extract cell number and centroid coordinates associated with each occurrence\n\nllOccs <- vect(pbdb.after, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.after$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.after[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.after$cell)\n\nsdSumry(pbdb.after, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()# Extract cell number and centroid coordinates associated with each occurrence\n\nllOccs <- vect(pbdb.after, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.after$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.after[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.after$cell)\n\nsdSumry(pbdb.after, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()\n```\n\n```{r}\n# Disregard SCOR \n\noccUniq <- uniqify(pbdb.after, xyCell)\nptsUniq <- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\n# Circular subsampling technique\n\ncircLocsAft <- cookies(dat = pbdb.after,\n                    xy = xyCell,\n                    iter = 100,\n                    nSite = 5,\n                    r = 2000,\n                    weight = TRUE,\n                    crs = prj,\n                    output = \"full\")\n\n\n# Convert one circular sample to sf\nsmplPts <- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr <- smplPts[1, ]\nr_km <- 2000\nbuf <- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n# worldP +\n#  geom_sf(data = smplPts, shape = 17, color = 'navyblue') +\n#  geom_sf(data = buf, fill = NA, linewidth = 1, color = 'navyblue')\n```\nNow, for the same thing with the “after-2” age for the second pulse.\n\n**After the second pulse, spatially standardized:**\n\n```{r}\n\nllOccs <- vect(pbdb.after2, geom = xyCartes, crs = 'epsg:4326')\nprjOccs <- terra::project(llOccs, prj)\npbdb.after2$cell <- cells(rPrj, prjOccs)[,'cell']\npbdb.after2[, xyCell] <- xyFromCell(rPrj, \n                                pbdb.after2$cell)\n\nsdSumry(pbdb.after2, taxVar = 'accepted_name', xy = xyCell, crs = prj) |> \n  print()\n\n```\n\n```{r}\n# Disregard SCOR \n\noccUniq <- uniqify(pbdb.after2, xyCell)\nptsUniq <- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\ncircLocsAft2 <- cookies(dat = pbdb.after2,\n                    xy = xyCell,\n                    iter = 1000, # number of iterations\n                    nSite = 5, # number of cells\n                    r = 2000, # radial distance in km\n                    weight = TRUE, \n                    crs = prj, # Equal Earth projection\n                    output = 'full')\n\n\n# Convert one circular sample to sf\nsmplPts <- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr <- smplPts[1, ]\nr_km <- 2000\nbuf <- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf <- st_as_sf(plates.lome) |> st_transform(crs = prj)\n\n# worldP +\n#  geom_sf(data = smplPts, shape = 17, color = 'navyblue') +\n#  geom_sf(data = buf, fill = NA, linewidth = 1, color = 'navyblue')\n``` \n\nNext, we will check to see how many InF values we have in `sdSumry$SCOR`. This won't matter for most interval-pairs, but for the P-T and LOME intervals in which homogenization does occur, it's a sanity check to understand why the SCOR does not reflect homogenization -- taxon that are present in all cells analyzed here are given an InF score instead of a numerical value and are not included in the SCOR calculation. As per the [Divvy vignette](https://gawainantell.github.io/divvy/articles/habitat-rangesize-case-study.html) walkthrough: \"When a taxon is present in all sampled locations, its log probability of incidence is infinite. Infinity is nonsensical in an empirical comparison (...).\"\n\nSince we won't use SCOR beyond this next chunk of code, the SCOR value will not affect our Jaccard calculations even for homogenized intervals, so we can still spatially standardize the data and see how that effects the Jaccard values.\n```{r}\nsdbef <- sdSumry(circLocs, taxVar = 'genus', xy = xyCell, crs = prj)\nsdaft <- sdSumry(circLocsAft, taxVar = 'genus', xy = xyCell, crs = prj) #tons of InF scores here in PT and LOME indicating widespread occurrences and homogenization has occurred.\nsdaft2 <- sdSumry(circLocsAft2, taxVar = 'genus', xy = xyCell, crs = prj) \n\nis.infinite(sdaft$SCOR) # occurrences that are everywhere are labeled InF. We need to change this to 100 to reflect that they are everywhere, but I don't know how to do that. So, ignore SCOR for now.\n```\n\n```{r}\n# before\nsdbef_inf <- lapply(sdbef, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdbef_mean <- mean(sdbef_inf$SCOR)\n\n\n# after:\nsdaft_inf <- lapply(sdaft, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdaft_mean <- mean(sdaft_inf$SCOR)\n\n\n# after-2:\nsdaft2_inf <- lapply(sdaft2, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdaf2t_mean <- mean(sdaft2_inf$SCOR)\n```\n\nNow, we will calculate the spatially standardized Jaccard similarity for each geologic age:\n\n```{r} \n# before.\nbefore_jaccard <- \n  jaccard_similarity(circLocs)\n\n# after.\nafter_jaccard <- \n  jaccard_similarity(circLocsAft)\n\n# after.\nafter2_jaccard <- \n  jaccard_similarity(circLocsAft2)\n\n\n#before\nave_before_jaccard <- \n  bind_rows(before_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after_jaccard <- \n  bind_rows(after_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after2_jaccard <- \n  bind_rows(after2_jaccard) |>\n  group_by(cell_x, cell_y) |> \n  summarise(avg_jaccard = mean(jaccard_similarity)) |> \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n```\n\n\n##Spatially Standardized Jaccard for the LOME\n```{r}\n# this function will get mean and 95% confidence intervals \nmean_ci <- function(x, conf = 0.95) {\n  x <- as.numeric(x)\n  x <- x[!is.na(x)]\n  n <- length(x)\n  if (n == 0) return(c(mean = NA, lower = NA, upper = NA))\n  m <- mean(x)\n  se <- sd(x) / sqrt(n)\n  t_crit <- qt(1 - (1 - conf)/2, df = n - 1)\n  ci <- t_crit * se\n  c(mean = m, lower = m - ci, upper = m + ci)\n}\n\n\ndivvy_before  <- ave_before_jaccard |>  mutate(age = \"Before\")\ndivvy_after   <- ave_after_jaccard   |> mutate(age = \"Pulse-1\")\ndivvy_after2  <- ave_after2_jaccard |>  mutate(age = \"Pulse-2\")\n\n\n#  add age columns\ndivvy_before  <- divvy_before  |> mutate(age = \"Before\")\ndivvy_after   <- divvy_after  |>  mutate(age = \"After\")\ndivvy_after2  <- divvy_after2  |>  mutate(age = \"After-2\")\n\n# combine all data\ndivvy_combined <- bind_rows(divvy_before, divvy_after, divvy_after2)\n\n# create a unique pair identifier\ndivvy_combined <- divvy_combined |>  mutate(pair = paste(x.cell, y.cell, sep = \"_\"))\n\n\n# get average and 95% ci\nsummary_divvy <- divvy_combined |> \n  group_by(age) |> \n  summarise(\n    mean = mean(avg_jaccard, na.rm = TRUE),\n    lower = mean_ci(avg_jaccard)[\"lower\"],\n    upper = mean_ci(avg_jaccard)[\"upper\"],\n    .groups = \"drop\"\n  )\n\n# rename so it's in order\nsummary_divvy$age <- c(\"Pulse-1 (Hirnantian)\", \"Pulse-2 (Rhuddanian)\",\"Before (Katian)\" )\n\n\n# now plot!\nggplot(summary_divvy, aes(x = age, y = mean, color = age)) +\n  scale_color_manual(values=c(\"black\", \"darkorange4\",\"goldenrod3\"))+\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n  theme_minimal(base_size = 14) +\n  theme_classic()+\n  labs(\n    title = \"Spatially Standardized Global Jaccard (with 95th and 5th percentile)\",\n    x = \"Age\",\n    y = \"Mean Jaccard Similarity\"\n  ) +\n  theme(legend.position = \"none\")\n```\nAs a final note, what we just analyzed is the average global Jaccard value of the spatially standardized data. This plot does not separate similarity as a function of distance the way the main calculations do. So, that one number is a global value after the late Ordovician mass extinction after spatially subsampling 100 times using the cookie method, which agrees with our Jaccard calculations beforehand. What we calculated in `Results` beforehand also shows that most of the lowered similarity is from cells that are farther apart from one another and that, for this interval, similarity can vary regionally. By using both methods, we can account for regional differences and for differences in the dispersion of cells.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":false,"reference-location":"margin","highlight-style":"pygments","html-math-method":"katex","embed-resources":true,"output-file":"02_AllCalcs.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","title":"All Similarity Calculations","title-block-banner":"#014240","title-block-banner-color":"#FFFFFF","subtitle":"Synergy of severe climate change and extinctions result in taxonomic homogenization in deep yime","date":"last-modified","date-format":"YYYY-MM-DD","mainfont":"Figtree","sansfont":"Figtree","footnotes-hover":true,"crossref":{"chapters":true},"author":[{"name":"anonymous","url":"https://github.com/Anonymous-paleobio"}],"fig-cap-location":"top","editor_options":{"chunk_output_type":"console"},"toc-expand":5,"toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}