[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taxonomic Homogenization Deep Time",
    "section": "",
    "text": "Workflow\nThis book documents the full workflow used for all analyses in Synergy of severe climate change and extinctions result in taxonomic homogenization in Deep Time including data acquisition, processing,statistical analyses, and figure generation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#workflow",
    "href": "index.html#workflow",
    "title": "Taxonomic Homogenization Deep Time",
    "section": "",
    "text": "Download PBDB data\nPerform all calculations\nGenerate Figure 3 (differences)\nGenerate Figure 2 (stripes)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html",
    "href": "02_AllCalcs.html",
    "title": "All Similarity Calculations",
    "section": "",
    "text": "Introduction\nCode compiled and curated by anonymous.\nThis code is used to calculate the “Before” and “After” similarity values for each interval-set, and can be customized for an interval and/or data of your choosing. The calculations for all intervals in our analysis are nearly identical by using this script, with the exception of differences in the interval names and ages. Here, we provide an example using the calculations for the Late Ordovician mass extinction.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#libraries",
    "href": "02_AllCalcs.html#libraries",
    "title": "All Similarity Calculations",
    "section": "Libraries",
    "text": "Libraries\n\n\nCode\nrpkg &lt;- c(\"dplyr ggplot2 readr boot divvy terra divDyn conflicted piggyback CoordinateCleaner fossilbrush rgplates icosa tidyr tibble readr purrr downloadthis ggpubr\")\n\nimport_pkg &lt;- function(x)\n  x |&gt; trimws() |&gt; strsplit(\"\\\\s+\")  |&gt; unlist() |&gt; \n  lapply(function(x) library(x, character.only = T)) |&gt; \n  invisible()\n\nrpkg |&gt; import_pkg()\n\n# Resolve conflicted functions.\nconflicted::conflict_prefer(name = \"filter\", winner = \"dplyr\",losers = \"stats\")\n\n# Test",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#custom-functions",
    "href": "02_AllCalcs.html#custom-functions",
    "title": "All Similarity Calculations",
    "section": "Custom functions",
    "text": "Custom functions\nMost of the functions we created for this script are stored here.\n\n\nCode\n#' @return calculate great circle distance in kilometers (km).\n#' @param R Earth mean radius (km)\n#' @param long1.r convert from degrees to radians for latitudes and longitudes.\n#' @export\n\ngcd.slc &lt;- function(long1, lat1, long2, lat2) {\n  R &lt;- 6371\n  long1.r &lt;- long1*pi/180\n  long2.r &lt;- long2*pi/180\n  lat1.r &lt;- lat1*pi/180\n  lat2.r &lt;- lat2*pi/180\n  d &lt;- acos(sin(lat1.r)*sin(lat2.r) + cos(lat1.r)*cos(lat2.r) * cos(long2.r-long1.r)) * R\n  return(d) \n  }\n\n# Return calculate jaccard similarity coefficient\n\njaccard_similarity &lt;- function(x) {\n  js_table &lt;- list()\n  for (k in seq_along(x)) {\n  \n  # Unique cells.\n  unique_cells &lt;- unique(x[[k]]$cell)\n  jaccard_similarity_table &lt;- data.frame(cell_x = character(), cell_y = character(), jaccard_similarity = numeric(), stringsAsFactors = F)\n  \n  for (i in 1:length(unique_cells)) {\n    cell_x &lt;- unique_cells[i]\n    # Cell_x\n    unique_names_cell_x &lt;- unique(x[[k]]$accepted_name[x[[k]]$cell == cell_x])\n    \n    for (j in 1:length(unique_cells)) {\n      cell_y &lt;- unique_cells[j]\n      \n      # Duplicate comparisons.\n      if (cell_x == cell_y || cell_x &gt; cell_y) {\n        next\n      }\n      \n      # Cell_y\n      unique_names_cell_y &lt;- unique(x[[k]]$accepted_name[x[[k]]$cell == cell_y])\n      # Intersections.\n      intersection &lt;- length(generics::intersect(unique_names_cell_x, unique_names_cell_y))\n      Un &lt;- length(generics::union(unique_names_cell_x, unique_names_cell_y))\n      jaccard_similarity &lt;- intersection/Un\n      # Combine results.\n      jaccard_similarity_table &lt;- rbind(jaccard_similarity_table, data.frame(cell_x = cell_x, cell_y = cell_y, jaccard_similarity = jaccard_similarity))\n    }\n  }\n  \n  # Results.\n  js_table[[k]] &lt;- jaccard_similarity_table \n  }\n  return(js_table)\n}\n\n# Calculate jaccard similarity coefficient\nczekanowski_similarity &lt;- function(x) {\n  2*abs(sum(x$minimum))/((sum(x$count_cell_x) + sum(x$count_cell_y)))\n}\n\n# Cross-join function.\ngridComb &lt;- function(x, cell, accepted_name) {\n  cA &lt;- expand.grid(cell = unique(x$cell), unique(x$accepted_name)) |&gt; setNames(nm = c(\"cell\",\"accepted_name\"))\n  return(cA)\n}\n\n# Count taxon occurrence per unique cell combination.\nczekanowski_data_prep &lt;- function(x, cell, accepted_name) {  \n  \n  count_taxa_x &lt;- x |&gt; \n    group_by(cell, accepted_name) |&gt;\n    summarize(count = n(), .groups = 'drop') |&gt; rename(\"cell_x\" = \"cell\", \"count_cell_x\" =\"count\")\n  \n  count_taxa_y &lt;- x |&gt; \n    group_by(cell, accepted_name) |&gt;\n    summarize(count = n(), .groups = 'drop') |&gt; rename(\"cell_y\" = \"cell\", \"count_cell_y\" =\"count\")\n\n  # Cell pairs.\n  cell &lt;- unique(x[[cell]])\n  taxa &lt;- unique(x[[accepted_name]])\n  \n  cell_combinations &lt;- expand.grid(cell_x = cell, cell_y = cell,accepted_name = taxa) |&gt;  filter(cell_x != cell_y)\n  \n  result &lt;- cell_combinations |&gt; \n    left_join(count_taxa_x, by = c(\"cell_x\",\"accepted_name\"), relationship = \"many-to-many\") |&gt; \n    # Second join (y) \n    left_join(count_taxa_y, by = c(\"cell_y\", \"accepted_name\")) |&gt; \n    select(\"cell_x\", \"cell_y\", \"accepted_name\", \"count_cell_x\", \"count_cell_y\") |&gt;\n    # Replace NA with 0\n    replace_na(replace = list(count_cell_x = 0, count_cell_y = 0)) |&gt; \n    # Remove rows that at 0 in both count fields.\n    filter(!(count_cell_x == 0 & count_cell_y == 0)) |&gt; \n    # Remove duplicated cell combinations\n    filter(cell_x == cell_y | cell_x &gt; cell_y) |&gt; \n    # Split by cell combination\n    group_split(cell_x,cell_y)\n    \n    return(result)\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#paleobiology-database",
    "href": "02_AllCalcs.html#paleobiology-database",
    "title": "All Similarity Calculations",
    "section": "Paleobiology Database",
    "text": "Paleobiology Database\nThe fossil occurrence data analysed in this study was retrieved from the Paleobiology Database on November of 2024. Data pre-processing made use of functions from the fossilbrush and CoordinateCleaner R packages.\nThe following script shows you how we processed the full Phanerozoic Pbdb dataset that was downloaded using the Pbdb_download_new.R script, which is available in our repository.\nHowever, since that requires you to download the data yourself, we have provided the files of Pbdb data used for each interval pair in the Pbdb_data folder and this section of the script can therefore be skipped. In the case of the Late Ordovician mass extinction, it is labelled pbdb_lome.csv within that folder. So, you have the option to simply load in that file and skip this section if you so choose!\n\n\nCode\n# Load the csv file directly from our repository: \npbdb &lt;- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\npbdb &lt;- pbdb[, -c(1, 22, 21)]\n\n\n \n #If you want to process the raw Phanerozoic-level dataset from 01 yourself, then follow the rest of this chunk of code instead and remove the hashtags from this section using ctrl (or command) + shift + C.\n \n \n# Read occurrence dataset you generated in step 01. Replace the directory with the one of where you stored it:\n# pbdb &lt;-read.csv(file = '~/Documents/BioHom/pbdb.data.Nov2024.csv')\n\n \n# # Adjust radiometric ages\n# interval.ma    &lt;- pbdb |&gt; \n#   group_by(early_interval) |&gt; \n#  summarise(min_ma = min(min_ma))\n# names(interval.ma) &lt;-c(\"early_interval\", \"interval.ma\")\n# pbdb       &lt;- merge(pbdb, interval.ma, by=c(\"early_interval\"))\n# \n# # Find first and last occurrences and merge back into data frame, using min_ma column\n# fadlad &lt;- pbdb |&gt; \n#   group_by(accepted_name)  |&gt; \n#   summarise(\n#     fad = max(interval.ma),\n#     lad = min(interval.ma)\n#   )\n# \n# # Merge fad and lad information into data frame\n# pbdb &lt;- merge(pbdb, fadlad, by=c(\"accepted_name\"))\n# \n# # Add extinction/survivor binary variable\n# pbdb$ex &lt;- 0\n# pbdb$ex[pbdb$interval.ma==pbdb$lad] &lt;- 1\n# \n# # Select variables.\n# pbdb &lt;- pbdb |&gt; \n#   select(any_of(c(\"interval.ma\",\"early_interval\",\"interval.ma\",\"fad\",\"lad\",\n#                   \"accepted_name\",\"genus\",\"ex\",\"phylum\",\"class\",\n#                   \"order\",\"family\",\"paleolat\",\"paleolng\",\"formation\",\"member\",\n#                   \"occurrence_no\",\"collection_no\",\"collection_name\",\n#                   \"reference_no\")))\n# \n# # Keep two classes and select the age-pair you want.\n#  pbdb &lt;- pbdb |&gt; \n#      filter(class %in% c(\"Gastropoda\", \"Bivalvia\", \"Trilobita\", \"Rhynchonellata\", \"Strophomenata\", \"Anthozoa\") &\n#     interval.ma %in% c(\"445.2\", \"443.8\", \"440.8\"))\n#  \n# # Identify Invalid Coordinates.\n#   cl &lt;- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n#   cl_rec &lt;- pbdb[!cl,] #extract and check them\n#   \n#  pbdb &lt;- pbdb |&gt; \n#    cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n#  \n# # Use fossilbrush to clean taxonomic errors\n# b_ranks &lt;- c(\"phylum\", \"class\", \"order\", \"family\", \"accepted_name\") #accepted_name is genus name\n# \n# # Define a list of suffixes to be used at each taxonomic level when scanning for synonyms\n# b_suff = list(NULL, NULL, NULL, NULL, c(\"ina\", \"ella\", \"etta\"))\n# \n# pbdb2 &lt;- check_taxonomy(pbdb, suff_set = b_suff, ranks = b_ranks, verbose = FALSE,clean_name = TRUE, resolve_duplicates = TRUE, jump = 5)\n# # resolves homonyms, and jump refers to which taxonomic rank is the highest we resolve to. jump = 5 will stop before phylum since phylum level usually has little error.\n# \n# # Extract PBDB data from obdb2 so we have the corrected taxa:\n# pbdb &lt;- pbdb2$data[1:nrow(pbdb),]\n# \n# pbdb_fulldata &lt;- pbdb # keep a record of all pertinent information, just in case",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#visualization-of-cells-and-occurrences",
    "href": "02_AllCalcs.html#visualization-of-cells-and-occurrences",
    "title": "All Similarity Calculations",
    "section": "Visualization of Cells and Occurrences",
    "text": "Visualization of Cells and Occurrences\nThe globe is divided into a grid of equal-area icosahedral hexagonal cells using the hexagrid() function in icosa. In hexagrid(deg = x), is roughly equivalent to longitudinal degrees, so that a degree of 1 is roughly equal to 111 km. This selects a tessellation vector, which translates to the amount of area you select for each cell. In our specified grid, each cell is roughly 629,000 km^2 and results in a grid of 812 cells.\n\n\nCode\n# Use this chunk of code if you are processing the raw dataset yourself. Otherwise, skip it!\n\npbdb.2before &lt;- pbdb |&gt; filter(interval.ma==445.2)\npbdb.2after &lt;- pbdb |&gt; filter(interval.ma==443.8)\npbdb.2after2 &lt;- pbdb |&gt; filter(interval.ma == 440.8)\n\n# Find raw locations for each stage:\ncoords.before &lt;- subset(pbdb.2before, select = c(paleolng, paleolat)) \ncoords.before &lt;- coords.before |&gt;  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\ncoords.after&lt;- subset(pbdb.2after, select = c(paleolng, paleolat)) \ncoords.after &lt;- coords.after |&gt;  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\ncoords.after2&lt;- subset(pbdb.2after2, select = c(paleolng, paleolat)) \ncoords.after2&lt;- coords.after2 |&gt;  \n                          mutate_at(c('paleolng', 'paleolat'), as.numeric)\n\n# Set up the grid\nhexa &lt;- hexagrid(deg= 4.5, sf=TRUE) #each deg = ~111 km\nhexa\n\n\nA/An hexagrid object with 1620 vertices, 2430 edges and 812 faces.\nThe mean grid edge length is 493.6 km or 4.44 degrees.\nUse plot3d() to see a 3d render.\n\n\n\n\nCode\n# Find cell locations for each occurrence\ncells.before &lt;-locate(hexa, coords.before) \n# str(cells.before) #to see which cells have occ's\ncells.after &lt;-locate(hexa, coords.after)\n#str(cells.after)\ncells.after2 &lt;-locate(hexa, coords.after2)\n\n\n# Next add cells df to coords df in order to match cells with their coordinates:\ncoords.before$cell &lt;- cells.before \nnames(coords.before) &lt;- c(\"long\", \"lat\", \"cell\")\ncoords.after$cell &lt;- cells.after \nnames(coords.after) &lt;- c(\"long\", \"lat\", \"cell\")\ncoords.after2$cell &lt;- cells.after2\nnames(coords.after2) &lt;- c(\"long\", \"lat\", \"cell\")\n\ntcells.before &lt;- table(cells.before) #to get no. of occupied cells\n#str(tcells.cha) #get frequency of cell occ's\ntcells.after &lt;- table(cells.after)\n#str(tcells.ind)\ntcells.after2&lt;- table(cells.after2)\n\ndata.2before &lt;- cbind(pbdb.2before, coords.before) #assigns cell number for each occurrence\ndata.2after &lt;- cbind(pbdb.2after, coords.after)\ndata.2after2 &lt;- cbind(pbdb.2after2, coords.after2)\n\n# pbdb &lt;- rbind(data.2before, data.2after, data.2after2) # use this line only if you are processing the raw dataset yourself.\n\n\nGrid Plots\nNext, visualize all occurrences for each stage, using the package rgplates and icosa on R. Please note that this version requires that you have the GPlates software (v.2.5.0 as of writing this script) installed in your computer, as it is the most optimal version of rgplates.\n\n\nCode\n# Call to Gplates offline (requires installed Gplates software)\n\ntd &lt;-tempdir() #temporary directory\n#td\nrgPath &lt;- system.file(package=\"rgplates\")\n#list.files(rgPath) #confirm that this is the correct path\nunzip(file.path(rgPath, \"extdata/paleomap_v3.zip\"), exdir=td)\n#list.files(file.path(td)) #confirm extraction has happened by looking at temporary directory\npathToPolygons &lt;- file.path(td, \"PALEOMAP_PlatePolygons.gpml\") #static plate polygons\npathToRotations &lt;- file.path(td, \"PALEOMAP_PlateModel.rot\")\n\npm &lt;- platemodel(\n  features = c(\"static_polygons\" = pathToPolygons),\n  rotation = pathToRotations\n)\n\n# Plot it out:\nedge &lt;-mapedge() #edge of the map\nplates.lome&lt;- reconstruct(\"static_polygons\", age= 440, model =pm)\nplot(edge, col = \"lightblue2\")\nplot(plates.lome$geometry, col = \"gray60\", border = NA, add = TRUE)\nplot(hexa,  border=\"white\",add = TRUE)\ngridlabs(hexa, cex=0.5) #get labels for each cell, labeled as spiral from North pole of grid\n\n\n\n\n\n\n\n\n\nBefore occurrences\nOccurrences in the Before stage, with colors indicating the number of occurrences in occupied cells.\n\n\nCode\n# Before\nplatesMoll &lt;- sf::st_transform(plates.lome, \"ESRI:54009\")\n#^transform plates to Mollweide projection to plot\nplot(hexa, tcells.before, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n\n\n\n\n\n\n\n\nAfter occurrences - Pulse 1\nOccurrences in the After (Pulse 1) stage, with colors indicating the number of occurrences in occupied cells.\n\n\nCode\n# After\nplot(hexa, tcells.after, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n\n\n\n\n\n\n\n\nAfter occurrences - Pulse 2\nOccurrences in the After (Pulse 2) stage, with colors indicating the number of occurrences in occupied cells.\n\n\nCode\n# After\nplot(hexa, tcells.after2, \n     crs =\"ESRI:54009\",\n     border = \"lightgrey\",\n     pal=c(\"#440154ff\",\"darkorchid2\",\"deepskyblue\",\"royalblue\", \"goldenrod\"),\n     breaks = c(0, 20, 100, 500, 1000, 2000),\n     reset=FALSE)\nplot(platesMoll$geometry, add = TRUE,border= NA, col = \"#66666688\")\n\n\n\n\n\n\n\n\n\nCode\n#save as landscape,10*6",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#data-pre-processing",
    "href": "02_AllCalcs.html#data-pre-processing",
    "title": "All Similarity Calculations",
    "section": "Data pre-processing",
    "text": "Data pre-processing\nWe investigate the data by dividing it by stage and taxonomic class. We determine the number of cells and occurrences for each stage.\n\n\nCode\n# Data balance.\npbdb |&gt; \n  arrange(early_interval) |&gt; \n  mutate(early_interval = factor(early_interval, levels=c(\"Katian\", \"Hirnantian\", \"Rhuddanian\"))) |&gt; # reorder the intervals\n  group_by(class,early_interval) |&gt; \n  count() |&gt; \n  ggplot(mapping = aes(x = class, y = n, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = NULL, y = \"Sample Size\") +\n  scale_fill_manual(values =  c(\"#FFBF00\",\"#0072B2\",\"#D5006D\",\"#009E73\",\"black\",\"#984EA3\", \"#003F5C\"))+\n  scale_color_manual(values = c(\"#FFBF00\",\"#0072B2\",\"#D5006D\",\"#009E73\", \"black\",\"#984EA3\", \"#003F5C\")) +\n  facet_wrap(.~ early_interval, scales = \"free\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 15, face = \"bold\"),\n        axis.title = element_text(size = 12,face = \"bold\"),\n        axis.text.x = element_text(size = 12, angle=45, hjust=1),\n        axis.text.y= element_text(size=12),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"none\",\n        aspect.ratio = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Set min occurrences\nmin_occ &lt;- 15\n\n# Katian cells.\n before_pbdb &lt;-\n  pbdb |&gt; \n  filter(early_interval == \"Katian\")\n\n# before_pbdb &lt;- \n#  before_pbdb |&gt; \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = before_pbdb |&gt; select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# before_pbdb &lt;-\n#  before_pbdb |&gt; \n#  group_by(cell) |&gt;\n#  count() |&gt; \n#  setNames(nm = c(\"cell\",\"occs\")) |&gt; \n#  inner_join(before_pbdb, by = c(\"cell\")) |&gt;\n#  filter(occs &gt;= min_occ)\n\n# Cell centroids.\n# before_centroid &lt;- \n# as.data.frame(centers(hexa))[names(table(before_pbdb$cell)),] |&gt; \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid to master dataframe: Longitude and Latitude.\n# before_pbdb &lt;- \n#  before_pbdb |&gt; \n#  left_join(before_centroid, by = \"cell\")\n\n# after&lt; cells\n after_pbdb &lt;-\n  pbdb |&gt; \n  filter(early_interval == \"Hirnantian\")\n\n# after_pbdb &lt;- \n# after_pbdb |&gt; \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = after_pbdb |&gt; select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# after_pbdb &lt;-\n#  after_pbdb |&gt; \n#  group_by(cell) |&gt;\n#  count() |&gt; \n#  setNames(nm = c(\"cell\",\"occs\")) |&gt; \n#  inner_join(after_pbdb,by = c(\"cell\")) |&gt;\n#  filter(occs &gt;= min_occ)\n\n# Cell centroids\n# after_centroid &lt;- \n#  as.data.frame(centers(hexa))[names(table(after_pbdb$cell)),] |&gt; \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid coordinates to master dataframe.\n# after_pbdb &lt;- \n#  after_pbdb |&gt; \n#  left_join(after_centroid, by = \"cell\")\n\n## After2\n after_pbdb2 &lt;-\n  pbdb |&gt; \n  filter(early_interval == \"Rhuddanian\")\n\n# after_pbdb2 &lt;- \n#  after_pbdb2 |&gt; \n  # Cells.\n#  mutate(cell = locate(x = hexa,y = after_pbdb2|&gt; select(\"paleolng\", \"paleolat\")))\n\n# Count occurrences per cell and filter by minimum occurrence.\n# after_pbdb2 &lt;-\n#  after_pbdb2 |&gt; \n#  group_by(cell) |&gt;\n#  count() |&gt; \n#  setNames(nm = c(\"cell\",\"occs\")) |&gt; \n#  inner_join(after_pbdb2,by = c(\"cell\")) |&gt;\n#  filter(occs &gt;= min_occ)\n\n# Cell centroids\n# after_centroid2 &lt;- \n#  as.data.frame(centers(hexa))[names(table(after_pbdb2$cell)),] |&gt; \n#  rownames_to_column(var = \"cell\")\n\n# Add centroid coordinates to master dataframe.\n# after_pbdb2 &lt;- \n#  after_pbdb2 |&gt; \n#  left_join(after_centroid2, by = \"cell\")\n\n# Combine the two datasets: before & after.\n# The pbdb dataset is has now been fully pre-processed.\n# pbdb &lt;- bind_rows(before_pbdb, after_pbdb, after_pbdb2)\n\n# Create unique identifier for each cell.\n# pbdb &lt;- \n#  data.frame(unique(pbdb$cell)) |&gt; \n#  setNames(nm = \"cell\") |&gt; \n#  mutate(cell_id = c(1:length(cell))) |&gt; \n#  inner_join(pbdb, by = \"cell\")\n\n# Get number of cells for each age\npbdb |&gt; group_by(interval.ma) |&gt; summarise(unique_cells = n_distinct(cell))\n\n\n# A tibble: 3 × 2\n  interval.ma unique_cells\n        &lt;dbl&gt;        &lt;int&gt;\n1        441.           15\n2        444.           23\n3        445.           63\n\n\n\n\nCode\n# Plot number of occurrences per stage and cell.\ncell_text &lt;- \n  data.frame(\n  label = c(\"N = 23 cells\", \"N = 62 cells\", \"N = 15 cells\"),\n  early_interval = c(\"Hirnantian\", \"Katian\", \"Rhuddanian\")\n)\n\n# Plot it\npbdb |&gt; \n   group_by(early_interval,cell) |&gt; \n    count() |&gt;\n mutate(early_interval = factor(early_interval, levels=c(\"Katian\", \"Hirnantian\", \"Rhuddanian\"))) |&gt; # reorder the intervals\n  ggplot(mapping = aes(x = cell, y = n)) + \n  geom_col(col = \"white\", bg = \"#53565A\") +\n  coord_flip() +\n  geom_hline(yintercept = 15, color = \"#B83A4B\") +\n  labs(x = NULL, y = \"Occurrences\") +\n  geom_text(data = cell_text, mapping = aes(x = c(6,12,18), y = 100, label = label),\n            hjust   = -1, vjust = -0.1, size = 3) +\n  facet_wrap(.~ early_interval,scales = \"free\",nrow = 1) +\n  theme_bw() +\n  theme(aspect.ratio = 1.25,\n        axis.text  = element_text(size = 8),\n        axis.title = element_text(face = \"bold\"),\n        strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nFor each stage we create individual dataframes based on the cell units and store these into separate lists.\n\n\nCode\n# Data splitting based on cell id and stage.\nbefore_split &lt;-\n  pbdb |&gt; \n  filter(early_interval == \"Katian\") |&gt;\n  group_split(cell_id) |&gt; \n  lapply(as.data.frame)\n\nafter_split &lt;-\n  pbdb |&gt; \n  filter(early_interval == \"Hirnantian\") |&gt;\n  group_split(cell_id) |&gt; \n  lapply(as.data.frame)\n\nafter_split2 &lt;-\n  pbdb |&gt; \n  filter(early_interval == \"Rhuddanian\") |&gt;\n  group_split(cell_id) |&gt; \n  lapply(as.data.frame)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#subsampling-by-cells-and-occurrence",
    "href": "02_AllCalcs.html#subsampling-by-cells-and-occurrence",
    "title": "All Similarity Calculations",
    "section": "Subsampling by cells and occurrence",
    "text": "Subsampling by cells and occurrence\nHere we perform subsampling without replacement on our stage-level datasets using 99 iterations. For the “before” age we randomly sample 15 occurrences per cell and repeated the process as stated above. Conversely, for the “after” age, we applied a two-step subsampling procedure by first subsampling down to match the number cells and then by occurrences. The results are subsampled datasets (cell-specific) saved as nested objects within a larger list. These are subsequently, merged into single master dataframes (i.e., the cells) to create one single list containing 99 dataframes.\n\n\nCode\n# after.\nset.seed(31)\n\nboot_after2 &lt;- purrr::map(1:99, ~ {\n  after_split2 |&gt; \n  # Samples rows uniformly.\n purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\nset.seed(3)\n\nboot_after &lt;- purrr::map(1:99, ~ {\n  after_split |&gt; \n  # Samples rows uniformly.\n sample(15, replace = FALSE) |&gt; \n purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\n# before.\nset.seed(4)\n\nboot_before &lt;- purrr::map(1:99, ~ {\n  before_split |&gt; \n  # Step 1. Cells.\n  sample(15, replace = FALSE) |&gt; \n  # Step 2. Rows (i.e., occurrences).\n  purrr::map(~ sample_n(.x, 15, replace = FALSE))\n  }\n)\n\n\nAs indicated in the previous section, we here combine cell-specific dataframes (N=13) into single joint dataframes (13*20 = 260 rows). This is repeated for all 99 sub-sampled dataframes. Worthy of note, the cells in the after list, will inevitably vary between the subsampled datasets, whereas, in the case of the “before” age they are all identical. This is because our analysis seeks to assess the impact by cell heterogeneity across geologic stages.\n\n\nCode\n# Before.\ncombined_boot_before &lt;- \n  list()\n\nfor(i in seq_along(boot_before)) {\n  pBe &lt;- purrr::map_dfr(boot_before[[i]], bind_rows)\n  combined_boot_before[[i]] &lt;- pBe\n}\n\n# after.\ncombined_boot_after &lt;- \n  list()\n\nfor(i in seq_along(boot_after)) {\n  pAf &lt;- purrr::map_dfr(boot_after[[i]], bind_rows)\n  combined_boot_after[[i]] &lt;- pAf\n}\n\n# after-2.\ncombined_boot_after2 &lt;- \n  list()\n\nfor(i in seq_along(boot_after2)) {\n  pAf2 &lt;- purrr::map_dfr(boot_after2[[i]], bind_rows)\n  combined_boot_after2[[i]] &lt;- pAf2\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#generic-occurrence-per-cell",
    "href": "02_AllCalcs.html#generic-occurrence-per-cell",
    "title": "All Similarity Calculations",
    "section": "Generic occurrence per cell",
    "text": "Generic occurrence per cell\nFor each subsampled dataset in both the After and Before lists we here count the number of occurrence of each genera by cell. This is done for all dataframes and are then combined into one master dataframe.\n\n\nCode\n# before.\nbefore_count_ls &lt;- \n  purrr::map(combined_boot_before, ~ .x |&gt; group_by(cell,accepted_name) |&gt; summarise(occs = n(), .groups = 'drop')) |&gt; \n  lapply(as.data.frame) |&gt; \n  bind_rows()\n\n# after.\nafter_count_ls &lt;- \n  purrr::map(combined_boot_after, ~ .x |&gt; group_by(cell,accepted_name) |&gt; summarise(occs = n(), .groups = 'drop')) |&gt; \n  lapply(as.data.frame) |&gt; \n  bind_rows()\n\n#after.2\nafter_count_ls2 &lt;- \n  purrr::map(combined_boot_after2, ~ .x |&gt; group_by(cell,accepted_name) |&gt; summarise(occs = n(), .groups = 'drop')) |&gt; \n  lapply(as.data.frame) |&gt; \n  bind_rows()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#unique-cell-pairs",
    "href": "02_AllCalcs.html#unique-cell-pairs",
    "title": "All Similarity Calculations",
    "section": "Unique cell pairs",
    "text": "Unique cell pairs\n\n\nCode\n# before & after.\ncells_distinct_before &lt;- \n  tibble(unique(before_count_ls$cell)) |&gt; setNames(nm = \"x\") |&gt; \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |&gt; \n  arrange(n_part) |&gt; \n  pull(x)\n\ncells_distinct_after &lt;- \n  tibble(unique(after_count_ls$cell)) |&gt; setNames(nm = \"x\") |&gt; \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |&gt; \n  arrange(n_part) |&gt; \n  pull(x)\n\ncells_distinct_after2 &lt;- \n  tibble(unique(after_count_ls2$cell)) |&gt; setNames(nm = \"x\") |&gt; \n  mutate(n_part = as.numeric(sub(\"F\", \"\", x))) |&gt; \n  arrange(n_part) |&gt; \n  pull(x)\n\n# Distinct cell pairs.\ncells_distinct_pair_before &lt;-\n  expand.grid(cells_distinct_before,cells_distinct_before,stringsAsFactors = F) |&gt; \n  setNames(nm = c(\"x\",\"y\")) |&gt; \n  filter(x&lt;y) |&gt; \n  as_tibble() \n\ncells_distinct_pair_after &lt;-\n  expand.grid(cells_distinct_after,cells_distinct_after,stringsAsFactors = F) |&gt;\n  setNames(nm = c(\"x\",\"y\")) |&gt; \n  filter(x&lt;y) |&gt; \n  as_tibble() \n\ncells_distinct_pair_after2 &lt;-\n  expand.grid(cells_distinct_after2,cells_distinct_after2,stringsAsFactors = F) |&gt;\n  setNames(nm = c(\"x\",\"y\")) |&gt; \n  filter(x&lt;y) |&gt; \n  as_tibble()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#jaccard-indices",
    "href": "02_AllCalcs.html#jaccard-indices",
    "title": "All Similarity Calculations",
    "section": "Jaccard indices",
    "text": "Jaccard indices\nThe Jaccard similiary equation following Miller et al., 2009\n J(Cell X, Cell Y) = \\frac{|Cell X \\cap Cell Y|}{|Cell X \\cup Cell Y|} \n\n\nCode\n# before.\nbefore_jaccard &lt;- \n  jaccard_similarity(combined_boot_before)\n\n# after.\nafter_jaccard &lt;- \n  jaccard_similarity(combined_boot_after)\n\n# after2.\nafter_jaccard2 &lt;- \n  jaccard_similarity(combined_boot_after2)\n\n\n\n\nCode\n# Average similarity for each cell-pair and stage.\n\n# before.\nave_before_jaccard &lt;- \n  bind_rows(before_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after\nave_after_jaccard &lt;- \n  bind_rows(after_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n# after2\nave_after_jaccard2 &lt;- \n  bind_rows(after_jaccard2) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#great-circle-distance",
    "href": "02_AllCalcs.html#great-circle-distance",
    "title": "All Similarity Calculations",
    "section": "Great circle distance",
    "text": "Great circle distance\n\n\nCode\n# colnames(pbdb)[23] &lt;- \"lat\"\n# colnames(pbdb)[22] &lt;- \"long\" \n\n#before\nbefore_res_matrix &lt;- cells_distinct_pair_before |&gt; \n  # X-coordinates\n  left_join(pbdb |&gt; select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |&gt; \n  distinct(x,y,long,lat) |&gt; \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |&gt; \n  # Y-coordinates\n  left_join(pbdb |&gt; select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |&gt; \n  distinct(x,y,x_long,x_lat,long,lat) |&gt; \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |&gt; \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |&gt; \n  as.data.frame()\n\n# after.\nafter_res_matrix &lt;- cells_distinct_pair_after |&gt; \n  # X-coordinates\n  left_join(pbdb |&gt; select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |&gt; \n  distinct(x,y,long,lat) |&gt; \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |&gt; \n  # Y-coordinates\n  left_join(pbdb |&gt; select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |&gt; \n  distinct(x,y,x_long,x_lat,long,lat) |&gt; \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |&gt; \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |&gt; \n  as.data.frame()\n\n# after2.\nafter_res_matrix2 &lt;- cells_distinct_pair_after2 |&gt; \n  # X-coordinates\n  left_join(pbdb |&gt; select(cell,long,lat), by = c(\"x\" = \"cell\"),relationship = \"many-to-many\") |&gt; \n  distinct(x,y,long,lat) |&gt; \n  rename(\"x_long\" = \"long\",\"x_lat\" = \"lat\") |&gt; \n  # Y-coordinates\n  left_join(pbdb |&gt; select(cell,long,lat), by = c(\"y\" = \"cell\"),relationship = \"many-to-many\") |&gt; \n  distinct(x,y,x_long,x_lat,long,lat) |&gt; \n  # Rename variables.\n  rename(\"y_long\" = \"long\",\"y_lat\" = \"lat\") |&gt; \n  # GCD.\n  mutate(gcd = gcd.slc(long1 = x_long,lat1 = x_lat,long2 = y_long,lat2 = y_lat)) |&gt; \n  as.data.frame()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#czekanowski-indices",
    "href": "02_AllCalcs.html#czekanowski-indices",
    "title": "All Similarity Calculations",
    "section": "Czekanowski indices",
    "text": "Czekanowski indices\nCzekanowski equation following Miller et al., 2009\n Czekanowski = 2 * \\frac{\\sum \\min(x_{1k}, x_{2k})}{\\sum x_{1k} + \\sum x_{2k}}  The occurrence of a given taxa between distinct cells are evaluated against each other.\n\n\nCode\n# before.\n# before.\nbefore_combs &lt;- gridComb(x = before_pbdb,cell = cell, accepted_name = accepted_name) # 13*221 = 2873 rows.\n\n# after.\nafter_combs &lt;- gridComb(x = after_pbdb,cell = cell, accepted_name = accepted_name) # 20*93 = 1860 rows. 190 unique cell pairs (check!)\n\nafter_combs2 &lt;- gridComb(x = after_pbdb2,cell = cell, accepted_name = accepted_name) # 20*93 = 1860 rows. 190 unique cell pairs (check!)\n\n# Next count the occurrence of genera per unique cell. This will also include genera with no occurrence in any given cell (i.e. 0).\n# These are subsequently removed in the next step.\n\ncountGen &lt;- function(combinations, age_lists) {\n  purrr::map(seq_along(age_lists), function(i) {\n    name_counts &lt;- \n      combinations |&gt; \n      left_join(age_lists[[i]] |&gt;  group_by(cell, accepted_name) |&gt; count(), by = c(\"cell\", \"accepted_name\")) |&gt; \n      # Replace NA with 0.\n      replace_na(list(n = 0))\n    return(name_counts)\n  })\n}\n\n# before.\nbefore_genCell &lt;- countGen(combinations = before_combs, age_lists = combined_boot_before)\n# after.\nafter_genCell &lt;- countGen(combinations = after_combs, age_lists = combined_boot_after)\nafter_genCell2 &lt;- countGen(combinations = after_combs2, age_lists = combined_boot_after2)\n\n# Create two identical count dataframes for each pair to join against.\n\n# before.\nbefore_count_lsX &lt;- purrr::map(before_genCell, ~ .x |&gt; rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nbefore_count_lsY &lt;- purrr::map(before_genCell, ~ .x |&gt; rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n# after.\nafter_count_lsX &lt;- purrr::map(after_genCell, ~ .x |&gt; rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nafter_count_lsY &lt;- purrr::map(after_genCell, ~ .x |&gt; rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n# after.2\nafter_count_lsX2 &lt;- purrr::map(after_genCell2, ~ .x |&gt; rename(\"cell_x\" = \"cell\", \"count_cell_x\" = \"n\"))\nafter_count_lsY2&lt;- purrr::map(after_genCell2, ~ .x |&gt; rename(\"cell_y\" = \"cell\", \"count_cell_y\" = \"n\"))\n\n\nset.seed(5)\n\n# Merge counts for each cell pair.\n mCount &lt;- function(cell_pairs, X, Y) {\n  \n  purrr::map(1:99, function(i) {\n    # Rename the fields so that it matches.\n    oG &lt;- \n      cell_pairs |&gt; rename(\"cell_x\" = \"x\", \"cell_y\" = \"y\") |&gt; \n      # First join (x)\n      left_join(X[[i]], by = \"cell_x\", relationship = \"many-to-many\") |&gt; \n      # Second join (y) \n      left_join(Y[[i]], by = c(\"cell_y\", \"accepted_name\")) |&gt; \n      select(\"cell_x\", \"cell_y\", \"accepted_name\", \"count_cell_x\", \"count_cell_y\")\n    \n    return(oG)\n  })\n}\n\n# before.\nbefore_joined &lt;- mCount(cell_pairs = cells_distinct_pair_before,X = before_count_lsX, Y = before_count_lsY)\n\n# after.\nafter_joined &lt;- mCount(cell_pairs = cells_distinct_pair_after,X = after_count_lsX, Y = after_count_lsY)\n\n# after.2\nafter_joined2 &lt;- mCount(cell_pairs = cells_distinct_pair_after2,X = after_count_lsX2, Y = after_count_lsY2)\n\n# We then split based on distinct cell pairs. This will creates a nested list with X splits each dataframe i.e. 99. We also remove any genera (i.e. accepted name) were 0 occurrences is recorded between cell pairs.\n# This step also add a new field (the minimum field) which is based on the lowest number occurrences of a particular taxa between two cells.\n\nczekanowski_splits &lt;- function(joined_lists) {\n  \n  purrr::map(1:99, function(i) {\n  oP &lt;- joined_lists[[i]] |&gt;\n    # Remove\n    filter(!(count_cell_x == 0 & count_cell_y == 0)) |&gt; \n    # Compute the minimum value between cell x and cell y (use count variable)\n    mutate(minimum = pmin(count_cell_x, count_cell_y)) |&gt; \n    group_by(cell_x, cell_y) |&gt;  \n    group_split()\n  \n  return(oP)\n  })\n}\n\ncz_before_prep &lt;- czekanowski_splits(before_joined)\ncz_after_prep &lt;- czekanowski_splits(after_joined)\ncz_after_prep2 &lt;- czekanowski_splits(after_joined2)\n\n# Compute the czekanowski index.\nbefore_czekanowski &lt;- vector(mode = \"list\")\nfor(i in seq_along(cz_before_prep)) {\n  cz &lt;- lapply(cz_before_prep[[i]], czekanowski_similarity)\n  before_czekanowski[[i]] &lt;- cz\n}\n\nafter_czekanowski &lt;- vector(mode = \"list\")\nfor(i in seq_along(cz_after_prep)) {\n  cz &lt;- lapply(cz_after_prep[[i]], czekanowski_similarity)\n  after_czekanowski[[i]] &lt;- cz\n}\n\nafter_czekanowski2 &lt;- vector(mode = \"list\")\nfor(i in seq_along(cz_after_prep2)) {\n  cz &lt;- lapply(cz_after_prep2[[i]], czekanowski_similarity)\n  after_czekanowski2[[i]] &lt;- cz\n}\n\n# Cell pairs.\npairs_before &lt;- do.call(\"rbind\",lapply(cz_before_prep[[1]], function(x) x[1:2][1,]))\n\n# Find all pairs in the After\npairs_after &lt;- vector(mode = \"list\")\npairs_after2 &lt;- vector(mode = \"list\")\n\nfor(i in 1:99) {\n  append_cells &lt;- do.call(\"rbind\",lapply(cz_after_prep[[i]], function(x) x[1:2][1,]))\n  pairs_after[[i]] &lt;- append_cells\n}\nfor(i in 1:99) {\n  append_cells &lt;- do.call(\"rbind\",lapply(cz_after_prep2[[i]], function(x) x[1:2][1,]))\n  pairs_after2[[i]] &lt;- append_cells\n}\n\n# Reformat \nbefore_cz_results &lt;- \n  purrr::map(before_czekanowski, ~as.data.frame(unlist(.x)) |&gt; \n               rename(\"cz\" = 1) |&gt;\n               cbind(pairs_before) |&gt; \n               relocate(.after = \"cell_y\",\"cz\") |&gt; \n               rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") )\n\nafter_cz_results &lt;- \n  purrr::map(after_czekanowski, ~as.data.frame(unlist(.x)) |&gt; \n               rename(\"cz\" = 1))\n\nafter_cz_results2 &lt;- \n  purrr::map(after_czekanowski2, ~as.data.frame(unlist(.x)) |&gt; \n               rename(\"cz\" = 1))\n\n# Now bind the cell pairs to the Af\\ter datasets.\nafter_cz_results &lt;- mapply(function(x, y) cbind(y, x), after_cz_results, pairs_after, SIMPLIFY = FALSE)\nafter_cz_results2 &lt;- mapply(function(x, y) cbind(y, x), after_cz_results2, pairs_after2, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell pair\nbefore_czekanowski_dataframe &lt;- bind_rows(before_cz_results) \nafter_czekanowski_dataframe &lt;- bind_rows(after_cz_results)\nafter_czekanowski_dataframe2 &lt;- bind_rows(after_cz_results2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#results",
    "href": "02_AllCalcs.html#results",
    "title": "All Similarity Calculations",
    "section": "Results",
    "text": "Results\n\n\nCode\n# before\nbefore_res_matrix &lt;- \n  before_res_matrix |&gt; \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |&gt; \n  left_join(ave_before_jaccard,by = c(\"x.cell\",\"y.cell\"))\n\n# after\nafter_res_matrix &lt;- \n  after_res_matrix |&gt; \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |&gt; \n  left_join(ave_after_jaccard,by = c(\"x.cell\",\"y.cell\"))\n\n# after2\nafter_res_matrix2 &lt;- \n  after_res_matrix2 |&gt; \n  rename(\"x.cell\" = \"x\",\"y.cell\" = \"y\") |&gt; \n  left_join(ave_after_jaccard2,by = c(\"x.cell\",\"y.cell\"))\n\n# Bin by distance between cells (GCD in km's)\nbefore_res_matrix$cutdist &lt;- \n  cut(before_res_matrix$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\nafter_res_matrix$cutdist &lt;- \n  cut(after_res_matrix$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\nafter_res_matrix2$cutdist &lt;- \n  cut(after_res_matrix2$gcd,\n      breaks = c(0, 2000, 4000, 6000, 8000, 10000, 12000, \n                 14000, 16000, 18000, 20000), \n      labels = c(\"0\", \"2000\", \"4000\", \"6000\",\"8000\", \n                 \"10000\", \"12000\",\"14000\", \"16000\", \"18000\"),\n                       include.lowest = TRUE)\n\n# Average and sd for Before.\nsumRes_01 &lt;-\n  before_res_matrix |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm= TRUE),\n    second = quantile(avg_jaccard,probs=0.975, na.rm = TRUE)\n  ) |&gt; \n  mutate(label = 'Before',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings() # This was added to ignore the last observation.\n\n# Average and sd for the After.\nsumRes_02 &lt;- \n  after_res_matrix |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n     # Quantiles\n    first = quantile(avg_jaccard,probs=0.25),\n    second = quantile(avg_jaccard,probs=0.975)\n  ) |&gt; \n  mutate(label = 'After',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings() # This was added to ignore the last observation.\n\n# Average and sd for the After 2.\nsumRes_025 &lt;- \n  after_res_matrix2 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n     # Quantiles\n    first = quantile(avg_jaccard,probs=0.25),\n    second = quantile(avg_jaccard,probs=0.95)\n  ) |&gt; \n  mutate(label = 'After-2',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings() # This was added to ignore the last observation.\n\n# Combine the two.\nsumRes_03 &lt;- bind_rows(sumRes_01,sumRes_02, sumRes_025)\n\n\n# Plot.\nsumRes_03 |&gt; \n   arrange(label) |&gt; \n  mutate(label = factor(label, levels=c(\"Before\", \"After\", \"After-2\"))) |&gt; # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"black\", \"darkorange4\",\"goldenrod3\")) + \n  scale_color_manual(values = c(\"black\",\"darkorange4\",\"goldenrod3\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Jaccard\",\n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# before\nbefore_res_matrix &lt;-\n  before_res_matrix |&gt;\n  left_join(\n    before_czekanowski_dataframe |&gt; #rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |&gt;\n      group_by(x.cell,y.cell) |&gt;\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |&gt;\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n\n`summarise()` has grouped output by 'x.cell'. You can override using the\n`.groups` argument.\n\n\nCode\n# after\nafter_res_matrix &lt;-\n  after_res_matrix |&gt;\n  left_join(\n    after_czekanowski_dataframe |&gt; rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |&gt; \n      group_by(x.cell,y.cell) |&gt;\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |&gt;\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n\n`summarise()` has grouped output by 'x.cell'. You can override using the\n`.groups` argument.\n\n\nCode\n# after2\nafter_res_matrix2 &lt;-\n  after_res_matrix2 |&gt;\n  left_join(\n    after_czekanowski_dataframe2 |&gt; rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\") |&gt; \n      group_by(x.cell,y.cell) |&gt;\n      summarise(avg_cz =  mean(cz, na.rm = TRUE)), by = c(\"x.cell\",\"y.cell\")) |&gt;\n  relocate(.after = \"avg_jaccard\",\"avg_cz\")\n\n\n`summarise()` has grouped output by 'x.cell'. You can override using the\n`.groups` argument.\n\n\nCode\n# Average and standard deviation for Before.\nsumRes_045 &lt;-\n  after_res_matrix2 |&gt;\n  group_by(cutdist) |&gt;\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n        # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |&gt;\n  mutate(label = 'After-2',label = as.factor(label)) |&gt;\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt;\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |&gt;\n  as.data.frame() |&gt; suppressWarnings() # This was added to ignore the last observation.\n\n# Average and standard deviation for Before.\nsumRes_04 &lt;-\n  before_res_matrix |&gt;\n  group_by(cutdist) |&gt;\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n        # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |&gt;\n  mutate(label = 'Before',label = as.factor(label)) |&gt;\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt;\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |&gt;\n  as.data.frame() |&gt; suppressWarnings() # This was added to ignore the last observation.\n\nsumRes_05 &lt;-\n  after_res_matrix |&gt;\n  group_by(cutdist) |&gt;\n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_cz,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_cz,probs=0.95, na.rm=TRUE)\n  ) |&gt;\n  mutate(label = 'After',label = as.factor(label)) |&gt;\n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt;\n  mutate(ci = se * qt(.975, n - 1), ci = as.numeric(ci)) |&gt;\n  as.data.frame() |&gt; suppressWarnings()\n\nsumRes_06 &lt;- bind_rows(sumRes_05, sumRes_04, sumRes_045)\n\n# Plot.\nsumRes_06 |&gt; \n   arrange(label) |&gt; \n  mutate(label = factor(label, levels=c(\"Before\", \"After\", \"After-2\"))) |&gt; # reorder the intervals\n  ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values = c(\"black\",\"darkorange4\", \"goldenrod3\")) + \n  scale_color_manual(values = c(\"black\", \"darkorange4\",\"goldenrod3\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\", \n       title = \"Czekanowski\", \n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"), \n        axis.title = element_text(face = \"bold\"), \n        legend.title = element_text(face = \"bold\"), \n        aspect.ratio = 1)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#sensitivity-analysis",
    "href": "02_AllCalcs.html#sensitivity-analysis",
    "title": "All Similarity Calculations",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis\nSimilarity measurements by survival status.\n\n\nCode\n# Retain occurrences with or greater than 15.\npbdb_sensitivity &lt;- \n  pbdb |&gt; \n  filter(occs &gt;= 15) # 3578 observations.\n\n# Split survival datasets by unique cell id.\n\n#First pulse:\n\n# after survivors\nsAft &lt;- pbdb_sensitivity |&gt; \n  filter(early_interval == \"Hirnantian\" & ex==0 & fad &gt;=443.8) |&gt;\n  group_split(cell_id) |&gt; lapply(as.data.frame)\n\n# before survivors\nsV &lt;- pbdb_sensitivity |&gt;\n  filter(early_interval == \"Katian\" & ex==1) |&gt;\n  group_split(cell_id) |&gt; lapply(as.data.frame)\n\n# after survivors.\nsBef &lt;- pbdb_sensitivity |&gt; \n  filter(early_interval  == \"Katian\" & ex==0) |&gt;\n  group_split(cell_id) |&gt; lapply(as.data.frame)\n\n## THIS IS FOR ANALYSIS OF SECOND PULSE: \n\n# Survivors After:\nsAftHir &lt;- pbdb_sensitivity |&gt; \n  filter(early_interval == \"Rhuddanian\" & ex==0 & fad &gt;=440.8) |&gt;\n  group_split(cell_id) |&gt; lapply(as.data.frame)\n\n# Victims of Hirnantian pulse.\nsVHir &lt;- pbdb_sensitivity |&gt;\n  filter(early_interval == \"Hirnantian\" & ex==1) |&gt;\n  group_split(cell_id) |&gt; lapply(as.data.frame)\n\n# Survivors of Hirnantian pulse.\nsBefHir &lt;- pbdb_sensitivity |&gt; \n  filter(early_interval  == \"Hirnantian\" & ex==0) |&gt;\n  group_split(cell_id) |&gt; lapply(as.data.frame)\n\n\n# Subsampling.\nsubsampling_fun &lt;-\n  function(x, n_boot = 99, sample_size = 12, seed = 5) {\n  set.seed(seed)\n  # Samples.\n  boot_samples &lt;- purrr::map(1:n_boot, ~ sample(x, sample_size, replace = FALSE))\n  # Combine cells into single dataframes.\n  comb_samples &lt;- purrr::map(boot_samples, ~ map_dfr(.x, bind_rows))\n  \n  return(comb_samples)\n}\n\n\n# Subsampled data.\nsBef_boot &lt;- subsampling_fun(sBef)\nsV_boot &lt;- subsampling_fun(sV)\nsAft_boot &lt;- subsampling_fun(sAft)\n\n\n# Second pulse.\nsBef_boot_Hir &lt;- subsampling_fun(sBefHir)\nsV_boot_Hir &lt;- subsampling_fun(sVHir)\nsAft_boot_Hir &lt;- subsampling_fun(sAftHir)\n\n\n###Jaccard index calculation\n\n\nCode\n# After survivors.\nafter_survivors_jaccard &lt;- jaccard_similarity(sAft_boot)\n# Before victims.\nbefore_victims_jaccard &lt;- jaccard_similarity(sV_boot)\n# Before survivors.\nbefore_survivors_jaccard &lt;- jaccard_similarity(sBef_boot)\n\n\n#Second pulse:\n\n# After survivors.\nafter_survivors_jaccard_Hir &lt;- jaccard_similarity(sAft_boot_Hir)\n# Before victims.\nbefore_victims_jaccard_Hir &lt;- jaccard_similarity(sV_boot_Hir)\n# Before survivors.\nbefore_survivors_jaccard_Hir &lt;- jaccard_similarity(sBef_boot_Hir)\n\n\nAverages\n\n\nCode\n##First pulse:\n\n# Mean jaccard for the After survivors.\naAftsJ &lt;- \n  bind_rows(after_survivors_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n# Mean jaccard for the Before victims.\navJ &lt;- \n  bind_rows(before_victims_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n# before survivors.\naBefsJ &lt;- \n  bind_rows(before_survivors_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n## Second pulse:\n\n# Mean jaccard for the After survivors.\naAftsJ_Hir &lt;- \n  bind_rows(after_survivors_jaccard_Hir) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n# Mean jaccard for the Before victims.\navJ_Hir &lt;- \n  bind_rows(before_victims_jaccard_Hir) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n# before survivors.\naBefsJ_Hir &lt;- \n  bind_rows(before_survivors_jaccard_Hir) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nVisualize results\n\n\nCode\n## FIRST PULSE: \n\n# After results: survivors.\nmRes_01 &lt;- after_res_matrix[,c(1:7,10)] |&gt; left_join(aAftsJ,by = c(\"x.cell\",\"y.cell\"))\n# Before results: survivors & victims.\nmRes_02 &lt;- before_res_matrix[,c(1:7,10)] |&gt; left_join(avJ,by = c(\"x.cell\",\"y.cell\"))\nmRes_03 &lt;- before_res_matrix[,c(1:7,10)] |&gt; left_join(aBefsJ,by = c(\"x.cell\",\"y.cell\"))\n\n# SECOND PULSE \nmRes_04 &lt;- after_res_matrix2[,c(1:7,10)] |&gt;  left_join(aAftsJ_Hir,by = c(\"x.cell\",\"y.cell\"))\n# Before results: survivors & victims.\nmRes_05 &lt;- after_res_matrix[,c(1:7,10)] |&gt; left_join(avJ_Hir,by = c(\"x.cell\",\"y.cell\"))\nmRes_06  &lt;- after_res_matrix[,c(1:7,10)] |&gt; left_join(aBefsJ_Hir,by = c(\"x.cell\",\"y.cell\"))\n\n# Summary statistics for each survival category.\n\n\n## FIRST PULSE \n# after survivors.\nsumRes_07 &lt;-\n  mRes_01 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25,na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |&gt; \n  mutate(label = 'After survivors',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame()\n\n\n\n\nCode\n# before victims.\nsumRes_08 &lt;-\n  mRes_02 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |&gt; \n  mutate(label = 'Before victims',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings()\n\n# before survivors.\nsumRes_09 &lt;-\n  mRes_03 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |&gt; \n  mutate(label = 'Before survivors',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings()\n\n## SECOND PULSE \n\n# after survivors.\nsumRes_10 &lt;-\n  mRes_04 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25,na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |&gt; \n  mutate(label = 'After survivors',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame()\n\n# before victims.\nsumRes_11  &lt;-\n  mRes_05 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |&gt; \n  mutate(label = 'Before victims',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings()\n\n# before survivors.\nsumRes_12 &lt;-\n  mRes_06 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    # Jaccard\n    avg =  mean(avg_jaccard, na.rm = TRUE),\n    sdev = sd(avg_jaccard, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_jaccard,probs=0.25, na.rm=TRUE),\n    second = quantile(avg_jaccard,probs=0.95, na.rm=TRUE)\n  ) |&gt; \n  mutate(label = 'Before survivors',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings()\n\n# Combine all results.\nsumRes_13 &lt;- bind_rows(sumRes_07,sumRes_08,sumRes_09) # First pulse\nsumRes_14 &lt;- bind_rows(sumRes_10,sumRes_11,sumRes_12) # Second pulse\n\n# Plot.\nsumRes_13 |&gt; \n  arrange(label) |&gt; \n   mutate(label = factor(label, levels=c(\"Before victims\", \"Before survivors\", \"After survivors\"))) |&gt; # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"gray60\",\"black\",\"goldenrod\")) + \n  scale_color_manual(values = c(\"gray60\",\"black\",\"goldenrod\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"First Pulse\",\n       subtitle = \"Jaccard by survival status\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot for second pulse\nsumRes_14 |&gt; \n  arrange(label) |&gt; \n   mutate(label = factor(label, levels=c(\"Before victims\", \"Before survivors\", \"After survivors\"))) |&gt; # reorder the intervals\n   ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n scale_fill_manual(values =  c(\"gray60\",\"black\",\"goldenrod\")) + \n  scale_color_manual(values = c(\"gray60\",\"black\",\"goldenrod\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Second Pulse \",\n       subtitle = \"Jaccard by survival status\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\nSetup for the second pulse analysis:\n\nCzekanowski index calculation\nAfter survivors\n\n\nCode\n# Data table \nprep_Afts &lt;- map(sAft_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_Afts &lt;- map(prep_Afts, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_Afts &lt;- vector(mode = \"list\")\nfor(i in seq_along(prep_Afts)) {\n  cZ &lt;- lapply(prep_Afts[[i]],czekanowski_similarity)\n  cz_Afts[[i]] &lt;- cZ\n}\n\n# Find all pairs.\nsp_01 &lt;- vector(mode = \"list\")\nfor(i in 1:99) {\n  pA &lt;- do.call(\"rbind\",lapply(prep_Afts[[i]], function(x) x[1:2][1,]))\n  sp_01[[i]] &lt;- pA\n}\n\n# Convert to dataframe and unlist.\nflat_Afts &lt;- purrr::map(cz_Afts, ~as.data.frame(unlist(.x)) |&gt; rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_afts &lt;- mapply(function(x, y) cbind(y, x), flat_Afts, sp_01, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_Afts &lt;- bind_rows(append_flat_afts) |&gt; rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\nBefore victims\n\n\nCode\n# Data table \nprep_v &lt;- map(sV_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_v &lt;- map(prep_v, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_v &lt;- vector(mode = \"list\")\nfor(i in seq_along(prep_v)) {\n  cZ &lt;- lapply(prep_v[[i]],czekanowski_similarity)\n  cz_v[[i]] &lt;- cZ\n}\n\n# Find all pairs.\nsp_02 &lt;- vector(mode = \"list\")\nfor(i in 1:99) {\n  pQ &lt;- do.call(\"rbind\",lapply(prep_v[[i]], function(x) x[1:2][1,]))\n  sp_02[[i]] &lt;- pQ\n}\n\n# Convert to dataframe and unlist.\nflat_v &lt;- purrr::map(cz_v, ~as.data.frame(unlist(.x)) |&gt; rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_v &lt;- mapply(function(x, y) cbind(y, x), flat_v, sp_02, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_v &lt;- bind_rows(append_flat_v) |&gt; rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\nBefore survivors\n\n\nCode\n# Data table \nprep_Befs &lt;- map(sBef_boot, ~ czekanowski_data_prep(.x, cell = \"cell\", accepted_name = \"accepted_name\"))\n# Add minimum field to each data sub-list.\nprep_Befs &lt;- map(prep_Befs, ~ map(.x, ~ mutate(.x, minimum = pmin(count_cell_x, count_cell_y))))\n\n# Compute czekanowski\ncz_Befs &lt;- vector(mode = \"list\")\nfor(i in seq_along(prep_Befs)) {\n  cM &lt;- lapply(prep_Befs[[i]],czekanowski_similarity)\n  cz_Befs[[i]] &lt;- cM\n}\n\n# Find all pairs.\nsp_03 &lt;- vector(mode = \"list\")\nfor(i in 1:99) {\n  pO &lt;- do.call(\"rbind\",lapply(prep_Befs[[i]], function(x) x[1:2][1,]))\n  sp_03[[i]] &lt;- pO\n}\n\n# Convert to dataframe and unlist.\nflat_Befs &lt;- purrr::map(cz_Befs, ~as.data.frame(unlist(.x)) |&gt; rename(\"cz\" = 1))\n# Append correctly.\nappend_flat_Befs &lt;- mapply(function(x, y) cbind(y, x), flat_Befs, sp_03, SIMPLIFY = FALSE)\n\n# Compute the average czekanowski per cell-pair.\nres_Befs &lt;- bind_rows(append_flat_Befs) |&gt; rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n\n\nCode\n# Averages.\nave_s1 &lt;- res_Afts |&gt; \n  group_by(x.cell,y.cell) |&gt;\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'x.cell'. You can override using the\n`.groups` argument.\n\n\nCode\nave_s2 &lt;- res_v |&gt; \n  group_by(x.cell,y.cell) |&gt;\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'x.cell'. You can override using the\n`.groups` argument.\n\n\nCode\nave_s3 &lt;- res_Befs |&gt; \n  group_by(x.cell,y.cell) |&gt;\n  summarise(avg_cz =  mean(cz, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'x.cell'. You can override using the\n`.groups` argument.\n\n\nVisualize results\n\n\nCode\n# Reverse cell pairs first.\nave_s1_reversed &lt;- ave_s1|&gt; rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\nave_s2_reversed &lt;- ave_s2|&gt; rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\nave_s3_reversed &lt;- ave_s3|&gt; rename(\"x.cell\" = \"y.cell\", \"y.cell\" = \"x.cell\")\n\n# after results: survivors.\nqRes_01 &lt;- after_res_matrix[,c(1:7,10)] |&gt;\n  inner_join(ave_s1_reversed, by = c(\"x.cell\",\"y.cell\"))\n# before results: survivors & victims.\nqRes_02 &lt;- ave_s2_reversed |&gt; \n  left_join(before_res_matrix[,c(1:7,10)],by = c(\"x.cell\",\"y.cell\"))\n\nqRes_03 &lt;- ave_s3_reversed |&gt; \n  left_join(before_res_matrix[,c(1:7,10)],by = c(\"x.cell\",\"y.cell\"))\n\n# Summary statistics for each survival category.\n\n# After survivors.\nsumRes_15 &lt;-\n  qRes_01 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n    # quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |&gt; \n  mutate(label = 'after survivors',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt;\n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame()\n\n\n\n\nCode\n# Before victims.\nsumRes_16 &lt;-\n  qRes_02 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |&gt; \n  mutate(label = 'before victims',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.95, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings()\n\n# before survivors.\nsumRes_17 &lt;-\n  qRes_03 |&gt; \n  group_by(cutdist) |&gt; \n  summarise(\n    avg =  mean(avg_cz, na.rm = TRUE),\n    sdev = sd(avg_cz, na.rm = TRUE),\n    n = n(),\n    se = sdev/sqrt(n),\n       # Quantiles\n    first = quantile(avg_cz,probs=0.25),\n    second = quantile(avg_cz,probs=0.95)\n  ) |&gt; \n  mutate(label = 'before survivors',label = as.factor(label)) |&gt; \n  mutate(cutdist = cutdist,\n         cutdist = factor(cutdist,levels = c(\"0\",\"2000\",\"4000\",\"6000\",\"8000\",\"10000\",\"12000\",\"14000\",\"16000\",\"18000\",\"20000\"))) |&gt; \n  mutate(ci = se * qt(.97, n - 1), ci = as.numeric(ci)) |&gt; \n  as.data.frame() |&gt; suppressWarnings()\n\n# Combine all results.\nsumRes_18 &lt;- bind_rows(sumRes_15,sumRes_16,sumRes_17)\n\n# Plot.\nsumRes_18 |&gt; \n ggplot(aes(x = cutdist, y = avg, group = label, colour = label, fill = label)) +\n  geom_errorbar(aes(ymin = second, ymax = first), width = 0.05, linewidth = 1, position = position_dodge(width = 0.3)) + \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) +\n  ylim(0, 0.7) +\n  scale_size_continuous(breaks = c(5, 10, 15, 20, 25, 30)) +\n  geom_point(aes(size = n), shape = 21, fill = \"white\", stroke = 2, position = position_dodge(width = 0.3)) +\n  scale_fill_manual(values =  c(\"goldenrod\",\"gray60\", \"black\")) + \n  scale_color_manual(values = c(\"goldenrod\",\"gray60\", \"black\")) + \n  labs(x = \"Great Circle Distance (km)\", \n       y = \"Similarity Value\",\n       title = \"Czekanowski by survival status\",\n       subtitle = \"Subsampling by cells and occurrences\", colour = \"Stages\", size = \"Cell-pair comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "02_AllCalcs.html#extra",
    "href": "02_AllCalcs.html#extra",
    "title": "All Similarity Calculations",
    "section": "Extra",
    "text": "Extra\nCount occurrences per genus in the Hirnantian to determine whether they match the disaster taxa in literature, such as the genus Hirnantia.\n\n\nCode\n# Find the top 5 genera based on number of occurrences\npbdb.hir  &lt;- pbdb |&gt; filter(interval.ma==443.8)\n\ngenus_counts &lt;- pbdb.hir |&gt;\n  group_by(accepted_name) |&gt; \n             summarise(count = n(), .groups = 'drop') |&gt; \n             top_n(5, wt = count) |&gt;\n             arrange(desc(count))\n\n# Convert accepted_name to a factor with levels in descending order of count\n\ngenus_counts$accepted_name &lt;- factor(genus_counts$accepted_name, levels = genus_counts$accepted_name[order(genus_counts$count, decreasing = TRUE)])\n\n# Visualize it with switched axes\n\nggplot(genus_counts, aes(x = count, y = accepted_name)) + theme_classic() + geom_bar(stat = \"identity\", fill = \"dodgerblue3\") + labs(x = \"Number of Occurrences\", y = \"Genus Name\")+ coord_flip() # Optional: this can be omitted if you want the horizontal bars\n\n\n\n\n\n\n\n\n\n\nWastebin taxa removed\nLocate and remove all wastebin taxa, following the methods of Plotnick and Wagner (2005). First, using the full database, locate all wastebin taxa throughout the “before” and “after” intervals by finding the 5 most frequent genus occurrences.\n\n\nCode\noptions(download.file.method = \"libcurl\")\npbdb &lt;- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\n\npbdb &lt;- pbdb[, -c(1, 22, 21)]\n\n\n \n# Get the frequency of each genus, then keep top 20 of them. these are our wastebasket genera\n \npbdb_occs &lt;- pbdb |&gt; \n  group_by(accepted_name) |&gt; \n  summarise(frequency = n()) |&gt; \n  arrange(desc(frequency)) |&gt; \n   slice_head(n = 20) |&gt;  # keep only the top 20 rows\n  arrange(accepted_name) # order alphabetically\n        \n\n# Next, continue with the normal process of getting Jaccard similarity.\n\n# Here is where we remove the wastebin taxa:            \npbdb &lt;- pbdb |&gt;    \n  filter(interval.ma %in% c(\"445.2\", \"443.8\", \"440.8\") &\n       (!accepted_name %in% pbdb_occs$accepted_name)) # particularly, this line\n \n# Identify Invalid Coordinates.\n  cl &lt;- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n\n\nTesting coordinate validity\n\n\nFlagged 0 records.\n\n\n\n\nCode\n  cl_rec &lt;- pbdb[!cl,] #extract and check them\n  \n pbdb &lt;- pbdb |&gt; \n   cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n\n\nTesting coordinate validity\n\n\nRemoved 0 records.\n\n\nNext, follow all the steps from Visualization of Cells and Occurrences down to Czekanowski indices again. You will notice slight changes to the number of cells since filtering out the wastebin taxa reduces the number of occurrences we have.\n##Spatial standardization\nHere, we will apply the Divvy package to spatially standardize our data using the circular “cookie” method and re-run our Jaccard calculations with the standarized data.\n\n\nCode\n library(raster)\npbdb &lt;- read.csv(\"https://raw.githubusercontent.com/Anonymous-paleobio/Taxonomic-Homogenization-DeepTime/refs/heads/main/Pbdb-files/pbdb_lome.csv\")\n\npbdb &lt;- pbdb[, -c(1, 22, 21)]\n\n# Make sure coordinates are numerical\npbdb$paleolat &lt;- as.numeric(pbdb$paleolat)\npbdb$paleolng &lt;- as.numeric(pbdb$paleolng)\n\n# Subset by age\npbdb.before &lt;- pbdb |&gt; filter(early_interval == \"Katian\")\npbdb.after &lt;- pbdb |&gt; filter(early_interval == \"Hirnantian\")\npbdb.after2 &lt;- pbdb |&gt; filter(early_interval == \"Rhuddanian\")\n\n# Get the frequency of each genus, then keep top 20 of them. these are our wastebasket genera\n \n# Initialize equal earth projections and coordinate:\nrWorld &lt;-rast()\nprj &lt;- 'EPSG:8857'\n\n# Match the divvy resolution with our hexa resution from the icosa package.\ndeg &lt;- 4.5 # hexa's resolution\n\n# Approximate meters per degree at the equator\nmeters_per_deg &lt;- 111320  \n\n# Adjust for the latitude of area\nlat_center &lt;- mean(raster::yFromCell(rWorld, 1:ncell(rWorld)))  # rough center latitude\nmeters_res &lt;- deg * meters_per_deg * cos(lat_center * pi/180)\nmeters_res\n\n\n[1] 500940\n\n\n\n\nCode\n# Get the new resolution\nnew_res &lt;- meters_res\n\n\n# New_res is the target resolution in meters\nrPrj &lt;- project(rWorld, prj, method = \"bilinear\", res = new_res)\n\n\nterra::values(rPrj) &lt;- 1:ncell(rPrj)\n\n# Coordinate column names for the current and target coordinate reference system\nxyCartes &lt;- c('paleolng','paleolat')\nxyCell   &lt;- c('cellX','cellY')\n\n\n\n\nCode\nllOccs &lt;- vect(pbdb.before, geom = xyCartes, crs = 'epsg:4326')\nprjOccs &lt;- terra::project(llOccs, prj)\npbdb.before$cell &lt;- cells(rPrj, prjOccs)[,'cell']\npbdb.before[, xyCell] &lt;- xyFromCell(rPrj, \n                                pbdb.before$cell)\n\nsdSumry(pbdb.before, taxVar = 'accepted_name', xy = xyCell, crs = prj) |&gt; \n  print()\n\n\n     nOcc nLoc centroidX centroidY latRange greatCircDist meanPairDist\n[1,] 3892  117  -4799668  -1670656  113.169      24627.71     8569.577\n     minSpanTree     SCOR nTax\n[1,]    90332.97 35.32539  813\n\n\n\n\nCode\n# Disregard SCOR \n\noccUniq &lt;- uniqify(pbdb.before, xyCell)\nptsUniq &lt;- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\nworldP &lt;- ggplot(data = plates.lome$geometry) +\n  theme_bw() +\n  geom_sf() +\n  geom_sf(data = ptsUniq, shape = 16, color = 'red3')\n\ncircLocs &lt;- cookies(dat = pbdb.before,\n                    xy = xyCell,\n                    iter = 100,\n                    nSite = 5,\n                    r = 2000,\n                    weight = TRUE,\n                    crs = prj,\n                    output = \"full\")\n\n\n# Convert one circular sample to sf\nsmplPts &lt;- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr &lt;- smplPts[1, ]\nr_km &lt;- 2000\nbuf &lt;- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf &lt;- st_as_sf(plates.lome) |&gt; st_transform(crs = prj)\n\n\n# Assume circLocs is your list of circular subsamples (1000 iterations)\n# We'll just plot the first 5 for clarity\nn_plot &lt;- 3\nset.seed(567)\nsubsamples &lt;- circLocs[1:n_plot]\n\n# Convert world map to sf and match projection\nworld_sf &lt;- st_as_sf(plates.lome) |&gt; st_transform(crs = prj)\n\n# Create a data frame of points and buffers for all subsamples\nall_points &lt;- list()\nall_buffers &lt;- list()\n\nfor (i in seq_along(subsamples)) {\n  smplPts &lt;- st_as_sf(subsamples[[i]], coords = xyCell, crs = prj)\n  \n  # Pick the first point as center\n  cntr &lt;- smplPts[1, ]\n  buf &lt;- st_buffer(cntr, dist = 2000 * 1000)  # 2000 km in meters\n  \n  smplPts$iteration &lt;- paste0(\"#\", i)\n  buf$iteration &lt;- paste0(\"#\", i)\n  \n  all_points[[i]] &lt;- smplPts\n  all_buffers[[i]] &lt;- buf\n}\n\n# Combine all iterations\nall_points_sf &lt;- do.call(rbind, all_points)\nall_buffers_sf &lt;- do.call(rbind, all_buffers)\n\n\n\n# Plot\nggplot() +\n  geom_sf(data = world_sf, fill = \"gray80\", color = \"white\") +\n    geom_sf(data = ptsUniq, shape = 16, color = 'black')+\n  geom_sf(data = all_buffers_sf, fill = NA, linewidth = 1, color = \"navyblue\") +\n  geom_sf(\n    data = all_points_sf,\n    aes(color = iteration),\n    shape = 16,\n    size = 3,\n  ) +\n  theme_bw() +\n  labs(\n    title = \"Circular subsamples (first 3 iterations)\",\n    subtitle=\"Katian\",\n    x = NULL, y = NULL,\n    color = \"Cells in Iteration\"\n  )\n\n\nWarning in rep(pch, length.out = length(x)): 'x' is NULL so the result will be\nNULL\n\n\n\n\n\n\n\n\n\nCircLocs now has 100 lists of subsampled occurrences which can be viewed with str(circLocs[[1]]):\n\n\nCode\nstr(circLocs[[1]])\n\n\n'data.frame':   1891 obs. of  23 variables:\n $ cell          : num  1186 1186 1186 1186 1186 ...\n $ cell_id       : int  26 26 26 26 26 26 26 26 26 26 ...\n $ occs          : int  487 487 487 487 487 487 487 487 487 487 ...\n $ interval.ma   : num  445 445 445 445 445 ...\n $ early_interval: chr  \"Katian\" \"Katian\" \"Katian\" \"Katian\" ...\n $ fad           : num  453 458 445 453 453 ...\n $ lad           : num  445 445 267 383 252 ...\n $ accepted_name : chr  \"Anazyga\" \"Asaphus\" \"Clinopistha\" \"Cyclonema\" ...\n $ genus         : chr  \"Anazyga\" \"Asaphus\" \"Clinopistha\" \"Cyclonema\" ...\n $ ex            : int  1 1 0 0 0 0 0 0 0 0 ...\n $ phylum        : chr  \"Brachiopoda\" \"Arthropoda\" \"Mollusca\" \"Mollusca\" ...\n $ class         : chr  \"Rhynchonellata\" \"Trilobita\" \"Bivalvia\" \"Gastropoda\" ...\n $ order         : chr  \"Atrypida\" \"Asaphida\" \"Solemyida\" \"Euomphalina\" ...\n $ family        : chr  \"Anazygidae\" \"Asaphidae\" \"Solemyidae\" \"Platyceratidae\" ...\n $ paleolat      : num  -5.95 -5.95 -5.95 -5.95 -5.95 -5.95 -6.46 -5.95 -5.95 -5.95 ...\n $ paleolng      : num  -114 -114 -114 -114 -114 ...\n $ occurrence_no : int  1302195 1302206 1302197 1302200 1302198 1302194 1255313 1302205 1302201 1302192 ...\n $ collection_no : int  174050 174050 174050 174050 174050 174050 166420 174050 174050 174050 ...\n $ reference_no  : int  16444 16444 16444 16444 16444 16444 53404 16444 16444 16444 ...\n $ long          : num  -114 -114 -114 -114 -114 ...\n $ lat           : num  -8.98 -8.98 -8.98 -8.98 -8.98 ...\n $ cellX         : num  -1.1e+07 -1.1e+07 -1.1e+07 -1.1e+07 -1.1e+07 ...\n $ cellY         : num  -595990 -595990 -595990 -595990 -595990 ...\n\n\n\n\nCode\ncircLocs_before &lt;- circLocs \n\n\nAfter the first pulse, spatially standardized:\n\n\nCode\n# Extract cell number and centroid coordinates associated with each occurrence\n\nllOccs &lt;- vect(pbdb.after, geom = xyCartes, crs = 'epsg:4326')\nprjOccs &lt;- terra::project(llOccs, prj)\npbdb.after$cell &lt;- cells(rPrj, prjOccs)[,'cell']\npbdb.after[, xyCell] &lt;- xyFromCell(rPrj, \n                                pbdb.after$cell)\n\nsdSumry(pbdb.after, taxVar = 'accepted_name', xy = xyCell, crs = prj) |&gt; \n  print()# Extract cell number and centroid coordinates associated with each occurrence\n\n\n     nOcc nLoc centroidX centroidY latRange greatCircDist meanPairDist\n[1,]  818   35 -76029.63  -2256249 96.56812      25271.42     8979.155\n     minSpanTree     SCOR nTax\n[1,]    52498.61 27.39688  339\n\n\nCode\nllOccs &lt;- vect(pbdb.after, geom = xyCartes, crs = 'epsg:4326')\nprjOccs &lt;- terra::project(llOccs, prj)\npbdb.after$cell &lt;- cells(rPrj, prjOccs)[,'cell']\npbdb.after[, xyCell] &lt;- xyFromCell(rPrj, \n                                pbdb.after$cell)\n\nsdSumry(pbdb.after, taxVar = 'accepted_name', xy = xyCell, crs = prj) |&gt; \n  print()\n\n\n     nOcc nLoc centroidX centroidY latRange greatCircDist meanPairDist\n[1,]  818   35 -76029.63  -2256249 96.56812      25271.42     8979.155\n     minSpanTree     SCOR nTax\n[1,]    52498.61 27.39688  339\n\n\n\n\nCode\n# Disregard SCOR \n\noccUniq &lt;- uniqify(pbdb.after, xyCell)\nptsUniq &lt;- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\n# Circular subsampling technique\n\ncircLocsAft &lt;- cookies(dat = pbdb.after,\n                    xy = xyCell,\n                    iter = 100,\n                    nSite = 5,\n                    r = 2000,\n                    weight = TRUE,\n                    crs = prj,\n                    output = \"full\")\n\n\n# Convert one circular sample to sf\nsmplPts &lt;- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr &lt;- smplPts[1, ]\nr_km &lt;- 2000\nbuf &lt;- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf &lt;- st_as_sf(plates.lome) |&gt; st_transform(crs = prj)\n\n# worldP +\n#  geom_sf(data = smplPts, shape = 17, color = 'navyblue') +\n#  geom_sf(data = buf, fill = NA, linewidth = 1, color = 'navyblue')\n\n\nNow, for the same thing with the “after-2” age for the second pulse.\nAfter the second pulse, spatially standardized:\n\n\nCode\nllOccs &lt;- vect(pbdb.after2, geom = xyCartes, crs = 'epsg:4326')\nprjOccs &lt;- terra::project(llOccs, prj)\npbdb.after2$cell &lt;- cells(rPrj, prjOccs)[,'cell']\npbdb.after2[, xyCell] &lt;- xyFromCell(rPrj, \n                                pbdb.after2$cell)\n\nsdSumry(pbdb.after2, taxVar = 'accepted_name', xy = xyCell, crs = prj) |&gt; \n  print()\n\n\n     nOcc nLoc centroidX centroidY latRange greatCircDist meanPairDist\n[1,]  538   21  -1702892  -2170373 90.82533      23677.03     9144.323\n     minSpanTree     SCOR nTax\n[1,]    37007.92 28.70596  244\n\n\n\n\nCode\n# Disregard SCOR \n\noccUniq &lt;- uniqify(pbdb.after2, xyCell)\nptsUniq &lt;- st_as_sf(occUniq, coords = xyCell, crs = prj)\n\n\ncircLocsAft2 &lt;- cookies(dat = pbdb.after2,\n                    xy = xyCell,\n                    iter = 1000, # number of iterations\n                    nSite = 5, # number of cells\n                    r = 2000, # radial distance in km\n                    weight = TRUE, \n                    crs = prj, # Equal Earth projection\n                    output = 'full')\n\n\n# Convert one circular sample to sf\nsmplPts &lt;- st_as_sf(circLocs[[1]], coords = xyCell, crs = prj)\n\n# Convert 2000 km to m\ncntr &lt;- smplPts[1, ]\nr_km &lt;- 2000\nbuf &lt;- st_buffer(cntr, dist = r_km * 1000)\n\n# Match world map to projection\nworld_sf &lt;- st_as_sf(plates.lome) |&gt; st_transform(crs = prj)\n\n# worldP +\n#  geom_sf(data = smplPts, shape = 17, color = 'navyblue') +\n#  geom_sf(data = buf, fill = NA, linewidth = 1, color = 'navyblue')\n\n\nNext, we will check to see how many InF values we have in sdSumry$SCOR. This won’t matter for most interval-pairs, but for the P-T and LOME intervals in which homogenization does occur, it’s a sanity check to understand why the SCOR does not reflect homogenization – taxon that are present in all cells analyzed here are given an InF score instead of a numerical value and are not included in the SCOR calculation. As per the Divvy vignette walkthrough: “When a taxon is present in all sampled locations, its log probability of incidence is infinite. Infinity is nonsensical in an empirical comparison (…).”\nSince we won’t use SCOR beyond this next chunk of code, the SCOR value will not affect our Jaccard calculations even for homogenized intervals, so we can still spatially standardize the data and see how that effects the Jaccard values.\n\n\nCode\nsdbef &lt;- sdSumry(circLocs, taxVar = 'genus', xy = xyCell, crs = prj)\nsdaft &lt;- sdSumry(circLocsAft, taxVar = 'genus', xy = xyCell, crs = prj) #tons of InF scores here in PT and LOME indicating widespread occurrences and homogenization has occurred.\nsdaft2 &lt;- sdSumry(circLocsAft2, taxVar = 'genus', xy = xyCell, crs = prj) \n\nis.infinite(sdaft$SCOR) # occurrences that are everywhere are labeled InF. We need to change this to 100 to reflect that they are everywhere, but I don't know how to do that. So, ignore SCOR for now.\n\n\n  [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [13] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n [25]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n [37]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [49] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n [61] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [73] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [97]  TRUE FALSE FALSE FALSE\n\n\n\n\nCode\n# before\nsdbef_inf &lt;- lapply(sdbef, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdbef_mean &lt;- mean(sdbef_inf$SCOR)\n\n\n# after:\nsdaft_inf &lt;- lapply(sdaft, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdaft_mean &lt;- mean(sdaft_inf$SCOR)\n\n\n# after-2:\nsdaft2_inf &lt;- lapply(sdaft2, function(x) {\n  replace(x, is.infinite(x), 100)\n})\nsdaf2t_mean &lt;- mean(sdaft2_inf$SCOR)\n\n\nNow, we will calculate the spatially standardized Jaccard similarity for each geologic age:\n\n\nCode\n# before.\nbefore_jaccard &lt;- \n  jaccard_similarity(circLocs)\n\n# after.\nafter_jaccard &lt;- \n  jaccard_similarity(circLocsAft)\n\n# after.\nafter2_jaccard &lt;- \n  jaccard_similarity(circLocsAft2)\n\n\n#before\nave_before_jaccard &lt;- \n  bind_rows(before_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n# after\nave_after_jaccard &lt;- \n  bind_rows(after_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\nCode\n# after\nave_after2_jaccard &lt;- \n  bind_rows(after2_jaccard) |&gt;\n  group_by(cell_x, cell_y) |&gt; \n  summarise(avg_jaccard = mean(jaccard_similarity)) |&gt; \n  rename(\"x.cell\" = \"cell_x\", \"y.cell\" = \"cell_y\")\n\n\n`summarise()` has grouped output by 'cell_x'. You can override using the\n`.groups` argument.\n\n\n##Spatially Standardized Jaccard for the LOME\n\n\nCode\n# this function will get mean and 95% confidence intervals \nmean_ci &lt;- function(x, conf = 0.95) {\n  x &lt;- as.numeric(x)\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n == 0) return(c(mean = NA, lower = NA, upper = NA))\n  m &lt;- mean(x)\n  se &lt;- sd(x) / sqrt(n)\n  t_crit &lt;- qt(1 - (1 - conf)/2, df = n - 1)\n  ci &lt;- t_crit * se\n  c(mean = m, lower = m - ci, upper = m + ci)\n}\n\n\ndivvy_before  &lt;- ave_before_jaccard |&gt;  mutate(age = \"Before\")\ndivvy_after   &lt;- ave_after_jaccard   |&gt; mutate(age = \"Pulse-1\")\ndivvy_after2  &lt;- ave_after2_jaccard |&gt;  mutate(age = \"Pulse-2\")\n\n\n#  add age columns\ndivvy_before  &lt;- divvy_before  |&gt; mutate(age = \"Before\")\ndivvy_after   &lt;- divvy_after  |&gt;  mutate(age = \"After\")\ndivvy_after2  &lt;- divvy_after2  |&gt;  mutate(age = \"After-2\")\n\n# combine all data\ndivvy_combined &lt;- bind_rows(divvy_before, divvy_after, divvy_after2)\n\n# create a unique pair identifier\ndivvy_combined &lt;- divvy_combined |&gt;  mutate(pair = paste(x.cell, y.cell, sep = \"_\"))\n\n\n# get average and 95% ci\nsummary_divvy &lt;- divvy_combined |&gt; \n  group_by(age) |&gt; \n  summarise(\n    mean = mean(avg_jaccard, na.rm = TRUE),\n    lower = mean_ci(avg_jaccard)[\"lower\"],\n    upper = mean_ci(avg_jaccard)[\"upper\"],\n    .groups = \"drop\"\n  )\n\n# rename so it's in order\nsummary_divvy$age &lt;- c(\"Pulse-1 (Hirnantian)\", \"Pulse-2 (Rhuddanian)\",\"Before (Katian)\" )\n\n\n# now plot!\nggplot(summary_divvy, aes(x = age, y = mean, color = age)) +\n  scale_color_manual(values=c(\"black\", \"darkorange4\",\"goldenrod3\"))+\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n  theme_minimal(base_size = 14) +\n  theme_classic()+\n  labs(\n    title = \"Spatially Standardized Global Jaccard (with 95th and 5th percentile)\",\n    x = \"Age\",\n    y = \"Mean Jaccard Similarity\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nAs a final note, what we just analyzed is the average global Jaccard value of the spatially standardized data. This plot does not separate similarity as a function of distance the way the main calculations do. So, that one number is a global value after the late Ordovician mass extinction after spatially subsampling 100 times using the cookie method, which agrees with our Jaccard calculations beforehand. What we calculated in Results beforehand also shows that most of the lowered similarity is from cells that are farther apart from one another and that, for this interval, similarity can vary regionally. By using both methods, we can account for regional differences and for differences in the dispersion of cells.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All Similarity Calculations</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html",
    "href": "03_Fig3.html",
    "title": "Generating Figure 3",
    "section": "",
    "text": "Setup\nCode\nrpkg &lt;- c(\"readr tidyverse fs stringr ggplot2 dplyr conflicted piggyback tidyr tibble downloadthis ggpubr\")\n\n\nimport_pkg &lt;- function(x)\n  x |&gt; trimws() |&gt; strsplit(\"\\\\s+\")  |&gt; unlist() |&gt; \n  lapply(function(x) library(x, character.only = T)) |&gt; \n  invisible()\n\nrpkg |&gt; import_pkg()\n\n# Resolve conflicted functions.\nconflicted::conflict_prefer(name = \"filter\", winner = \"dplyr\",losers = \"stats\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html#multiple-csv-files",
    "href": "03_Fig3.html#multiple-csv-files",
    "title": "Generating Figure 3",
    "section": "Multiple csv files",
    "text": "Multiple csv files\nWe begin by loading in the Jaccard .csv files we generated for each interval set in BeforeAfter.R. These files are also readily available for you in the Github Repository under the “Difference” folder.\nTo calculate the error and in Figure 3, we get the data from our calculations BeforeAfter as well, which is also readily available for you in the folder “avg_diff”.\nRead multiple csv files in and organize them using fs to read in all the .csv files at once, as well as purrr and dplyr from tidyverse to organize them. We will do this for the main dataset and also for the grey error band we calculated earlier.\n\n\nCode\n# For main dataset\ndata_dir &lt;- \"~/Documents/BioHom/Difference/\" #setup directory of where we want to read the multiple files. This is simply folder of the sumRes_03 dataframe for each interval-set we've calculated, all collated into one folder. \n\n#For error band\ndata_diff &lt;- \"~/Documents/BioHom/avg_diff/\"\n\n# Test to be sure this works by loading in all. csv files:\ncsv_files &lt;- fs::dir_ls(data_dir, regexp = \"\\\\.csv$\")  # use fs to collect all data in data_dir that ends in .csv\ncsv_files_diff &lt;- fs::dir_ls(data_diff, regexp = \"\\\\.csv$\") # for error\n\n# Now load in the .csv files and ID each .csv to keep tabs:\nall_Jacc &lt;- data_dir |&gt; \n  dir_ls(regexp = \"\\\\.csv$\") |&gt;   # read in all files in the directory that end in .csv\n  map_dfr(read_csv, .id = \"source\")   # id everything by source\n#for errors:\nall_errors &lt;- data_diff |&gt; \n  dir_ls(regexp = \"\\\\.csv$\") |&gt;   # read in all files in the directory that end in .csv\n  map_dfr(read_csv, .id = \"source\")   # id everything by source\n\n# Set up better ID's:\nall_Jacc$ID &lt;- gsub(\".*\\\\/([^\\\\/]*)(Kpg|LOI|PT|TJ|lome|Messi|SanCamp|AlbCen|Pliens|Assel|Serra).*\", \"\\\\2\", all_Jacc$source)\n# ^ add a new column with ID's based on all_Jacc$source, removing all but the interval-pair name from it\n# Set up better ID's:\nall_errors$ID &lt;- gsub(\".*\\\\/([^\\\\/]*)(Alb|Assel|Camp|Ceno|Cha|Dan|Dar|Het|Het|Hir|Ind|Kat|Maa|Messi|Ole|Plie|Rhae|Rhud|Sak|San|Toa|Zanc).*\", \"\\\\2\", all_errors$source)\n\n# Rearrange data for clarity using dplyr's arrange():\nall_Jacc &lt;- all_Jacc |&gt;   arrange(ID, cutdist)\nall_errors &lt;- all_errors|&gt;   arrange(ID, cutdist)\n\nall_Jacc$source &lt;- NULL # remove source column now that it is no longer needed\nall_errors$source &lt;- NULL # remove source column now that it is no longer needed\n\nall_Jacc$...1 &lt;- NULL # remove this weird column that was added in with source as well\nall_errors$...1 &lt;- NULL # remove this weird column that was added in with source as well\n\nall_Jacc$age &lt;- NULL # remove this weird column that was added in with source as well\n\n# Separate the LOME and PT data for later:\n\n# After\nLOME_Jacc &lt;- all_Jacc[all_Jacc$label == 'After' & all_Jacc$ID == 'lome', ]\nPT_Jacc   &lt;- all_Jacc[all_Jacc$label == 'After' & all_Jacc$ID == 'PT', ]\n\n# After-2\nLOME2_Jacc &lt;- all_Jacc[all_Jacc$label == 'After-2' & all_Jacc$ID == 'lome', ]\nPT2_Jacc   &lt;- all_Jacc[all_Jacc$label == 'After-2' & all_Jacc$ID == 'PT', ]\n\n# Add in missing cutdist values:\nmissing_LOME &lt;- data.frame(cutdist=c(18000, 20000),\n                           avg= c(NA, NA), sdev= c(NA, NA), \n                           n= c(NA,NA), se= c(NA,NA), first= c(NA,NA), \n                           second= c(NA, NA), label = c('After-2','After-2'),\n                           ci = c(NA, NA),\n                           ID = c('lome','lome'), Jaccard_change=c(NA,NA))\n\nLOME_Jacc &lt;-rbind(LOME_Jacc, missing_LOME)\nLOME2_Jacc &lt;- rbind(LOME2_Jacc, missing_LOME)\n\n# Add in missing cutdist values:\nmissing_PT &lt;- data.frame(cutdist=c(18000, 18000,20000, 20000),\n                           avg= c(NA, NA, NA, NA),\n                          sdev= c(NA, NA, NA, NA),\n                           n=   c(NA, NA, NA, NA),\n                           se=  c(NA, NA, NA, NA),\n                         first= c(NA, NA, NA, NA),\n                        second= c(NA, NA, NA, NA),\n                        label = c('After','After-2', 'After', 'After-2'),\n                           ci = c(NA, NA, NA, NA),\n                           ID = c('PT','PT','PT', 'PT'),\n                        Jaccard_change=c(NA,NA,NA,NA))\n\nPT2_Jacc &lt;- rbind(PT2_Jacc, missing_PT)\n\nLOME2_Jacc$ID &lt;- 'LOME2' # Give it a unique label\nPT2_Jacc$ID &lt;- 'PT2'\n\nLOME_Jacc &lt;- rbind(LOME_Jacc, LOME2_Jacc) # Bring After and After-2 together\nPT_Jacc &lt;- rbind(PT_Jacc, PT2_Jacc)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html#before-and-after-columns",
    "href": "03_Fig3.html#before-and-after-columns",
    "title": "Generating Figure 3",
    "section": "Before and after columns",
    "text": "Before and after columns\nAll interval pairs need to have columns from 0 -20000 in order to perform the calculations, so those with missing columns are filled in with NA data. We are separating by the labels Before and After. The goal is to create two new columns, one ‘ave_before’ and one ‘ave_after’ so we can subtract one from the other and calculate the change in similarity:\n\n\nCode\n# Using dplyr, I add each missing row into a new df: \nmissing &lt;- data.frame(cutdist=c(20000, 20000, # AlbCen\n                                20000, 20000,# Assel\n                                20000, 20000, # LOI\n                                18000, 20000, # lome\n                                20000, 20000, # messi\n                                20000, 20000, # pliens\n                                18000, 18000, 20000, 20000, 20000, # PT\n                                20000, 20000, #SanCamp\n                                18000, 20000,  #Serra\n                                20000), #TJ\n                     avg=    c(NA, NA,\n                               NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA, NA,\n                               NA, NA, \n                               NA, NA, NA, NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA),\n                     sdev=   c(NA, NA, \n                               NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA, NA, \n                               NA, NA, \n                               NA, NA, NA, NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA),\n                     n=      c(NA, NA, \n                               NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA, NA, \n                               NA, NA,\n                               NA, NA, NA, NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA),\n                     se=     c(NA, NA, \n                               NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA, NA, \n                               NA, NA,\n                               NA, NA, NA, NA, NA, \n                               NA, NA,\n                               NA, NA, \n                               NA),\n                     first=  c(NA, NA,\n                               NA, NA, \n                               NA, NA,\n                               NA, NA,  \n                               NA, NA, \n                               NA, NA, \n                               NA, NA, NA, NA, NA,\n                               NA, NA,\n                               NA, NA, \n                               NA), \n                     second= c(NA, NA, NA,\n                               NA,\n                               NA, NA, \n                               NA, NA, \n                               NA, NA,\n                               NA, NA, \n                               NA, NA, NA, NA, NA, \n                               NA, NA,\n                               NA, NA, \n                               NA),\n                     label=  c('Before', 'After',\n                               'Before', 'After',\n                               'Before', 'After',\n                               'After-2', 'After-2', \n                               'Before','After', \n                               'Before','After',\n                               'After', 'After-2', 'Before', \"After\", \"After-2\",\n                               'Before','After',\n                               'Before', 'After',\n                               'After'),\n                     ci=    c(NA, NA,\n                              NA, NA,\n                              NA, NA,\n                              NA, NA, \n                              NA, NA, \n                              NA, NA,\n                              NA, NA, NA, NA, NA, \n                              NA,NA,\n                              NA, NA, \n                              NA),\n                     ID =   c('AlbCen', 'AlbCen',\n                              'Assel', 'Assel',\n                              'LOI', 'LOI',\n                              'lome', 'lome',\n                              'Messi','Messi',\n                              'Pliens', 'Pliens', \n                              'PT', 'PT', 'PT', 'PT', 'PT',\n                              'SanCamp','SanCamp',\n                              'Serra', 'Serra',\n                              'TJ'),\n                     Jaccard_change= c(NA, NA,\n                              NA, NA,\n                              NA, NA,\n                              NA, NA, \n                              NA, NA, \n                              NA, NA,\n                              NA, NA, NA, NA, NA, \n                              NA,NA,\n                              NA, NA, \n                              NA))\n\n# Add these missing rows to the all_Jacc df\nall_Jacc &lt;- rbind(all_Jacc, missing)\n\n# Rearrange all_Jacc by ID and cutDist and label\nall_Jacc &lt;- all_Jacc |&gt; \n            arrange(ID, cutdist, label)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html#difference",
    "href": "03_Fig3.html#difference",
    "title": "Generating Figure 3",
    "section": "Difference",
    "text": "Difference\nSeparate the data to separate before and after columns so we can subtract them:\n\n\nCode\n# Remove After-2\nall_Jacc2 &lt;- all_Jacc |&gt; \n  filter(!label == 'After-2')\n\n# Before: \nbefore_data2 = \n  all_Jacc2 |&gt;\n  filter(label == 'Before') |&gt; \n  group_by(cutdist,ID) |&gt;\n  mutate(ave_before = avg) |&gt; \n  mutate(ave_first_b= first) |&gt; \n  mutate(ave_second_b = second) |&gt;\n  select(c(ave_before ))\n\n# After:\nafter_data2 = \n  all_Jacc2 |&gt;\n  filter(label == 'After') |&gt; \n  group_by(cutdist,ID) |&gt;\n  mutate(ave_after = avg) |&gt;\n    mutate(ave_first= first) |&gt; \n  mutate(ave_second = second) |&gt; #change the name to avg \n  select(c(ave_after)) # only these columns are selected\n\nbefore_data2 &lt;- na.omit(before_data2)\nafter_data2 &lt;- na.omit(after_data2)\n\n# Add the after_data to before_data:\nall_Jacc_diff2 &lt;- left_join(after_data2, before_data2, by=c(\"cutdist\", \"ID\"))\n\n# Calculate the differences for avg:\nall_Jacc_diff&lt;- all_Jacc_diff2 |&gt; \n  mutate(Tavg_diff = ave_after - ave_before )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html#the-interval-trio-problem",
    "href": "03_Fig3.html#the-interval-trio-problem",
    "title": "Generating Figure 3",
    "section": "The interval-trio problem",
    "text": "The interval-trio problem\nSince the LOME and PT have three intervals that we are analyzing, we need to treat it differently so we can calculate the difference. We will do much of what was done in the previous chunk to After-2 and apply it to find the difference between that and the After column.\n\n\nCode\n# Before: \nbefore_LOME = \n  LOME_Jacc |&gt;\n  filter(label == 'After') |&gt; \n  group_by(cutdist,ID) |&gt;\n mutate(ave_before = avg) |&gt;\n #change the name to avg \n  select(ave_before)\n\nbefore_PT = \n  PT_Jacc |&gt;\n  filter(label == 'After') |&gt; \n  group_by(cutdist,ID) |&gt;\n mutate(ave_before = avg) |&gt;\n select(ave_before)\n\n# After:\nafter_LOME = \n  LOME2_Jacc |&gt;\n  group_by(cutdist,ID) |&gt;\n  filter(label == 'After-2') |&gt; \n mutate(ave_after = avg) |&gt;\n #change the name to avg \n    ungroup() |&gt; \n  select(ave_after) # only this column is selected\n\nafter_PT = \n  PT_Jacc |&gt;\n  group_by(cutdist,ID) |&gt;\n  filter(label == 'After-2') |&gt; \n mutate(ave_after = avg) |&gt;\nungroup() |&gt; \n  select(ave_after) # only this column is selected\n\n# Add the after_data to before_data:\nLOME_Jacc &lt;- cbind(before_LOME,after_LOME)\nLOME_Jacc$ID &lt;- 'LOME2'\n\nPT_Jacc &lt;- cbind(before_PT, after_PT)\nPT_Jacc$ID &lt;- 'PT2'\n\n# Calculate the differences for avg:\nLOME_Jacc&lt;- LOME_Jacc |&gt; \n  mutate(Tavg_diff = ave_after - ave_before ) \n\n\nJacc &lt;- rbind(all_Jacc_diff,LOME_Jacc)\n\nPT_Jacc &lt;- PT_Jacc |&gt; \n  mutate(Tavg_diff = ave_after - ave_before ) \n\nJacc &lt;- rbind(Jacc,PT_Jacc)\n\n\n# Add a label to determine whether an interval is a mass extinction or background interval:\n\nJacc$mass &lt;- NA\n\n#Using stringr's str_detect and dplyr's mutate, we can do this:\nJacc &lt;- Jacc |&gt; \n  mutate(mass = case_when(str_detect(ID, \"(lome|LOME2|PT|PT2|TJ|Kpg)$\")~ \"mass\", TRUE ~ \"background\")) #all else are labeled background\n\n\nNow, here is the final dataset with all interval pairs and trios!\n\n\nCode\nJacc\n\n\n# A tibble: 133 × 6\n# Groups:   cutdist, ID [133]\n   cutdist ID     ave_after ave_before Tavg_diff mass      \n     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n 1       0 AlbCen    0.0571     0.0417  0.0153   background\n 2    2000 AlbCen    0.0345     0.0222  0.0123   background\n 3    4000 AlbCen    0.0416     0.0435 -0.00194  background\n 4    6000 AlbCen    0.0240     0.0338 -0.00981  background\n 5    8000 AlbCen    0.0247     0.0258 -0.00110  background\n 6   10000 AlbCen    0.0210     0.0292 -0.00816  background\n 7   12000 AlbCen    0.0181     0.0241 -0.00595  background\n 8   14000 AlbCen    0.0246     0.0250 -0.000375 background\n 9   16000 AlbCen    0.0235     0.0212  0.00230  background\n10   18000 AlbCen    0.0112     0.0771 -0.0659   background\n# ℹ 123 more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html#finding-the-no-change-zone",
    "href": "03_Fig3.html#finding-the-no-change-zone",
    "title": "Generating Figure 3",
    "section": "Finding the “no-change” zone",
    "text": "Finding the “no-change” zone\nFor this particular set of plots, we are going to want the dataframes of when we compare each age to itself (Changhsingian-to-Changhsingian, for example) instead of comparing before and after. Luckily, we already calculated that in the previous set of codes (before_after.R). From that code, we are interested in looking at the dataframes for each age that stored all our calculated similarity values for each iteration of our subsamples per age. We loaded that dataset in earlier in this script and labeled it all_errors, which gives the age as well as the maximumum and minimum values of those differences.\n\n\nCode\nall_errors\n\n\n# A tibble: 226 × 8\n   cutdist  mean_diff      sd     n       se lower_95 upper_95 ID   \n     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1       0 -0.00132   0.00751    99 0.000755  -0.0160  0.0134  Alb  \n 2    2000 -0.000565  0.00492    99 0.000495  -0.0102  0.00908 Alb  \n 3    4000 -0.000692  0.00844    99 0.000848  -0.0172  0.0159  Alb  \n 4    6000  0.000185  0.00661    99 0.000664  -0.0128  0.0131  Alb  \n 5    8000 -0.000686  0.00532    99 0.000535  -0.0111  0.00975 Alb  \n 6   10000  0.000160  0.00581    99 0.000584  -0.0112  0.0115  Alb  \n 7   12000 -0.000378  0.00548    99 0.000551  -0.0111  0.0104  Alb  \n 8   14000 -0.0000350 0.00629    99 0.000632  -0.0124  0.0123  Alb  \n 9   16000 -0.000755  0.00853    99 0.000857  -0.0175  0.0160  Alb  \n10   18000 -0.00788   0.0802     98 0.00810   -0.165   0.149   Alb  \n# ℹ 216 more rows\n\n\nTo calculate the “band of no-change”, We want to use the average of the upper and lower 95% confidence intervals for each distance bin (labeled as cut_dist here) across all the ages.\n\n\nCode\n# Upper 95 confidence interval:\nave_up95ci &lt;- all_errors |&gt;\n  group_by(cutdist) |&gt;\n  summarise(upci = mean(upper_95, na.rm = TRUE)) |&gt; \n  ungroup()\n\n# Lowe 95 confidence interval:\nave_low95ci &lt;- all_errors |&gt;\n  group_by(cutdist) |&gt;\n  summarise(lowci = mean(lower_95, na.rm = TRUE)) |&gt; \n  ungroup()\n\nciband &lt;- left_join(ave_up95ci,ave_low95ci, by = \"cutdist\")\nciband$cutdist[is.na(ciband$cutdist)] &lt;- 20000\n\n\n\nciband\n\n\n# A tibble: 11 × 3\n   cutdist   upci   lowci\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1       0 0.0377 -0.0382\n 2    2000 0.0260 -0.0263\n 3    4000 0.0273 -0.0271\n 4    6000 0.0219 -0.0222\n 5    8000 0.0258 -0.0261\n 6   10000 0.0286 -0.0277\n 7   12000 0.0297 -0.0283\n 8   14000 0.0323 -0.0317\n 9   16000 0.0417 -0.0398\n10   18000 0.0598 -0.0592\n11   20000 0.0622 -0.0772",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "03_Fig3.html#plot-it-out",
    "href": "03_Fig3.html#plot-it-out",
    "title": "Generating Figure 3",
    "section": "Plot it out",
    "text": "Plot it out\nHere we create two plots to examine the change in similarity for each interval pair (or trio), as either a positive change representing trends towards homogenization, or a negative change representing change towards provincialization. The first plot is for background events and for intervals that directly succeed mass extinction events, so the Olenekian age which does not directly succeed the end-Permian mass extinction event is not included. The second plot investigates change in similarity across three subsequent ages for each of the Late Ordovician Mass Extinction (LOME) and for the end-Permian mass extinction events.\n\n\nCode\n# background versus mass extinctions plot:\nall_Jacc_noOlen &lt;- filter(Jacc, !(ID ==\"PT2\" ))\n\nPlot.allJacc &lt;-  all_Jacc_noOlen |&gt; \n  ggplot(aes(x = cutdist, \n             y = Tavg_diff,\n             group = ID, \n             colour = mass)) +     \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) + \n  ylim(-0.25, 0.25) +\n  geom_hline(yintercept = 0, linewidth = 0.5) + \n  scale_color_manual(values = c(\"grey50\", \"#CC0033\")) +\n\n  labs(\n    x = \"Great Circle Distance (km)\",\n    y = \"Change in Similarity\", \n    colour = \"Extinction type\"\n  ) +\n  theme_light() +\n  geom_ribbon(\n    data = ciband, \n    aes(x = cutdist, ymin = lowci, ymax = upci), \n    fill = \"grey50\", \n    alpha = 0.3, \n    inherit.aes = FALSE, #use this or there will be an error\n  )+\n  theme(aspect.ratio = 1)\nPlot.allJacc\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"plot.allJacc.pdf\", plot = Plot.allJacc, width = 10, height =6 , units = \"in\", dpi = 300)\n\nall_Jacc3 &lt;- Jacc |&gt;   filter(ID %in% c(\"PT\", \"lome\", \"LOME2\", \"PT2\"))\n\n\n# LOME and PT interval-trio plot: \nPlot.Jacc3 &lt;- all_Jacc3 |&gt; \n  ggplot(aes(x = cutdist, \n             y = Tavg_diff,\n             group = ID, \n             colour = ID)) +     \n  geom_line(linewidth = 1.2, position = position_dodge(width = 0.3)) + ylim(-0.25, 0.25) +\n  geom_hline(yintercept=0, linewidth=0.5) + \nscale_color_manual(values = c( \"chocolate4\",\"tan2\", \"dodgerblue2\",\"skyblue2\")) +\n   geom_ribbon(\n    data = ciband, \n    aes(x = cutdist, ymin = lowci, ymax = upci), \n    fill = \"grey50\", \n    alpha = 0.3, \n    inherit.aes = FALSE\n  )+\nlabs(x = \"Great Circle Distance (km)\",\n     y = \"Change in Similarity\", \n     colour = \"Extinction type\") +\n  theme_light() +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"),\n        aspect.ratio = 1)\nPlot.Jacc3\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"plot.Jaccwaste-pt.pdf\", plot = Plot.Jacc3, width = 10, height =6 , units = \"in\", dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating Figure 3</span>"
    ]
  },
  {
    "objectID": "04_Fig2.html",
    "href": "04_Fig2.html",
    "title": "Generating Figure 2 - Phanerozoic Climate Stripes",
    "section": "",
    "text": "Climate Stripes\nLoad in the dataset from the Supplementary Materials of Scotese (2021), which has the average global temperature per million years. In that file, you should calculate the average temperature per geologic age. If you can’t do that, then no worries! The file Scotese_intervalT.csv is made available in Github for you to use and has all the information needed for this script.\nThe following block of code creates two plots: The first is a climate stripe of Global Average Temperature (in degrees C) per geologic age, and the second is a climate stripe of climate change per geologic age (e.g., the Induan age became 5 degrees warmer than its previous age, the Changhsingian):\nCode\nlibrary(tidyverse) #(to use |&gt; and to code more cleanly)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nTemps &lt;- read.csv(file='~/Documents/BioHom/Temperature-change/Scotese_intervalT.csv')\nTemps &lt;- Temps |&gt; select (-c(ScoteseAge, GAT,T_change_1 )) #remove unwanted columns\nTemps &lt;- na.omit(Temps)\n        \n# Make the values numeric,except for the age number, which is factor\nTemps$ave_T &lt;- as.numeric(Temps$ave_T)\nTemps$T_change &lt;- as.numeric(Temps$T_change)\nTemps &lt;- Temps[order(-Temps$Age), ]  #this will reverse the x axis\nTemps$Age &lt;- factor(Temps$Age, levels =(Temps$Age)) # make it go in sequential order\n\n# Draw the plot:\nlibrary(ggplot2)\nphan_climate&lt;- Temps |&gt; \n  ggplot(aes(x = factor(Age), y = 1, fill = ave_T)) +\n  geom_tile() +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Mean Global Temperature by Age (°C)\")+\n  scale_fill_gradient2(\n    low = \"#39A9C9\",     # Cooler than 18\n    mid = \"#F8F8FF\",        # Exactly 18\n    high = \"red3\",            # Warmer than 18\n    midpoint = 18, \n    #18 degrees is the divide between icecaps and no icecaps in Scotese et al. (2021)\n    name = \"Global Ave Temp by Geologic (°C)\"\n  ) +\n   geom_text(aes(label = Age), y = 0.95, size = 2.5, angle = 90, vjust = 1.5) +\n   # the above line adds labels to keep track of which stripe corresponds to which age\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\",\n    plot.margin = margin(20, 10, 40, 10) # Bottom space for custom labels\n  )\nphan_climate\n\n\n\n\n\n\n\n\n\nCode\n # Make sure Ma is ordered and treated as a factor\nTemps$Ma &lt;- factor(Temps$Age, levels = rev(levels(Temps$Age))) #levels = rev() will reverse the x axis\nTemps$T_change &lt;- as.numeric(Temps$T_change)\n# Plot the climate stripes\n\nclimate_plot &lt;- Temps |&gt; \n  ggplot( aes(x = Age, y = 1, fill = T_change)) +\n  geom_tile() +\n  labs(title = \"Climate Change per Geologic Age\")+\n  scale_fill_gradient2(\n    low = \"#39A9C9\",       # For negative values\n    mid = \"#F8F8FF\",      # For zero\n    high = \"red3\",       # For positive values\n    midpoint = 0,       # Center the scale at 0\n    name = \"Climate Change (°C)\"\n  ) +\n  theme_void() +\n geom_text(aes(label = Age), y = 0.95, size = 2.5, angle = 90, vjust = 1.5) +\n  labs(title = \"T Change by Geologic Age\") +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\",\n    aspect.ratio = 0.25, # get the ratio of plot defined so it matches with the Foote bars \n     plot.margin = margin(20, 10, 40, 10) # Bottom space for custom labels\n  )\nclimate_plot\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"phan_climate.pdf\", plot = phan_climate, width = 8, height = 6, dpi = 300)\nggsave(\"climate-plot.pdf\", plot = climate_plot, width = 8, height = 6, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generating Figure 2 - Phanerozoic Climate Stripes</span>"
    ]
  },
  {
    "objectID": "04_Fig2.html#footes-calculation-of-per-capita-extinction-rate",
    "href": "04_Fig2.html#footes-calculation-of-per-capita-extinction-rate",
    "title": "Generating Figure 2 - Phanerozoic Climate Stripes",
    "section": "Foote’s calculation of per-capita extinction rate",
    "text": "Foote’s calculation of per-capita extinction rate\nNet we calculate Foote’s per-capita extinction rate using the simple equation from his 2000 publication. This was the gray portion of Figure 1, which was overlaid on the climate stripe:\nNow, let’s load in and clean the data. You can find the Pbdb .csv file in our Github repository as well.\n\n\nCode\n#read in the Phanerozoic pbdb dataset\npbdb &lt;-read.csv(file = '~/Documents/BioHom/pbdb.data.Nov2024.csv')\n\ninterval.ma    &lt;-  pbdb |&gt; \n  filter(!is.na(early_interval) & !is.na(min_ma)) |&gt; \n  group_by(early_interval) |&gt; \n summarise(min_ma = min(min_ma))\nnames(interval.ma) &lt;-c(\"early_interval\", \"interval.ma\")\npbdb       &lt;- merge(pbdb, interval.ma, by=c(\"early_interval\"))\n\n# round the digits to keep consistent\ninterval.ma &lt;- interval.ma |&gt; \n  mutate(\n    interval.ma = round(interval.ma, 3)\n  )\n\n\n# Find first and last occurrences and merge back into data frame, using min_ma column\nfadlad &lt;- pbdb |&gt; \n  group_by(accepted_name)  |&gt; \n  summarise(\n    fadname = max(early_interval),\n    ladname = min(early_interval),\n    fad = max(interval.ma),\n    lad = min(interval.ma)\n  )\n\n# Round to correct number of digits:\nfadlad &lt;- fadlad |&gt; \n  mutate(\n    fad = round(fad, 3),\n    lad = round(lad, 3)\n  )\n\n\n# Merge fad and lad information into data frame\npbdb &lt;- merge(pbdb, fadlad, by=c(\"accepted_name\"))\n\n# Add extinction/survivor binary variable\npbdb$ex &lt;- 0\npbdb$ex[pbdb$interval.ma==pbdb$lad] &lt;- 1\n\n# Select variables.\npbdb &lt;- pbdb |&gt; \n  select(any_of(c(\"interval.ma\",\"early_interval\",\"interval.ma\",\"fad\",\"lad\",\n                  \"accepted_name\",\"genus\",\"ex\",\"phylum\",\"class\",\n                  \"order\",\"family\",\"paleolat\",\"paleolng\",\"formation\",\"member\",\n                  \"occurrence_no\",\"collection_no\",\"collection_name\",\n                  \"reference_no\")))\n\n# Keep the classes analyzed in this study\npbdb &lt;- pbdb |&gt; \n     filter(class %in% c(\"Gastropoda\", \"Bivalvia\", \"Trilobita\", \"Rhynchonellata\", \"Strophomenata\", \"Anthozoa\"))\n \n library(CoordinateCleaner)\n# Identify Invalid Coordinates.\n  cl &lt;- cc_val(pbdb, value = \"flagged\", lat=\"paleolat\", lon  =\"paleolng\") #flags incorrect coordinates\n\n\nTesting coordinate validity\n\n\nFlagged 37220 records.\n\n\nCode\n  cl_rec &lt;- pbdb[!cl,] #extract and check them\n  \n pbdb &lt;- pbdb |&gt; \n   cc_val(lat = \"paleolat\", lon=\"paleolng\") #remove them\n\n\nTesting coordinate validity\n\n\nRemoved 37220 records.\n\n\nCode\n# Use fossilbrush to clean taxonomic errors\n library(fossilbrush)\nb_ranks &lt;- c(\"phylum\", \"class\", \"order\", \"family\", \"accepted_name\") #accepted_name is genus name\n\n# Define a list of suffixes to be used at each taxonomic level when scanning for synonyms\nb_suff = list(NULL, NULL, NULL, NULL, c(\"ina\", \"ella\", \"etta\"))\n\npbdb2 &lt;- check_taxonomy(pbdb, suff_set = b_suff, ranks = b_ranks, verbose = FALSE,clean_name = TRUE, resolve_duplicates = TRUE, jump = 5)\n\n\nChecking formatting [1/4]\n\n\n + cleaning names at rank phylum        \n + cleaning names at rank class        \n + cleaning names at rank order        \n + cleaning names at rank family        \n + cleaning names at rank accepted_name        \n\n\nChecking spelling   [2/4]\n\n\nChecking ranks      [3/4]\n\n\nChecking taxonomy   [4/4]\n\n\n + resolving duplicates at rank accepted_name      \n + resolving duplicates at rank family      \n + resolving duplicates at rank order      \n + resolving duplicates at rank class      \n\n\nCode\n# resolves homonyms, and jump refers to which taxonomic rank is the highest we resolve to. jump = 5 will stop before phylum since phylum level usually has little error.\n\n# Extract PBDB data from obdb2 so we have the corrected taxa:\npbdb &lt;- pbdb2$data[1:nrow(pbdb),]\n\n\npbdb_fulldata &lt;- pbdb # keep a record of all pertinent information, just in case\n\n\n#write.csv(pbdb, file=('~/Desktop/BioHom/pbdb_biohom_phan_cleaned.csv'))\n\n\nNow, we want to calculate Foote’s rate of extinction:  q = -ln(\\frac{N_b}{N_{bt}})  …where N_b = the number of taxa that originated before the time interval and crossed into it (boundary-crossers), and N_{bt} = the number of taxa that cross into the interval and survive it.\n\n\nCode\n# Create a matrix of age-pairs using interval.ma\n \nagepairs &lt;- interval.ma |&gt; \n  arrange(interval.ma) |&gt; # arrange by age number\n  mutate(\n    next_interval.ma= lead(interval.ma), #get the age number of the next age\n    next_early_interval= lead(early_interval) #get the name of the next age\n  ) |&gt; \n  filter(!is.na(next_interval.ma)) #remove final row which does not have a next age\n\n\n\nGet Nb and Nbt\nN_{b} will be taxa that originated at or before interval.ma, and went extinct after or at next_interval.ma.\nN_{bt} will be taxa that originated at or before interval.ma, and went extinct after next_interval.ma\n\n\nCode\nextinction_rate &lt;- agepairs |&gt; rowwise() |&gt;  \n  # rowwise is a function in dplyr that treats each row as its own group, so operations inside mutate() or other functions are applied one row at a time, instead of the standard, which is one column at a time \n mutate(\n    Nb =  sum(fadlad$fad &gt;= interval.ma      &   fadlad$lad &lt;= next_interval.ma),\n  \n    Nbt = sum(fadlad$lad &lt;  next_interval.ma &   fadlad$fad &gt;= interval.ma),\n    q = ifelse(Nb &gt; 0 & Nbt &gt; 0, -log(Nbt / Nb), NA_real_) ) |&gt; \n  ungroup()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generating Figure 2 - Phanerozoic Climate Stripes</span>"
    ]
  },
  {
    "objectID": "04_Fig2.html#foote-plot",
    "href": "04_Fig2.html#foote-plot",
    "title": "Generating Figure 2 - Phanerozoic Climate Stripes",
    "section": "Foote plot:",
    "text": "Foote plot:\n\n\nCode\nTemps$Ma &lt;- as.numeric(as.character(Temps$Ma)) \nextinction_rate$interval.ma &lt;- as.numeric(extinction_rate$interval.ma)\n\nextinction_sub &lt;- extinction_rate |&gt;\n  arrange(desc(interval.ma)) |&gt; \n  mutate(interval_ma_factor = factor(interval.ma, levels = unique(interval.ma))) #convert interval.ma to factor so we can space each age evenly to match the climate stripe spacing \n\nextinction_plot &lt;- ggplot(extinction_sub, aes(x = interval_ma_factor, y = q)) + geom_col(fill = \"gray50\", width=1) + \n   geom_text(    #label all ages with q &gt; 0.3 # \n  aes(label = ifelse(q &gt; 0.1, interval.ma, \"\")), \n vjust = -0.5,  size = 3 ) +\n  theme_void() + labs(title = \"\") + #\"Foote's Per Capita Extinction Rate\" \n  theme( axis.text.x = element_blank(), # hide x labels \n         axis.ticks.x = element_blank(), \n         plot.margin = margin(0, 10, 10, 10), \n         plot.title = element_text(hjust =), aspect.ratio = 0.5 )\n\nextinction_plot",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generating Figure 2 - Phanerozoic Climate Stripes</span>"
    ]
  },
  {
    "objectID": "04_Fig2.html#both-plots-together",
    "href": "04_Fig2.html#both-plots-together",
    "title": "Generating Figure 2 - Phanerozoic Climate Stripes",
    "section": "Both plots together:",
    "text": "Both plots together:\n\n\nCode\nlibrary(patchwork)\nfoote_climate &lt;- climate_plot / extinction_plot + plot_layout(heights = c(1, 2)) + plot_annotation(title = \"Climate Change StripsExtinction\")\n\nfoote_climate",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generating Figure 2 - Phanerozoic Climate Stripes</span>"
    ]
  }
]